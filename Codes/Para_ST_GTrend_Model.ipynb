{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V3iPKSit76yc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rxf131/ondemand/ubuntu2204/python310/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: libcudart.so.12: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/rxf131/ondemand/ubuntu2204/python310/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.12: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#import geopy.distance # to compute distances between stations\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torch_geometric_temporal.nn import STConv\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPUs: 40\n",
      "Used CPUs: 40\n",
      "Current CPU Usage: 15.1%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Get the total number of CPUs\n",
    "total_cpus = os.cpu_count()\n",
    "print(f\"Total CPUs: {total_cpus}\")\n",
    "\n",
    "# Get the number of CPUs used by multiprocessing\n",
    "used_cpus = multiprocessing.cpu_count()\n",
    "print(f\"Used CPUs: {used_cpus}\")\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Get the CPU usage\n",
    "cpu_usage_percent = psutil.cpu_percent(interval=1)\n",
    "\n",
    "print(f\"Current CPU Usage: {cpu_usage_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TCGPu6Ri76r4"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('rwb_simulated_PR_pw_36.1_-107.61.csv', sep=',')\n",
    "df2 = pd.read_csv('rwb_simulated_PR_pw_39.86_-107.43.csv', sep=',')\n",
    "df3 = pd.read_csv('rwb_simulated_PR_pw_38.49_-108.38.csv', sep=',')\n",
    "df4 = pd.read_csv('rwb_simulated_PR_pw_39.74_-107.82.csv', sep=',')\n",
    "df5 = pd.read_csv('rwb_simulated_PR_pw_39.17_-103.68.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0jq1Wu8761C",
    "outputId": "ca605a0b-9ab7-4e65-c63d-0f20ecac8113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3344393247471684"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.concat([df1.iloc[:,10:30],df2.iloc[:,10:30],df3.iloc[:,10:30],df4.iloc[:,10:30],df5.iloc[:,10:30]], axis=1, ignore_index=False)\n",
    "x = x.fillna(0)\n",
    "# x = x*100\n",
    "np.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "V36gxyhP7639",
    "outputId": "23f64c78-cf9e-486a-f530-12c0a73f1475"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latd</th>\n",
       "      <th>lond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.0864</td>\n",
       "      <td>-107.6322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.1014</td>\n",
       "      <td>-107.6733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.0877</td>\n",
       "      <td>-107.6005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.0979</td>\n",
       "      <td>-107.5536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.1005</td>\n",
       "      <td>-107.5811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>39.1736</td>\n",
       "      <td>-103.6255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>39.1636</td>\n",
       "      <td>-103.6559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>39.1593</td>\n",
       "      <td>-103.7698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>39.1708</td>\n",
       "      <td>-103.7034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>39.1618</td>\n",
       "      <td>-103.6987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       latd      lond\n",
       "0   36.0864 -107.6322\n",
       "1   36.1014 -107.6733\n",
       "2   36.0877 -107.6005\n",
       "3   36.0979 -107.5536\n",
       "4   36.1005 -107.5811\n",
       "..      ...       ...\n",
       "95  39.1736 -103.6255\n",
       "96  39.1636 -103.6559\n",
       "97  39.1593 -103.7698\n",
       "98  39.1708 -103.7034\n",
       "99  39.1618 -103.6987\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = pd.DataFrame(columns=['latd', 'lond'])\n",
    "PLR = []\n",
    "\n",
    "for i in range(10, 30):\n",
    "  l = df1.columns[i].split('_')\n",
    "  a = float(l[0])\n",
    "  b = float(l[1])\n",
    "  location.loc[i-10] = [a, b]\n",
    "  PLR.append(float(l[2]))\n",
    "\n",
    "for i in range(10, 30):\n",
    "  l = df2.columns[i].split('_')\n",
    "  a = float(l[0])\n",
    "  b = float(l[1])\n",
    "  location.loc[i+10] = [a, b]\n",
    "  PLR.append(float(l[2]))\n",
    "\n",
    "for i in range(10, 30):\n",
    "  l = df3.columns[i].split('_')\n",
    "  a = float(l[0])\n",
    "  b = float(l[1])\n",
    "  location.loc[i+30] = [a, b]\n",
    "  PLR.append(float(l[2]))\n",
    "\n",
    "for i in range(10, 30):\n",
    "  l = df4.columns[i].split('_')\n",
    "  a = float(l[0])\n",
    "  b = float(l[1])\n",
    "  location.loc[i+50] = [a, b]\n",
    "  PLR.append(float(l[2]))\n",
    "\n",
    "for i in range(10, 30):\n",
    "  l = df5.columns[i].split('_')\n",
    "  a = float(l[0])\n",
    "  b = float(l[1])\n",
    "  location.loc[i+70] = [a, b]\n",
    "  PLR.append(float(l[2]))\n",
    "\n",
    "PLR = [x*(-1) for x in PLR]\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4F13dpBP766i",
    "outputId": "b2173006-2937-4b76-f059-8ff78e4dbf69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...   90   91   92   93   \n",
       "0   0.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0  \\\n",
       "1   1.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "2   1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "3   1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "4   1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "96  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "97  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "98  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "99  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "\n",
       "     94   95   96   97   98   99  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..  ...  ...  ...  ...  ...  ...  \n",
       "95  1.0  0.0  1.0  1.0  1.0  1.0  \n",
       "96  1.0  1.0  0.0  1.0  1.0  1.0  \n",
       "97  1.0  1.0  1.0  0.0  1.0  1.0  \n",
       "98  1.0  1.0  1.0  1.0  0.0  1.0  \n",
       "99  1.0  1.0  1.0  1.0  1.0  0.0  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "distance = np.zeros(shape=(100,100))\n",
    "dist = []\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        d = haversine(location.iloc[i][1], location.iloc[i][0], location.iloc[j][1], location.iloc[j][0])\n",
    "        distance[i][j] = d\n",
    "        dist.append(d)\n",
    "\n",
    "dist_std = np.std(dist)\n",
    "distance = pd.DataFrame(distance)\n",
    "\n",
    "# epsilon = 0, 0.25, 0.5, 0.75, 1\n",
    "epsilon = 0.5\n",
    "sigma = dist_std\n",
    "W = np.zeros(shape=(100,100))\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        if i == j:\n",
    "            W[i][j] = 0\n",
    "        else:\n",
    "            # Compute distance between stations\n",
    "            d_ij = distance.loc[i][j]\n",
    "\n",
    "            # Compute weight w_ij\n",
    "            w_ij = np.exp(-d_ij**2 / sigma**2)\n",
    "\n",
    "            if w_ij >= epsilon:\n",
    "                W[i, j] = 1\n",
    "\n",
    "W = pd.DataFrame(W)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.FloatTensor(120, 100)\n",
    "N = 24*30\n",
    "for i in range(120):\n",
    "  for j in range(100):\n",
    "    # avg = np.mean(x.iloc[i*N:(i+1)*N, j])\n",
    "    # data[i][j] = avg\n",
    "    M = x.iloc[i*N:(i+1)*N, j].to_numpy()\n",
    "    data[i][j] = M[M!=0].mean()\n",
    "\n",
    "data = data*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJNMBBZb8sN5",
    "outputId": "fb73ec9e-0777-4e47-f552-054569d1e733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(62.1814)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "x = data.T\n",
    "A = W\n",
    "G = sp.coo_matrix(W)\n",
    "edge_index = torch.tensor(np.array([G.row, G.col]), dtype=torch.int64)\n",
    "edge_weight = torch.tensor(G.data).float()\n",
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "         4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n",
       "         7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n",
       "         9, 9, 9, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
       "         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
       "         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
       "         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n",
       "         6, 7, 8, 9]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook file path: /mnt/vstor/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current directory\n",
    "print(\"Current notebook file path:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_Adj Close</th>\n",
       "      <th>ABBV_Adj Close</th>\n",
       "      <th>ABT_Adj Close</th>\n",
       "      <th>ACN_Adj Close</th>\n",
       "      <th>ADBE_Adj Close</th>\n",
       "      <th>AMD_Adj Close</th>\n",
       "      <th>AMGN_Adj Close</th>\n",
       "      <th>AMZN_Adj Close</th>\n",
       "      <th>AVGO_Adj Close</th>\n",
       "      <th>BAC_Adj Close</th>\n",
       "      <th>...</th>\n",
       "      <th>PG_Adj Close</th>\n",
       "      <th>QCOM_Adj Close</th>\n",
       "      <th>TMO_Adj Close</th>\n",
       "      <th>TSLA_Adj Close</th>\n",
       "      <th>UNH_Adj Close</th>\n",
       "      <th>V_Adj Close</th>\n",
       "      <th>VZ_Adj Close</th>\n",
       "      <th>WFC_Adj Close</th>\n",
       "      <th>WMT_Adj Close</th>\n",
       "      <th>XOM_Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.544558</td>\n",
       "      <td>13.277270</td>\n",
       "      <td>4.345852</td>\n",
       "      <td>6.222199</td>\n",
       "      <td>4.420626</td>\n",
       "      <td>2.144714</td>\n",
       "      <td>14.235278</td>\n",
       "      <td>9.157560</td>\n",
       "      <td>4.864894</td>\n",
       "      <td>13.069831</td>\n",
       "      <td>...</td>\n",
       "      <td>6.583977</td>\n",
       "      <td>5.175687</td>\n",
       "      <td>3.997492</td>\n",
       "      <td>3.047108</td>\n",
       "      <td>14.164092</td>\n",
       "      <td>7.356254</td>\n",
       "      <td>64.797510</td>\n",
       "      <td>56.978983</td>\n",
       "      <td>5.086431</td>\n",
       "      <td>35.254216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.364982</td>\n",
       "      <td>12.782527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.776371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.956510</td>\n",
       "      <td>9.643498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.385837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.271788</td>\n",
       "      <td>7.169387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.102325</td>\n",
       "      <td>53.526864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.804969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.177705</td>\n",
       "      <td>17.228049</td>\n",
       "      <td>4.756792</td>\n",
       "      <td>0.889368</td>\n",
       "      <td>2.988094</td>\n",
       "      <td>0.642692</td>\n",
       "      <td>13.392514</td>\n",
       "      <td>4.294397</td>\n",
       "      <td>4.996045</td>\n",
       "      <td>12.791998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243843</td>\n",
       "      <td>4.652308</td>\n",
       "      <td>1.459997</td>\n",
       "      <td>2.509750</td>\n",
       "      <td>9.240533</td>\n",
       "      <td>4.537370</td>\n",
       "      <td>58.349203</td>\n",
       "      <td>55.131357</td>\n",
       "      <td>5.388179</td>\n",
       "      <td>28.848190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.758181</td>\n",
       "      <td>1.045990</td>\n",
       "      <td>1.313691</td>\n",
       "      <td>3.626165</td>\n",
       "      <td>1.494801</td>\n",
       "      <td>16.354711</td>\n",
       "      <td>8.452150</td>\n",
       "      <td>3.112016</td>\n",
       "      <td>16.465279</td>\n",
       "      <td>...</td>\n",
       "      <td>1.764985</td>\n",
       "      <td>4.516602</td>\n",
       "      <td>1.308743</td>\n",
       "      <td>2.236684</td>\n",
       "      <td>7.504964</td>\n",
       "      <td>6.378115</td>\n",
       "      <td>62.232428</td>\n",
       "      <td>60.406742</td>\n",
       "      <td>6.799907</td>\n",
       "      <td>31.424435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.595500</td>\n",
       "      <td>15.213228</td>\n",
       "      <td>2.565195</td>\n",
       "      <td>3.065266</td>\n",
       "      <td>5.994954</td>\n",
       "      <td>2.411901</td>\n",
       "      <td>19.574762</td>\n",
       "      <td>11.234945</td>\n",
       "      <td>4.716587</td>\n",
       "      <td>17.854332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928980</td>\n",
       "      <td>5.098151</td>\n",
       "      <td>5.307288</td>\n",
       "      <td>2.735731</td>\n",
       "      <td>9.615433</td>\n",
       "      <td>9.373591</td>\n",
       "      <td>68.146284</td>\n",
       "      <td>60.212293</td>\n",
       "      <td>8.308593</td>\n",
       "      <td>31.917208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>86.733697</td>\n",
       "      <td>79.967583</td>\n",
       "      <td>46.018557</td>\n",
       "      <td>67.565237</td>\n",
       "      <td>73.995449</td>\n",
       "      <td>68.833042</td>\n",
       "      <td>89.646544</td>\n",
       "      <td>59.548066</td>\n",
       "      <td>77.494065</td>\n",
       "      <td>35.801093</td>\n",
       "      <td>...</td>\n",
       "      <td>92.138714</td>\n",
       "      <td>55.129373</td>\n",
       "      <td>53.509815</td>\n",
       "      <td>52.558441</td>\n",
       "      <td>94.159080</td>\n",
       "      <td>86.437526</td>\n",
       "      <td>26.672618</td>\n",
       "      <td>59.928699</td>\n",
       "      <td>98.214941</td>\n",
       "      <td>89.157689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>92.730057</td>\n",
       "      <td>77.404643</td>\n",
       "      <td>43.486232</td>\n",
       "      <td>69.822596</td>\n",
       "      <td>80.993385</td>\n",
       "      <td>73.411318</td>\n",
       "      <td>87.792535</td>\n",
       "      <td>63.783954</td>\n",
       "      <td>85.630605</td>\n",
       "      <td>33.241673</td>\n",
       "      <td>...</td>\n",
       "      <td>93.904655</td>\n",
       "      <td>58.550540</td>\n",
       "      <td>52.110902</td>\n",
       "      <td>51.214200</td>\n",
       "      <td>97.061874</td>\n",
       "      <td>87.594702</td>\n",
       "      <td>25.250619</td>\n",
       "      <td>58.909289</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>84.699075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>94.907150</td>\n",
       "      <td>77.142016</td>\n",
       "      <td>51.042603</td>\n",
       "      <td>72.786279</td>\n",
       "      <td>82.127735</td>\n",
       "      <td>74.877239</td>\n",
       "      <td>86.396583</td>\n",
       "      <td>65.167448</td>\n",
       "      <td>87.827814</td>\n",
       "      <td>41.196632</td>\n",
       "      <td>...</td>\n",
       "      <td>93.456583</td>\n",
       "      <td>62.387498</td>\n",
       "      <td>56.099258</td>\n",
       "      <td>56.188642</td>\n",
       "      <td>95.676630</td>\n",
       "      <td>90.989980</td>\n",
       "      <td>27.635905</td>\n",
       "      <td>64.734652</td>\n",
       "      <td>87.352922</td>\n",
       "      <td>86.037768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>95.079579</td>\n",
       "      <td>77.477095</td>\n",
       "      <td>55.538507</td>\n",
       "      <td>75.047282</td>\n",
       "      <td>85.624621</td>\n",
       "      <td>76.097628</td>\n",
       "      <td>87.965019</td>\n",
       "      <td>66.499715</td>\n",
       "      <td>87.951755</td>\n",
       "      <td>40.331964</td>\n",
       "      <td>...</td>\n",
       "      <td>93.865120</td>\n",
       "      <td>61.132828</td>\n",
       "      <td>61.499823</td>\n",
       "      <td>56.479765</td>\n",
       "      <td>98.720198</td>\n",
       "      <td>94.321300</td>\n",
       "      <td>33.048659</td>\n",
       "      <td>64.622082</td>\n",
       "      <td>88.181276</td>\n",
       "      <td>86.664209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>95.861676</td>\n",
       "      <td>81.769795</td>\n",
       "      <td>58.260417</td>\n",
       "      <td>76.510918</td>\n",
       "      <td>84.173317</td>\n",
       "      <td>75.433273</td>\n",
       "      <td>93.087193</td>\n",
       "      <td>66.747373</td>\n",
       "      <td>82.638653</td>\n",
       "      <td>44.586129</td>\n",
       "      <td>...</td>\n",
       "      <td>95.551991</td>\n",
       "      <td>62.533386</td>\n",
       "      <td>62.685913</td>\n",
       "      <td>57.335421</td>\n",
       "      <td>98.737079</td>\n",
       "      <td>95.832346</td>\n",
       "      <td>38.415552</td>\n",
       "      <td>70.531877</td>\n",
       "      <td>86.174539</td>\n",
       "      <td>84.900178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AAPL_Adj Close  ABBV_Adj Close  ABT_Adj Close  ACN_Adj Close   \n",
       "0          2.544558       13.277270       4.345852       6.222199  \\\n",
       "1          0.364982       12.782527       0.000000       0.000000   \n",
       "2          1.177705       17.228049       4.756792       0.889368   \n",
       "3          0.000000       15.758181       1.045990       1.313691   \n",
       "4          0.595500       15.213228       2.565195       3.065266   \n",
       "..              ...             ...            ...            ...   \n",
       "255       86.733697       79.967583      46.018557      67.565237   \n",
       "256       92.730057       77.404643      43.486232      69.822596   \n",
       "257       94.907150       77.142016      51.042603      72.786279   \n",
       "258       95.079579       77.477095      55.538507      75.047282   \n",
       "259       95.861676       81.769795      58.260417      76.510918   \n",
       "\n",
       "     ADBE_Adj Close  AMD_Adj Close  AMGN_Adj Close  AMZN_Adj Close   \n",
       "0          4.420626       2.144714       14.235278        9.157560  \\\n",
       "1          0.000000       0.000000        6.776371        0.000000   \n",
       "2          2.988094       0.642692       13.392514        4.294397   \n",
       "3          3.626165       1.494801       16.354711        8.452150   \n",
       "4          5.994954       2.411901       19.574762       11.234945   \n",
       "..              ...            ...             ...             ...   \n",
       "255       73.995449      68.833042       89.646544       59.548066   \n",
       "256       80.993385      73.411318       87.792535       63.783954   \n",
       "257       82.127735      74.877239       86.396583       65.167448   \n",
       "258       85.624621      76.097628       87.965019       66.499715   \n",
       "259       84.173317      75.433273       93.087193       66.747373   \n",
       "\n",
       "     AVGO_Adj Close  BAC_Adj Close  ...  PG_Adj Close  QCOM_Adj Close   \n",
       "0          4.864894      13.069831  ...      6.583977        5.175687  \\\n",
       "1          3.956510       9.643498  ...      0.000000        3.385837   \n",
       "2          4.996045      12.791998  ...      0.243843        4.652308   \n",
       "3          3.112016      16.465279  ...      1.764985        4.516602   \n",
       "4          4.716587      17.854332  ...      0.928980        5.098151   \n",
       "..              ...            ...  ...           ...             ...   \n",
       "255       77.494065      35.801093  ...     92.138714       55.129373   \n",
       "256       85.630605      33.241673  ...     93.904655       58.550540   \n",
       "257       87.827814      41.196632  ...     93.456583       62.387498   \n",
       "258       87.951755      40.331964  ...     93.865120       61.132828   \n",
       "259       82.638653      44.586129  ...     95.551991       62.533386   \n",
       "\n",
       "     TMO_Adj Close  TSLA_Adj Close  UNH_Adj Close  V_Adj Close  VZ_Adj Close   \n",
       "0         3.997492        3.047108      14.164092     7.356254     64.797510  \\\n",
       "1         0.000000        2.271788       7.169387     0.000000     57.102325   \n",
       "2         1.459997        2.509750       9.240533     4.537370     58.349203   \n",
       "3         1.308743        2.236684       7.504964     6.378115     62.232428   \n",
       "4         5.307288        2.735731       9.615433     9.373591     68.146284   \n",
       "..             ...             ...            ...          ...           ...   \n",
       "255      53.509815       52.558441      94.159080    86.437526     26.672618   \n",
       "256      52.110902       51.214200      97.061874    87.594702     25.250619   \n",
       "257      56.099258       56.188642      95.676630    90.989980     27.635905   \n",
       "258      61.499823       56.479765      98.720198    94.321300     33.048659   \n",
       "259      62.685913       57.335421      98.737079    95.832346     38.415552   \n",
       "\n",
       "     WFC_Adj Close  WMT_Adj Close  XOM_Adj Close  \n",
       "0        56.978983       5.086431      35.254216  \n",
       "1        53.526864       0.000000      28.804969  \n",
       "2        55.131357       5.388179      28.848190  \n",
       "3        60.406742       6.799907      31.424435  \n",
       "4        60.212293       8.308593      31.917208  \n",
       "..             ...            ...            ...  \n",
       "255      59.928699      98.214941      89.157689  \n",
       "256      58.909289     100.000000      84.699075  \n",
       "257      64.734652      87.352922      86.037768  \n",
       "258      64.622082      88.181276      86.664209  \n",
       "259      70.531877      86.174539      84.900178  \n",
       "\n",
       "[260 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing CSV files\n",
    "folder_path = '/mnt/vstor/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Stock'\n",
    "\n",
    "# List all files in the folder\n",
    "all_files = os.listdir(folder_path)\n",
    "\n",
    "# Filter only CSV files\n",
    "csv_files = [file for file in all_files if file.endswith('.csv')]\n",
    "\n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['Adj Close']  # Replace with your actual column names\n",
    "\n",
    "# Initialize an empty DataFrame to store the data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file and read the specified columns\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "    # Read only the specified columns\n",
    "    df = pd.read_csv(file_path, usecols=columns_to_read)\n",
    "\n",
    "    # Rename columns with the CSV file name (without '.csv')\n",
    "    df.columns = [csv_file.replace('.csv', '') + '_' + col for col in df.columns]\n",
    "\n",
    "    # Concatenate the data to the merged DataFrame\n",
    "    merged_df = pd.concat([merged_df, df], axis=1)\n",
    "\n",
    "merged_df\n",
    "# # Display the resulting DataFrame\n",
    "def min_max_normalize(df):\n",
    "    # Copy the original DataFrame to avoid modifying it in place\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    # Iterate through each column and apply min-max normalization\n",
    "    for column in df.columns:\n",
    "        min_val = df[column].min()\n",
    "        max_val = df[column].max()\n",
    "\n",
    "        # Min-max normalization formula: (x - min) / (max - min)\n",
    "        normalized_df[column] = (df[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "# Apply min-max normalization\n",
    "normalized_df = min_max_normalize(merged_df)\n",
    "normalized_df = normalized_df * 100\n",
    "normalized_df = normalized_df.iloc[:260,:]\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix:\n",
      "                 AAPL_Adj Close  ABBV_Adj Close  ABT_Adj Close  ACN_Adj Close   \n",
      "AAPL_Adj Close         1.000000        0.985586       0.968800       0.985598  \\\n",
      "ABBV_Adj Close         0.985586        1.000000       0.972777       0.982711   \n",
      "ABT_Adj Close          0.968800        0.972777       1.000000       0.992942   \n",
      "ACN_Adj Close          0.985598        0.982711       0.992942       1.000000   \n",
      "ADBE_Adj Close         0.964204        0.948055       0.984848       0.985546   \n",
      "AMD_Adj Close          0.987319        0.959075       0.964489       0.980956   \n",
      "AMGN_Adj Close         0.962221        0.979087       0.989334       0.982898   \n",
      "AMZN_Adj Close         0.953466        0.941215       0.987873       0.979431   \n",
      "AVGO_Adj Close         0.983713        0.975749       0.942737       0.967104   \n",
      "BAC_Adj Close          0.951830        0.961807       0.990337       0.986783   \n",
      "BRK-B_Adj Close        0.977978        0.989679       0.987923       0.992313   \n",
      "CMCSA_Adj Close        0.940026        0.944898       0.990362       0.978650   \n",
      "COST_Adj Close         0.991792        0.993332       0.981005       0.993186   \n",
      "CRM_Adj Close          0.951634        0.943865       0.988831       0.981215   \n",
      "CSCO_Adj Close         0.949122        0.964780       0.989024       0.984123   \n",
      "CVX_Adj Close          0.958200        0.988212       0.959594       0.967410   \n",
      "DHR_Adj Close          0.984112        0.980435       0.991755       0.995658   \n",
      "DIS_Adj Close          0.892859        0.899092       0.970358       0.947530   \n",
      "GOOG_Adj Close         0.986800        0.977173       0.984963       0.996149   \n",
      "GOOGL_Adj Close        0.986790        0.977504       0.985393       0.996407   \n",
      "HD_Adj Close           0.979888        0.981148       0.996022       0.997601   \n",
      "IBM_Adj Close          0.957639        0.976849       0.986835       0.983956   \n",
      "INTC_Adj Close         0.882538        0.896135       0.963445       0.938723   \n",
      "INTU_Adj Close         0.984384        0.974841       0.985549       0.996721   \n",
      "JNJ_Adj Close          0.964191        0.979989       0.995477       0.988732   \n",
      "JPM_Adj Close          0.966502        0.972334       0.991362       0.991515   \n",
      "KO_Adj Close           0.963590        0.982316       0.990473       0.986676   \n",
      "LIN_Adj Close          0.989226        0.991417       0.983409       0.992947   \n",
      "LLY_Adj Close          0.961225        0.960165       0.899732       0.931805   \n",
      "MA_Adj Close           0.971966        0.977892       0.994417       0.991156   \n",
      "MCD_Adj Close          0.974045        0.986279       0.989867       0.990406   \n",
      "META_Adj Close         0.941262        0.922678       0.972806       0.969650   \n",
      "MRK_Adj Close          0.961512        0.983382       0.976778       0.976087   \n",
      "MSFT_Adj Close         0.994962        0.983923       0.980982       0.994114   \n",
      "NFLX_Adj Close         0.916460        0.904380       0.968931       0.955424   \n",
      "NKE_Adj Close          0.959464        0.956687       0.994487       0.987449   \n",
      "NVDA_Adj Close         0.944392        0.909998       0.862606       0.905459   \n",
      "ORCL_Adj Close         0.983251        0.982123       0.974753       0.987867   \n",
      "PEP_Adj Close          0.974260        0.988358       0.990017       0.990005   \n",
      "PFE_Adj Close          0.952154        0.973890       0.988046       0.982726   \n",
      "PG_Adj Close           0.973222        0.982661       0.995223       0.992410   \n",
      "QCOM_Adj Close         0.980074        0.970984       0.989703       0.991099   \n",
      "TMO_Adj Close          0.986284        0.986054       0.992524       0.995546   \n",
      "TSLA_Adj Close         0.960398        0.923150       0.910399       0.936541   \n",
      "UNH_Adj Close          0.988423        0.997162       0.979341       0.989615   \n",
      "V_Adj Close            0.967070        0.975121       0.994119       0.989577   \n",
      "VZ_Adj Close           0.901104        0.921072       0.977029       0.953091   \n",
      "WFC_Adj Close          0.928395        0.954062       0.972356       0.969780   \n",
      "WMT_Adj Close          0.970865        0.978671       0.993942       0.989711   \n",
      "XOM_Adj Close          0.947570        0.978603       0.934021       0.948539   \n",
      "\n",
      "                 ADBE_Adj Close  AMD_Adj Close  AMGN_Adj Close   \n",
      "AAPL_Adj Close         0.964204       0.987319        0.962221  \\\n",
      "ABBV_Adj Close         0.948055       0.959075        0.979087   \n",
      "ABT_Adj Close          0.984848       0.964489        0.989334   \n",
      "ACN_Adj Close          0.985546       0.980956        0.982898   \n",
      "ADBE_Adj Close         1.000000       0.974621        0.970571   \n",
      "AMD_Adj Close          0.974621       1.000000        0.943098   \n",
      "AMGN_Adj Close         0.970571       0.943098        1.000000   \n",
      "AMZN_Adj Close         0.992668       0.962155        0.971543   \n",
      "AVGO_Adj Close         0.942588       0.965277        0.948383   \n",
      "BAC_Adj Close          0.972441       0.948933        0.976813   \n",
      "BRK-B_Adj Close        0.970751       0.960543        0.990235   \n",
      "CMCSA_Adj Close        0.984533       0.941243        0.980601   \n",
      "COST_Adj Close         0.967824       0.976564        0.980129   \n",
      "CRM_Adj Close          0.993319       0.960533        0.974953   \n",
      "CSCO_Adj Close         0.973919       0.940600        0.986504   \n",
      "CVX_Adj Close          0.923405       0.923392        0.974072   \n",
      "DHR_Adj Close          0.981373       0.978402        0.978084   \n",
      "DIS_Adj Close          0.960072       0.901336        0.951617   \n",
      "GOOG_Adj Close         0.985242       0.985209        0.971189   \n",
      "GOOGL_Adj Close        0.985214       0.985304        0.971614   \n",
      "HD_Adj Close           0.985075       0.974038        0.987850   \n",
      "IBM_Adj Close          0.968251       0.939139        0.994699   \n",
      "INTC_Adj Close         0.955873       0.889376        0.958362   \n",
      "INTU_Adj Close         0.986967       0.984594        0.971674   \n",
      "JNJ_Adj Close          0.974268       0.949674        0.995941   \n",
      "JPM_Adj Close          0.981513       0.959109        0.986177   \n",
      "KO_Adj Close           0.965836       0.945570        0.994073   \n",
      "LIN_Adj Close          0.973421       0.973580        0.984842   \n",
      "LLY_Adj Close          0.897917       0.927792        0.920279   \n",
      "MA_Adj Close           0.982837       0.960413        0.994915   \n",
      "MCD_Adj Close          0.970251       0.956104        0.992719   \n",
      "META_Adj Close         0.992417       0.955503        0.957064   \n",
      "MRK_Adj Close          0.952086       0.935460        0.990535   \n",
      "MSFT_Adj Close         0.981011       0.988719        0.973959   \n",
      "NFLX_Adj Close         0.981845       0.934983        0.950686   \n",
      "NKE_Adj Close          0.987645       0.965059        0.975668   \n",
      "NVDA_Adj Close         0.890201       0.938649        0.862404   \n",
      "ORCL_Adj Close         0.971636       0.969212        0.975325   \n",
      "PEP_Adj Close          0.967886       0.955560        0.993421   \n",
      "PFE_Adj Close          0.957461       0.941181        0.981089   \n",
      "PG_Adj Close           0.978183       0.961212        0.995013   \n",
      "QCOM_Adj Close         0.980122       0.981368        0.971878   \n",
      "TMO_Adj Close          0.977250       0.978340        0.981512   \n",
      "TSLA_Adj Close         0.915101       0.966124        0.877745   \n",
      "UNH_Adj Close          0.959579       0.966509        0.982240   \n",
      "V_Adj Close            0.982742       0.955430        0.995230   \n",
      "VZ_Adj Close           0.958073       0.900667        0.972802   \n",
      "WFC_Adj Close          0.946935       0.914505        0.971818   \n",
      "WMT_Adj Close          0.980920       0.958496        0.995828   \n",
      "XOM_Adj Close          0.899104       0.905249        0.954735   \n",
      "\n",
      "                 AMZN_Adj Close  AVGO_Adj Close  BAC_Adj Close  ...   \n",
      "AAPL_Adj Close         0.953466        0.983713       0.951830  ...  \\\n",
      "ABBV_Adj Close         0.941215        0.975749       0.961807  ...   \n",
      "ABT_Adj Close          0.987873        0.942737       0.990337  ...   \n",
      "ACN_Adj Close          0.979431        0.967104       0.986783  ...   \n",
      "ADBE_Adj Close         0.992668        0.942588       0.972441  ...   \n",
      "AMD_Adj Close          0.962155        0.965277       0.948933  ...   \n",
      "AMGN_Adj Close         0.971543        0.948383       0.976813  ...   \n",
      "AMZN_Adj Close         1.000000        0.920384       0.974185  ...   \n",
      "AVGO_Adj Close         0.920384        1.000000       0.926444  ...   \n",
      "BAC_Adj Close          0.974185        0.926444       1.000000  ...   \n",
      "BRK-B_Adj Close        0.964991        0.972343       0.982609  ...   \n",
      "CMCSA_Adj Close        0.987428        0.919588       0.986167  ...   \n",
      "COST_Adj Close         0.959027        0.980086       0.970393  ...   \n",
      "CRM_Adj Close          0.993577        0.928167       0.977793  ...   \n",
      "CSCO_Adj Close         0.973192        0.940313       0.987672  ...   \n",
      "CVX_Adj Close          0.916842        0.961374       0.956721  ...   \n",
      "DHR_Adj Close          0.978604        0.953720       0.983056  ...   \n",
      "DIS_Adj Close          0.973916        0.857262       0.973934  ...   \n",
      "GOOG_Adj Close         0.976296        0.968747       0.980001  ...   \n",
      "GOOGL_Adj Close        0.976378        0.968801       0.980416  ...   \n",
      "HD_Adj Close           0.981630        0.959131       0.988415  ...   \n",
      "IBM_Adj Close          0.965208        0.952853       0.981473  ...   \n",
      "INTC_Adj Close         0.967293        0.859472       0.961793  ...   \n",
      "INTU_Adj Close         0.976005        0.967821       0.980320  ...   \n",
      "JNJ_Adj Close          0.976624        0.945616       0.987280  ...   \n",
      "JPM_Adj Close          0.975346        0.956233       0.991439  ...   \n",
      "KO_Adj Close           0.965496        0.950769       0.984149  ...   \n",
      "LIN_Adj Close          0.962508        0.983739       0.970990  ...   \n",
      "LLY_Adj Close          0.868920        0.986252       0.879218  ...   \n",
      "MA_Adj Close           0.980847        0.957654       0.983095  ...   \n",
      "MCD_Adj Close          0.966257        0.965390       0.980214  ...   \n",
      "META_Adj Close         0.986268        0.925578       0.960979  ...   \n",
      "MRK_Adj Close          0.945999        0.961382       0.964024  ...   \n",
      "MSFT_Adj Close         0.969121        0.982912       0.967119  ...   \n",
      "NFLX_Adj Close         0.984132        0.892266       0.957316  ...   \n",
      "NKE_Adj Close          0.988761        0.927530       0.988766  ...   \n",
      "NVDA_Adj Close         0.852322        0.972039       0.840052  ...   \n",
      "ORCL_Adj Close         0.955532        0.986575       0.964379  ...   \n",
      "PEP_Adj Close          0.965058        0.962177       0.980209  ...   \n",
      "PFE_Adj Close          0.961881        0.926674       0.988010  ...   \n",
      "PG_Adj Close           0.977209        0.957217       0.984171  ...   \n",
      "QCOM_Adj Close         0.981398        0.946760       0.980581  ...   \n",
      "TMO_Adj Close          0.974944        0.959116       0.981465  ...   \n",
      "TSLA_Adj Close         0.908102        0.923648       0.900832  ...   \n",
      "UNH_Adj Close          0.952893        0.975416       0.970076  ...   \n",
      "V_Adj Close            0.981053        0.953095       0.983800  ...   \n",
      "VZ_Adj Close           0.971610        0.874234       0.973205  ...   \n",
      "WFC_Adj Close          0.944724        0.923189       0.987211  ...   \n",
      "WMT_Adj Close          0.980099        0.956546       0.980211  ...   \n",
      "XOM_Adj Close          0.885150        0.964280       0.930909  ...   \n",
      "\n",
      "                 PG_Adj Close  QCOM_Adj Close  TMO_Adj Close  TSLA_Adj Close   \n",
      "AAPL_Adj Close       0.973222        0.980074       0.986284        0.960398  \\\n",
      "ABBV_Adj Close       0.982661        0.970984       0.986054        0.923150   \n",
      "ABT_Adj Close        0.995223        0.989703       0.992524        0.910399   \n",
      "ACN_Adj Close        0.992410        0.991099       0.995546        0.936541   \n",
      "ADBE_Adj Close       0.978183        0.980122       0.977250        0.915101   \n",
      "AMD_Adj Close        0.961212        0.981368       0.978340        0.966124   \n",
      "AMGN_Adj Close       0.995013        0.971878       0.981512        0.877745   \n",
      "AMZN_Adj Close       0.977209        0.981398       0.974944        0.908102   \n",
      "AVGO_Adj Close       0.957217        0.946760       0.959116        0.923648   \n",
      "BAC_Adj Close        0.984171        0.980581       0.981465        0.900832   \n",
      "BRK-B_Adj Close      0.995015        0.977020       0.987634        0.906916   \n",
      "CMCSA_Adj Close      0.984633        0.972215       0.970844        0.871448   \n",
      "COST_Adj Close       0.988589        0.981658       0.991965        0.936745   \n",
      "CRM_Adj Close        0.981954        0.976800       0.975321        0.892879   \n",
      "CSCO_Adj Close       0.990797        0.967870       0.975848        0.870762   \n",
      "CVX_Adj Close        0.975255        0.945290       0.967048        0.875509   \n",
      "DHR_Adj Close        0.987338        0.993171       0.998141        0.943541   \n",
      "DIS_Adj Close        0.954458        0.950335       0.940709        0.835004   \n",
      "GOOG_Adj Close       0.982605        0.987031       0.990035        0.948896   \n",
      "GOOGL_Adj Close      0.983147        0.987224       0.990364        0.948635   \n",
      "HD_Adj Close         0.995182        0.990466       0.995649        0.922841   \n",
      "IBM_Adj Close        0.994204        0.966636       0.977819        0.870152   \n",
      "INTC_Adj Close       0.955020        0.932842       0.927967        0.801963   \n",
      "INTU_Adj Close       0.983605        0.986854       0.990523        0.943066   \n",
      "JNJ_Adj Close        0.997036        0.979164       0.988019        0.891181   \n",
      "JPM_Adj Close        0.991325        0.979095       0.984004        0.899445   \n",
      "KO_Adj Close         0.997123        0.974157       0.984972        0.883204   \n",
      "LIN_Adj Close        0.990870        0.978413       0.989798        0.922343   \n",
      "LLY_Adj Close        0.922684        0.901973       0.925219        0.892632   \n",
      "MA_Adj Close         0.997465        0.981454       0.987473        0.896097   \n",
      "MCD_Adj Close        0.997041        0.975956       0.988169        0.895278   \n",
      "META_Adj Close       0.965387        0.961520       0.956173        0.882569   \n",
      "MRK_Adj Close        0.990200        0.956202       0.975254        0.864760   \n",
      "MSFT_Adj Close       0.984205        0.983377       0.989894        0.945813   \n",
      "NFLX_Adj Close       0.955494        0.951488       0.946803        0.857435   \n",
      "NKE_Adj Close        0.984857        0.988800       0.986596        0.911430   \n",
      "NVDA_Adj Close       0.879829        0.880290       0.892022        0.905873   \n",
      "ORCL_Adj Close       0.983308        0.966542       0.980418        0.913897   \n",
      "PEP_Adj Close        0.997558        0.976731       0.989449        0.896492   \n",
      "PFE_Adj Close        0.987380        0.975511       0.984335        0.891853   \n",
      "PG_Adj Close         1.000000        0.983049       0.990794        0.899287   \n",
      "QCOM_Adj Close       0.983049        1.000000       0.992869        0.947665   \n",
      "TMO_Adj Close        0.990794        0.992869       1.000000        0.940557   \n",
      "TSLA_Adj Close       0.899287        0.947665       0.940557        1.000000   \n",
      "UNH_Adj Close        0.986582        0.977931       0.990636        0.931101   \n",
      "V_Adj Close          0.997009        0.978603       0.985302        0.887761   \n",
      "VZ_Adj Close         0.971882        0.949302       0.948514        0.818544   \n",
      "WFC_Adj Close        0.976048        0.950619       0.959129        0.850728   \n",
      "WMT_Adj Close        0.998078        0.979699       0.987554        0.894892   \n",
      "XOM_Adj Close        0.955334        0.919326       0.946575        0.856852   \n",
      "\n",
      "                 UNH_Adj Close  V_Adj Close  VZ_Adj Close  WFC_Adj Close   \n",
      "AAPL_Adj Close        0.988423     0.967070      0.901104       0.928395  \\\n",
      "ABBV_Adj Close        0.997162     0.975121      0.921072       0.954062   \n",
      "ABT_Adj Close         0.979341     0.994119      0.977029       0.972356   \n",
      "ACN_Adj Close         0.989615     0.989577      0.953091       0.969780   \n",
      "ADBE_Adj Close        0.959579     0.982742      0.958073       0.946935   \n",
      "AMD_Adj Close         0.966509     0.955430      0.900667       0.914505   \n",
      "AMGN_Adj Close        0.982240     0.995230      0.972802       0.971818   \n",
      "AMZN_Adj Close        0.952893     0.981053      0.971610       0.944724   \n",
      "AVGO_Adj Close        0.975416     0.953095      0.874234       0.923189   \n",
      "BAC_Adj Close         0.970076     0.983800      0.973205       0.987211   \n",
      "BRK-B_Adj Close       0.991962     0.992570      0.955956       0.980753   \n",
      "CMCSA_Adj Close       0.953478     0.990458      0.988085       0.974794   \n",
      "COST_Adj Close        0.996734     0.981774      0.931088       0.959251   \n",
      "CRM_Adj Close         0.954606     0.986415      0.976227       0.957940   \n",
      "CSCO_Adj Close        0.970118     0.992464      0.979510       0.988269   \n",
      "CVX_Adj Close         0.985187     0.968090      0.922638       0.968617   \n",
      "DHR_Adj Close         0.987426     0.983465      0.947656       0.957626   \n",
      "DIS_Adj Close         0.911472     0.963562      0.986253       0.956508   \n",
      "GOOG_Adj Close        0.984561     0.980804      0.938012       0.959383   \n",
      "GOOGL_Adj Close       0.984807     0.981102      0.938797       0.960101   \n",
      "HD_Adj Close          0.987351     0.992724      0.962008       0.971499   \n",
      "IBM_Adj Close         0.980855     0.995166      0.972597       0.985115   \n",
      "INTC_Adj Close        0.906450     0.964253      0.991690       0.954627   \n",
      "INTU_Adj Close        0.983375     0.981736      0.939592       0.961242   \n",
      "JNJ_Adj Close         0.984068     0.995892      0.977951       0.980060   \n",
      "JPM_Adj Close         0.978167     0.994062      0.967135       0.984279   \n",
      "KO_Adj Close          0.985296     0.994524      0.972527       0.984198   \n",
      "LIN_Adj Close         0.993654     0.988335      0.937049       0.962123   \n",
      "LLY_Adj Close         0.957211     0.917193      0.819131       0.883152   \n",
      "MA_Adj Close          0.982386     0.999381      0.971725       0.973506   \n",
      "MCD_Adj Close         0.989502     0.994875      0.961723       0.978532   \n",
      "META_Adj Close        0.935088     0.974365      0.957523       0.940459   \n",
      "MRK_Adj Close         0.982983     0.988041      0.952351       0.971511   \n",
      "MSFT_Adj Close        0.989194     0.981256      0.927146       0.948373   \n",
      "NFLX_Adj Close        0.917164     0.964824      0.964679       0.931845   \n",
      "NKE_Adj Close         0.965030     0.985913      0.972946       0.964165   \n",
      "NVDA_Adj Close        0.910033     0.875041      0.766342       0.829524   \n",
      "ORCL_Adj Close        0.985292     0.982801      0.927483       0.960919   \n",
      "PEP_Adj Close         0.990775     0.994006      0.961930       0.977310   \n",
      "PFE_Adj Close         0.978450     0.980413      0.970080       0.980857   \n",
      "PG_Adj Close          0.986582     0.997009      0.971882       0.976048   \n",
      "QCOM_Adj Close        0.977931     0.978603      0.949302       0.950619   \n",
      "TMO_Adj Close         0.990636     0.985302      0.948514       0.959129   \n",
      "TSLA_Adj Close        0.931101     0.887761      0.818544       0.850728   \n",
      "UNH_Adj Close         1.000000     0.979978      0.929637       0.959873   \n",
      "V_Adj Close           0.979978     1.000000      0.975947       0.976275   \n",
      "VZ_Adj Close          0.929637     0.975947      1.000000       0.967992   \n",
      "WFC_Adj Close         0.959873     0.976275      0.967992       1.000000   \n",
      "WMT_Adj Close         0.982909     0.997688      0.972089       0.971539   \n",
      "XOM_Adj Close         0.973354     0.948919      0.888042       0.951470   \n",
      "\n",
      "                 WMT_Adj Close  XOM_Adj Close  \n",
      "AAPL_Adj Close        0.970865       0.947570  \n",
      "ABBV_Adj Close        0.978671       0.978603  \n",
      "ABT_Adj Close         0.993942       0.934021  \n",
      "ACN_Adj Close         0.989711       0.948539  \n",
      "ADBE_Adj Close        0.980920       0.899104  \n",
      "AMD_Adj Close         0.958496       0.905249  \n",
      "AMGN_Adj Close        0.995828       0.954735  \n",
      "AMZN_Adj Close        0.980099       0.885150  \n",
      "AVGO_Adj Close        0.956546       0.964280  \n",
      "BAC_Adj Close         0.980211       0.930909  \n",
      "BRK-B_Adj Close       0.993031       0.973013  \n",
      "CMCSA_Adj Close       0.986291       0.910609  \n",
      "COST_Adj Close        0.985324       0.966922  \n",
      "CRM_Adj Close         0.984623       0.900403  \n",
      "CSCO_Adj Close        0.990189       0.948301  \n",
      "CVX_Adj Close         0.970313       0.994140  \n",
      "DHR_Adj Close         0.984612       0.936926  \n",
      "DIS_Adj Close         0.956723       0.856203  \n",
      "GOOG_Adj Close        0.980426       0.938566  \n",
      "GOOGL_Adj Close       0.980860       0.939123  \n",
      "HD_Adj Close          0.993024       0.946741  \n",
      "IBM_Adj Close         0.993302       0.964903  \n",
      "INTC_Adj Close        0.957766       0.858878  \n",
      "INTU_Adj Close        0.981659       0.939414  \n",
      "JNJ_Adj Close         0.995836       0.953519  \n",
      "JPM_Adj Close         0.991202       0.949659  \n",
      "KO_Adj Close          0.994115       0.964805  \n",
      "LIN_Adj Close         0.989841       0.969983  \n",
      "LLY_Adj Close         0.923055       0.961756  \n",
      "MA_Adj Close          0.997603       0.950330  \n",
      "MCD_Adj Close         0.995238       0.970576  \n",
      "META_Adj Close        0.970321       0.878018  \n",
      "MRK_Adj Close         0.989632       0.978973  \n",
      "MSFT_Adj Close        0.982865       0.949087  \n",
      "NFLX_Adj Close        0.962845       0.855147  \n",
      "NKE_Adj Close         0.984196       0.909639  \n",
      "NVDA_Adj Close        0.880974       0.893822  \n",
      "ORCL_Adj Close        0.983668       0.968423  \n",
      "PEP_Adj Close         0.995267       0.969987  \n",
      "PFE_Adj Close         0.981013       0.947460  \n",
      "PG_Adj Close          0.998078       0.955334  \n",
      "QCOM_Adj Close        0.979699       0.919326  \n",
      "TMO_Adj Close         0.987554       0.946575  \n",
      "TSLA_Adj Close        0.894892       0.856852  \n",
      "UNH_Adj Close         0.982909       0.973354  \n",
      "V_Adj Close           0.997688       0.948919  \n",
      "VZ_Adj Close          0.972089       0.888042  \n",
      "WFC_Adj Close         0.971539       0.951470  \n",
      "WMT_Adj Close         1.000000       0.950916  \n",
      "XOM_Adj Close         0.950916       1.000000  \n",
      "\n",
      "[50 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample DataFrame\n",
    "# Replace this with your actual DataFrame\n",
    "data = {\n",
    "    'Column1': [1, 2, 3, 4],\n",
    "    'Column2': [5, 6, 7, 8],\n",
    "    'Column3': [9, 10, 11, 12],\n",
    "    # Add more columns as needed\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to compute cosine similarity between columns\n",
    "def compute_cosine_similarity(df):\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(df.T)  # Transpose to get columns as samples\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    adjacency_matrix = pd.DataFrame(cosine_sim_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "adjacency_matrix = compute_cosine_similarity(merged_df)\n",
    "\n",
    "# Display the adjacency matrix\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_Adj Close</th>\n",
       "      <th>ABBV_Adj Close</th>\n",
       "      <th>ABT_Adj Close</th>\n",
       "      <th>ACN_Adj Close</th>\n",
       "      <th>ADBE_Adj Close</th>\n",
       "      <th>AMD_Adj Close</th>\n",
       "      <th>AMGN_Adj Close</th>\n",
       "      <th>AMZN_Adj Close</th>\n",
       "      <th>AVGO_Adj Close</th>\n",
       "      <th>BAC_Adj Close</th>\n",
       "      <th>...</th>\n",
       "      <th>PG_Adj Close</th>\n",
       "      <th>QCOM_Adj Close</th>\n",
       "      <th>TMO_Adj Close</th>\n",
       "      <th>TSLA_Adj Close</th>\n",
       "      <th>UNH_Adj Close</th>\n",
       "      <th>V_Adj Close</th>\n",
       "      <th>VZ_Adj Close</th>\n",
       "      <th>WFC_Adj Close</th>\n",
       "      <th>WMT_Adj Close</th>\n",
       "      <th>XOM_Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL_Adj Close</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888553</td>\n",
       "      <td>0.835096</td>\n",
       "      <td>0.930854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.913382</td>\n",
       "      <td>0.779497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947701</td>\n",
       "      <td>0.843805</td>\n",
       "      <td>0.927840</td>\n",
       "      <td>0.901147</td>\n",
       "      <td>0.923524</td>\n",
       "      <td>0.878010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.918141</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABBV_Adj Close</th>\n",
       "      <td>0.888553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.817756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789635</td>\n",
       "      <td>0.814668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT_Adj Close</th>\n",
       "      <td>0.835096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.898797</td>\n",
       "      <td>0.776546</td>\n",
       "      <td>0.883380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822624</td>\n",
       "      <td>0.935783</td>\n",
       "      <td>0.921596</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770920</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACN_Adj Close</th>\n",
       "      <td>0.930854</td>\n",
       "      <td>0.785601</td>\n",
       "      <td>0.898797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776719</td>\n",
       "      <td>0.933396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807020</td>\n",
       "      <td>0.753376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902944</td>\n",
       "      <td>0.886435</td>\n",
       "      <td>0.933729</td>\n",
       "      <td>0.923211</td>\n",
       "      <td>0.859254</td>\n",
       "      <td>0.846789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.835145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADBE_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776546</td>\n",
       "      <td>0.776719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMD_Adj Close</th>\n",
       "      <td>0.913382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883380</td>\n",
       "      <td>0.933396</td>\n",
       "      <td>0.841673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.775973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869876</td>\n",
       "      <td>0.894577</td>\n",
       "      <td>0.894111</td>\n",
       "      <td>0.910014</td>\n",
       "      <td>0.757026</td>\n",
       "      <td>0.809585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.835794</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMGN_Adj Close</th>\n",
       "      <td>0.779497</td>\n",
       "      <td>0.817756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.811358</td>\n",
       "      <td>0.757692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.807236</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVGO_Adj Close</th>\n",
       "      <td>0.895973</td>\n",
       "      <td>0.849173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.775973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854421</td>\n",
       "      <td>0.806828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.846596</td>\n",
       "      <td>0.760602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRK-B_Adj Close</th>\n",
       "      <td>0.894791</td>\n",
       "      <td>0.914204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.927813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797096</td>\n",
       "      <td>0.763251</td>\n",
       "      <td>0.917704</td>\n",
       "      <td>0.804116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816245</td>\n",
       "      <td>0.801920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMCSA_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COST_Adj Close</th>\n",
       "      <td>0.956148</td>\n",
       "      <td>0.923489</td>\n",
       "      <td>0.754225</td>\n",
       "      <td>0.913692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847671</td>\n",
       "      <td>0.760047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941243</td>\n",
       "      <td>0.775030</td>\n",
       "      <td>0.902537</td>\n",
       "      <td>0.853934</td>\n",
       "      <td>0.959133</td>\n",
       "      <td>0.803872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.873344</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRM_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSCO_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVX_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DHR_Adj Close</th>\n",
       "      <td>0.894189</td>\n",
       "      <td>0.766927</td>\n",
       "      <td>0.935820</td>\n",
       "      <td>0.942493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862556</td>\n",
       "      <td>0.913387</td>\n",
       "      <td>0.974442</td>\n",
       "      <td>0.917447</td>\n",
       "      <td>0.839015</td>\n",
       "      <td>0.796897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG_Adj Close</th>\n",
       "      <td>0.902750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866556</td>\n",
       "      <td>0.966388</td>\n",
       "      <td>0.815136</td>\n",
       "      <td>0.920320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842264</td>\n",
       "      <td>0.845909</td>\n",
       "      <td>0.880250</td>\n",
       "      <td>0.913009</td>\n",
       "      <td>0.813291</td>\n",
       "      <td>0.832234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.801316</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL_Adj Close</th>\n",
       "      <td>0.903822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867052</td>\n",
       "      <td>0.967771</td>\n",
       "      <td>0.812216</td>\n",
       "      <td>0.923063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.794697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.846601</td>\n",
       "      <td>0.882142</td>\n",
       "      <td>0.915167</td>\n",
       "      <td>0.814376</td>\n",
       "      <td>0.827947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800575</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HD_Adj Close</th>\n",
       "      <td>0.906978</td>\n",
       "      <td>0.768816</td>\n",
       "      <td>0.916393</td>\n",
       "      <td>0.962719</td>\n",
       "      <td>0.759551</td>\n",
       "      <td>0.907882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901877</td>\n",
       "      <td>0.891152</td>\n",
       "      <td>0.943013</td>\n",
       "      <td>0.885199</td>\n",
       "      <td>0.832119</td>\n",
       "      <td>0.838470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.837979</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.828643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTC_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.797748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTU_Adj Close</th>\n",
       "      <td>0.881979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855855</td>\n",
       "      <td>0.967585</td>\n",
       "      <td>0.827771</td>\n",
       "      <td>0.919553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.779595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833807</td>\n",
       "      <td>0.840596</td>\n",
       "      <td>0.883103</td>\n",
       "      <td>0.891322</td>\n",
       "      <td>0.790701</td>\n",
       "      <td>0.813458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JNJ_Adj Close</th>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.902003</td>\n",
       "      <td>0.833397</td>\n",
       "      <td>0.864632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767661</td>\n",
       "      <td>0.787517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871326</td>\n",
       "      <td>0.794334</td>\n",
       "      <td>0.909583</td>\n",
       "      <td>0.843047</td>\n",
       "      <td>0.920262</td>\n",
       "      <td>0.782555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798881</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM_Adj Close</th>\n",
       "      <td>0.752228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KO_Adj Close</th>\n",
       "      <td>0.819749</td>\n",
       "      <td>0.905123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.783396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LIN_Adj Close</th>\n",
       "      <td>0.948331</td>\n",
       "      <td>0.900848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833904</td>\n",
       "      <td>0.788295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.949293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855927</td>\n",
       "      <td>0.785087</td>\n",
       "      <td>0.916371</td>\n",
       "      <td>0.886497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907378</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLY_Adj Close</th>\n",
       "      <td>0.826420</td>\n",
       "      <td>0.863218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792056</td>\n",
       "      <td>0.837922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MA_Adj Close</th>\n",
       "      <td>0.897581</td>\n",
       "      <td>0.751068</td>\n",
       "      <td>0.803019</td>\n",
       "      <td>0.856765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829423</td>\n",
       "      <td>0.785290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901842</td>\n",
       "      <td>0.772651</td>\n",
       "      <td>0.816899</td>\n",
       "      <td>0.756898</td>\n",
       "      <td>0.777947</td>\n",
       "      <td>0.982893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCD_Adj Close</th>\n",
       "      <td>0.897921</td>\n",
       "      <td>0.898085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.903910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910259</td>\n",
       "      <td>0.811878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.829969</td>\n",
       "      <td>0.810765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>META_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRK_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.817076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.834689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT_Adj Close</th>\n",
       "      <td>0.972543</td>\n",
       "      <td>0.823227</td>\n",
       "      <td>0.831089</td>\n",
       "      <td>0.950385</td>\n",
       "      <td>0.772075</td>\n",
       "      <td>0.941555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923173</td>\n",
       "      <td>0.814407</td>\n",
       "      <td>0.895576</td>\n",
       "      <td>0.882351</td>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.891767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.903860</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.756355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKE_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891996</td>\n",
       "      <td>0.774060</td>\n",
       "      <td>0.814040</td>\n",
       "      <td>0.792371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858636</td>\n",
       "      <td>0.777001</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA_Adj Close</th>\n",
       "      <td>0.870559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.822067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769826</td>\n",
       "      <td>0.799963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840907</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORCL_Adj Close</th>\n",
       "      <td>0.871395</td>\n",
       "      <td>0.790612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814599</td>\n",
       "      <td>0.838320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.840071</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEP_Adj Close</th>\n",
       "      <td>0.902910</td>\n",
       "      <td>0.938156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.818396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937318</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831563</td>\n",
       "      <td>0.801176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFE_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG_Adj Close</th>\n",
       "      <td>0.947701</td>\n",
       "      <td>0.868976</td>\n",
       "      <td>0.822624</td>\n",
       "      <td>0.902944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869876</td>\n",
       "      <td>0.778392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819395</td>\n",
       "      <td>0.906964</td>\n",
       "      <td>0.818840</td>\n",
       "      <td>0.889641</td>\n",
       "      <td>0.869922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.917826</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QCOM_Adj Close</th>\n",
       "      <td>0.843805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935783</td>\n",
       "      <td>0.886435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909368</td>\n",
       "      <td>0.917720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMO_Adj Close</th>\n",
       "      <td>0.927840</td>\n",
       "      <td>0.835733</td>\n",
       "      <td>0.921596</td>\n",
       "      <td>0.933729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906964</td>\n",
       "      <td>0.909368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920203</td>\n",
       "      <td>0.879827</td>\n",
       "      <td>0.793966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA_Adj Close</th>\n",
       "      <td>0.901147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.923211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818840</td>\n",
       "      <td>0.917720</td>\n",
       "      <td>0.920203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.780372</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNH_Adj Close</th>\n",
       "      <td>0.923524</td>\n",
       "      <td>0.968426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.757026</td>\n",
       "      <td>0.811358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879827</td>\n",
       "      <td>0.815430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811382</td>\n",
       "      <td>0.762958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V_Adj Close</th>\n",
       "      <td>0.878010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.777723</td>\n",
       "      <td>0.846789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.809585</td>\n",
       "      <td>0.757692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.806828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894885</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VZ_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WFC_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMT_Adj Close</th>\n",
       "      <td>0.918141</td>\n",
       "      <td>0.789635</td>\n",
       "      <td>0.770920</td>\n",
       "      <td>0.835145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835794</td>\n",
       "      <td>0.807236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832604</td>\n",
       "      <td>0.780372</td>\n",
       "      <td>0.811382</td>\n",
       "      <td>0.894885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM_Adj Close</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.760602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL_Adj Close  ABBV_Adj Close  ABT_Adj Close  ACN_Adj Close   \n",
       "AAPL_Adj Close         1.000000        0.888553       0.835096       0.930854  \\\n",
       "ABBV_Adj Close         0.888553        1.000000       0.000000       0.785601   \n",
       "ABT_Adj Close          0.835096        0.000000       1.000000       0.898797   \n",
       "ACN_Adj Close          0.930854        0.785601       0.898797       1.000000   \n",
       "ADBE_Adj Close         0.000000        0.000000       0.776546       0.776719   \n",
       "AMD_Adj Close          0.913382        0.000000       0.883380       0.933396   \n",
       "AMGN_Adj Close         0.779497        0.817756       0.000000       0.000000   \n",
       "AMZN_Adj Close         0.000000        0.000000       0.767864       0.000000   \n",
       "AVGO_Adj Close         0.895973        0.849173       0.000000       0.807020   \n",
       "BAC_Adj Close          0.000000        0.000000       0.000000       0.753376   \n",
       "BRK-B_Adj Close        0.894791        0.914204       0.000000       0.857946   \n",
       "CMCSA_Adj Close        0.000000        0.000000       0.000000       0.000000   \n",
       "COST_Adj Close         0.956148        0.923489       0.754225       0.913692   \n",
       "CRM_Adj Close          0.000000        0.000000       0.000000       0.000000   \n",
       "CSCO_Adj Close         0.000000        0.000000       0.000000       0.000000   \n",
       "CVX_Adj Close          0.000000        0.864086       0.000000       0.000000   \n",
       "DHR_Adj Close          0.894189        0.766927       0.935820       0.942493   \n",
       "DIS_Adj Close          0.000000        0.000000       0.000000       0.000000   \n",
       "GOOG_Adj Close         0.902750        0.000000       0.866556       0.966388   \n",
       "GOOGL_Adj Close        0.903822        0.000000       0.867052       0.967771   \n",
       "HD_Adj Close           0.906978        0.768816       0.916393       0.962719   \n",
       "IBM_Adj Close          0.000000        0.795663       0.000000       0.000000   \n",
       "INTC_Adj Close         0.000000        0.000000       0.000000       0.000000   \n",
       "INTU_Adj Close         0.881979        0.000000       0.855855       0.967585   \n",
       "JNJ_Adj Close          0.893491        0.902003       0.833397       0.864632   \n",
       "JPM_Adj Close          0.752228        0.000000       0.000000       0.844053   \n",
       "KO_Adj Close           0.819749        0.905123       0.000000       0.765511   \n",
       "LIN_Adj Close          0.948331        0.900848       0.000000       0.891223   \n",
       "LLY_Adj Close          0.826420        0.863218       0.000000       0.000000   \n",
       "MA_Adj Close           0.897581        0.751068       0.803019       0.856765   \n",
       "MCD_Adj Close          0.897921        0.898085       0.000000       0.826619   \n",
       "META_Adj Close         0.000000        0.000000       0.000000       0.000000   \n",
       "MRK_Adj Close          0.000000        0.817076       0.000000       0.000000   \n",
       "MSFT_Adj Close         0.972543        0.823227       0.831089       0.950385   \n",
       "NFLX_Adj Close         0.000000        0.000000       0.000000       0.000000   \n",
       "NKE_Adj Close          0.000000        0.000000       0.891996       0.774060   \n",
       "NVDA_Adj Close         0.870559        0.000000       0.000000       0.804367   \n",
       "ORCL_Adj Close         0.871395        0.790612       0.000000       0.836748   \n",
       "PEP_Adj Close          0.902910        0.938156       0.000000       0.818396   \n",
       "PFE_Adj Close          0.000000        0.000000       0.000000       0.000000   \n",
       "PG_Adj Close           0.947701        0.868976       0.822624       0.902944   \n",
       "QCOM_Adj Close         0.843805        0.000000       0.935783       0.886435   \n",
       "TMO_Adj Close          0.927840        0.835733       0.921596       0.933729   \n",
       "TSLA_Adj Close         0.901147        0.000000       0.893397       0.923211   \n",
       "UNH_Adj Close          0.923524        0.968426       0.000000       0.859254   \n",
       "V_Adj Close            0.878010        0.000000       0.777723       0.846789   \n",
       "VZ_Adj Close           0.000000        0.000000       0.000000       0.000000   \n",
       "WFC_Adj Close          0.000000        0.000000       0.000000       0.000000   \n",
       "WMT_Adj Close          0.918141        0.789635       0.770920       0.835145   \n",
       "XOM_Adj Close          0.000000        0.814668       0.000000       0.000000   \n",
       "\n",
       "                 ADBE_Adj Close  AMD_Adj Close  AMGN_Adj Close   \n",
       "AAPL_Adj Close         0.000000       0.913382        0.779497  \\\n",
       "ABBV_Adj Close         0.000000       0.000000        0.817756   \n",
       "ABT_Adj Close          0.776546       0.883380        0.000000   \n",
       "ACN_Adj Close          0.776719       0.933396        0.000000   \n",
       "ADBE_Adj Close         1.000000       0.841673        0.000000   \n",
       "AMD_Adj Close          0.841673       1.000000        0.000000   \n",
       "AMGN_Adj Close         0.000000       0.000000        1.000000   \n",
       "AMZN_Adj Close         0.893600       0.000000        0.000000   \n",
       "AVGO_Adj Close         0.000000       0.775973        0.000000   \n",
       "BAC_Adj Close          0.000000       0.000000        0.000000   \n",
       "BRK-B_Adj Close        0.000000       0.750979        0.000000   \n",
       "CMCSA_Adj Close        0.789433       0.000000        0.000000   \n",
       "COST_Adj Close         0.000000       0.847671        0.760047   \n",
       "CRM_Adj Close          0.911478       0.000000        0.000000   \n",
       "CSCO_Adj Close         0.000000       0.000000        0.000000   \n",
       "CVX_Adj Close          0.000000       0.000000        0.000000   \n",
       "DHR_Adj Close          0.000000       0.882148        0.000000   \n",
       "DIS_Adj Close          0.000000       0.000000        0.000000   \n",
       "GOOG_Adj Close         0.815136       0.920320        0.000000   \n",
       "GOOGL_Adj Close        0.812216       0.923063        0.000000   \n",
       "HD_Adj Close           0.759551       0.907882        0.000000   \n",
       "IBM_Adj Close          0.000000       0.000000        0.000000   \n",
       "INTC_Adj Close         0.000000       0.000000        0.000000   \n",
       "INTU_Adj Close         0.827771       0.919553        0.000000   \n",
       "JNJ_Adj Close          0.000000       0.767661        0.787517   \n",
       "JPM_Adj Close          0.000000       0.750659        0.000000   \n",
       "KO_Adj Close           0.000000       0.000000        0.000000   \n",
       "LIN_Adj Close          0.000000       0.833904        0.788295   \n",
       "LLY_Adj Close          0.000000       0.000000        0.772590   \n",
       "MA_Adj Close           0.000000       0.829423        0.785290   \n",
       "MCD_Adj Close          0.000000       0.000000        0.000000   \n",
       "META_Adj Close         0.900307       0.000000        0.000000   \n",
       "MRK_Adj Close          0.000000       0.000000        0.000000   \n",
       "MSFT_Adj Close         0.772075       0.941555        0.000000   \n",
       "NFLX_Adj Close         0.756355       0.000000        0.000000   \n",
       "NKE_Adj Close          0.814040       0.792371        0.000000   \n",
       "NVDA_Adj Close         0.000000       0.822067        0.000000   \n",
       "ORCL_Adj Close         0.000000       0.780637        0.000000   \n",
       "PEP_Adj Close          0.000000       0.000000        0.758353   \n",
       "PFE_Adj Close          0.000000       0.000000        0.000000   \n",
       "PG_Adj Close           0.000000       0.869876        0.778392   \n",
       "QCOM_Adj Close         0.000000       0.894577        0.000000   \n",
       "TMO_Adj Close          0.000000       0.894111        0.000000   \n",
       "TSLA_Adj Close         0.000000       0.910014        0.000000   \n",
       "UNH_Adj Close          0.000000       0.757026        0.811358   \n",
       "V_Adj Close            0.000000       0.809585        0.757692   \n",
       "VZ_Adj Close           0.000000       0.000000        0.000000   \n",
       "WFC_Adj Close          0.000000       0.000000        0.000000   \n",
       "WMT_Adj Close          0.000000       0.835794        0.807236   \n",
       "XOM_Adj Close          0.000000       0.000000        0.000000   \n",
       "\n",
       "                 AMZN_Adj Close  AVGO_Adj Close  BAC_Adj Close  ...   \n",
       "AAPL_Adj Close         0.000000        0.895973       0.000000  ...  \\\n",
       "ABBV_Adj Close         0.000000        0.849173       0.000000  ...   \n",
       "ABT_Adj Close          0.767864        0.000000       0.000000  ...   \n",
       "ACN_Adj Close          0.000000        0.807020       0.753376  ...   \n",
       "ADBE_Adj Close         0.893600        0.000000       0.000000  ...   \n",
       "AMD_Adj Close          0.000000        0.775973       0.000000  ...   \n",
       "AMGN_Adj Close         0.000000        0.000000       0.000000  ...   \n",
       "AMZN_Adj Close         1.000000        0.000000       0.000000  ...   \n",
       "AVGO_Adj Close         0.000000        1.000000       0.000000  ...   \n",
       "BAC_Adj Close          0.000000        0.000000       1.000000  ...   \n",
       "BRK-B_Adj Close        0.000000        0.927813       0.000000  ...   \n",
       "CMCSA_Adj Close        0.767387        0.000000       0.000000  ...   \n",
       "COST_Adj Close         0.000000        0.891872       0.000000  ...   \n",
       "CRM_Adj Close          0.884394        0.000000       0.000000  ...   \n",
       "CSCO_Adj Close         0.000000        0.000000       0.000000  ...   \n",
       "CVX_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "DHR_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "DIS_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "GOOG_Adj Close         0.000000        0.793166       0.000000  ...   \n",
       "GOOGL_Adj Close        0.000000        0.794697       0.000000  ...   \n",
       "HD_Adj Close           0.000000        0.753115       0.000000  ...   \n",
       "IBM_Adj Close          0.000000        0.832101       0.000000  ...   \n",
       "INTC_Adj Close         0.000000        0.000000       0.000000  ...   \n",
       "INTU_Adj Close         0.000000        0.779595       0.000000  ...   \n",
       "JNJ_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "JPM_Adj Close          0.000000        0.000000       0.791203  ...   \n",
       "KO_Adj Close           0.000000        0.784289       0.000000  ...   \n",
       "LIN_Adj Close          0.000000        0.949293       0.000000  ...   \n",
       "LLY_Adj Close          0.000000        0.948628       0.000000  ...   \n",
       "MA_Adj Close           0.000000        0.820520       0.000000  ...   \n",
       "MCD_Adj Close          0.000000        0.903910       0.000000  ...   \n",
       "META_Adj Close         0.814288        0.000000       0.000000  ...   \n",
       "MRK_Adj Close          0.000000        0.803859       0.000000  ...   \n",
       "MSFT_Adj Close         0.000000        0.901848       0.000000  ...   \n",
       "NFLX_Adj Close         0.772016        0.000000       0.000000  ...   \n",
       "NKE_Adj Close          0.793638        0.000000       0.757046  ...   \n",
       "NVDA_Adj Close         0.000000        0.965623       0.000000  ...   \n",
       "ORCL_Adj Close         0.000000        0.953036       0.000000  ...   \n",
       "PEP_Adj Close          0.000000        0.865606       0.000000  ...   \n",
       "PFE_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "PG_Adj Close           0.000000        0.847971       0.000000  ...   \n",
       "QCOM_Adj Close         0.000000        0.000000       0.000000  ...   \n",
       "TMO_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "TSLA_Adj Close         0.000000        0.000000       0.000000  ...   \n",
       "UNH_Adj Close          0.000000        0.854421       0.000000  ...   \n",
       "V_Adj Close            0.000000        0.806828       0.000000  ...   \n",
       "VZ_Adj Close           0.000000        0.000000       0.000000  ...   \n",
       "WFC_Adj Close          0.000000        0.000000       0.000000  ...   \n",
       "WMT_Adj Close          0.000000        0.846596       0.000000  ...   \n",
       "XOM_Adj Close          0.000000        0.760602       0.000000  ...   \n",
       "\n",
       "                 PG_Adj Close  QCOM_Adj Close  TMO_Adj Close  TSLA_Adj Close   \n",
       "AAPL_Adj Close       0.947701        0.843805       0.927840        0.901147  \\\n",
       "ABBV_Adj Close       0.868976        0.000000       0.835733        0.000000   \n",
       "ABT_Adj Close        0.822624        0.935783       0.921596        0.893397   \n",
       "ACN_Adj Close        0.902944        0.886435       0.933729        0.923211   \n",
       "ADBE_Adj Close       0.000000        0.000000       0.000000        0.000000   \n",
       "AMD_Adj Close        0.869876        0.894577       0.894111        0.910014   \n",
       "AMGN_Adj Close       0.778392        0.000000       0.000000        0.000000   \n",
       "AMZN_Adj Close       0.000000        0.000000       0.000000        0.000000   \n",
       "AVGO_Adj Close       0.847971        0.000000       0.000000        0.000000   \n",
       "BAC_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "BRK-B_Adj Close      0.879582        0.000000       0.797096        0.763251   \n",
       "CMCSA_Adj Close      0.000000        0.000000       0.000000        0.000000   \n",
       "COST_Adj Close       0.941243        0.775030       0.902537        0.853934   \n",
       "CRM_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "CSCO_Adj Close       0.000000        0.000000       0.000000        0.000000   \n",
       "CVX_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "DHR_Adj Close        0.862556        0.913387       0.974442        0.917447   \n",
       "DIS_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "GOOG_Adj Close       0.842264        0.845909       0.880250        0.913009   \n",
       "GOOGL_Adj Close      0.844452        0.846601       0.882142        0.915167   \n",
       "HD_Adj Close         0.901877        0.891152       0.943013        0.885199   \n",
       "IBM_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "INTC_Adj Close       0.000000        0.000000       0.000000        0.000000   \n",
       "INTU_Adj Close       0.833807        0.840596       0.883103        0.891322   \n",
       "JNJ_Adj Close        0.871326        0.794334       0.909583        0.843047   \n",
       "JPM_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "KO_Adj Close         0.871885        0.000000       0.783396        0.000000   \n",
       "LIN_Adj Close        0.925852        0.000000       0.855927        0.785087   \n",
       "LLY_Adj Close        0.776354        0.000000       0.000000        0.000000   \n",
       "MA_Adj Close         0.901842        0.772651       0.816899        0.756898   \n",
       "MCD_Adj Close        0.899690        0.000000       0.820976        0.000000   \n",
       "META_Adj Close       0.000000        0.000000       0.000000        0.000000   \n",
       "MRK_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "MSFT_Adj Close       0.923173        0.814407       0.895576        0.882351   \n",
       "NFLX_Adj Close       0.000000        0.000000       0.000000        0.000000   \n",
       "NKE_Adj Close        0.000000        0.858636       0.777001        0.763400   \n",
       "NVDA_Adj Close       0.813992        0.000000       0.000000        0.000000   \n",
       "ORCL_Adj Close       0.824784        0.000000       0.000000        0.000000   \n",
       "PEP_Adj Close        0.917484        0.000000       0.845008        0.000000   \n",
       "PFE_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "PG_Adj Close         1.000000        0.819395       0.906964        0.818840   \n",
       "QCOM_Adj Close       0.819395        1.000000       0.909368        0.917720   \n",
       "TMO_Adj Close        0.906964        0.909368       1.000000        0.920203   \n",
       "TSLA_Adj Close       0.818840        0.917720       0.920203        1.000000   \n",
       "UNH_Adj Close        0.889641        0.000000       0.879827        0.815430   \n",
       "V_Adj Close          0.869922        0.000000       0.793966        0.000000   \n",
       "VZ_Adj Close         0.000000        0.000000       0.000000        0.000000   \n",
       "WFC_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "WMT_Adj Close        0.917826        0.000000       0.832604        0.780372   \n",
       "XOM_Adj Close        0.000000        0.000000       0.000000        0.000000   \n",
       "\n",
       "                 UNH_Adj Close  V_Adj Close  VZ_Adj Close  WFC_Adj Close   \n",
       "AAPL_Adj Close        0.923524     0.878010      0.000000            0.0  \\\n",
       "ABBV_Adj Close        0.968426     0.000000      0.000000            0.0   \n",
       "ABT_Adj Close         0.000000     0.777723      0.000000            0.0   \n",
       "ACN_Adj Close         0.859254     0.846789      0.000000            0.0   \n",
       "ADBE_Adj Close        0.000000     0.000000      0.000000            0.0   \n",
       "AMD_Adj Close         0.757026     0.809585      0.000000            0.0   \n",
       "AMGN_Adj Close        0.811358     0.757692      0.000000            0.0   \n",
       "AMZN_Adj Close        0.000000     0.000000      0.000000            0.0   \n",
       "AVGO_Adj Close        0.854421     0.806828      0.000000            0.0   \n",
       "BAC_Adj Close         0.000000     0.000000      0.000000            0.0   \n",
       "BRK-B_Adj Close       0.917704     0.804116      0.000000            0.0   \n",
       "CMCSA_Adj Close       0.000000     0.000000      0.000000            0.0   \n",
       "COST_Adj Close        0.959133     0.803872      0.000000            0.0   \n",
       "CRM_Adj Close         0.000000     0.000000      0.000000            0.0   \n",
       "CSCO_Adj Close        0.000000     0.000000      0.000000            0.0   \n",
       "CVX_Adj Close         0.816051     0.000000      0.773236            0.0   \n",
       "DHR_Adj Close         0.839015     0.796897      0.000000            0.0   \n",
       "DIS_Adj Close         0.000000     0.000000      0.000000            0.0   \n",
       "GOOG_Adj Close        0.813291     0.832234      0.000000            0.0   \n",
       "GOOGL_Adj Close       0.814376     0.827947      0.000000            0.0   \n",
       "HD_Adj Close          0.832119     0.838470      0.000000            0.0   \n",
       "IBM_Adj Close         0.804815     0.000000      0.000000            0.0   \n",
       "INTC_Adj Close        0.000000     0.000000      0.797748            0.0   \n",
       "INTU_Adj Close        0.790701     0.813458      0.000000            0.0   \n",
       "JNJ_Adj Close         0.920262     0.782555      0.000000            0.0   \n",
       "JPM_Adj Close         0.000000     0.821689      0.000000            0.0   \n",
       "KO_Adj Close          0.899535     0.000000      0.000000            0.0   \n",
       "LIN_Adj Close         0.916371     0.886497      0.000000            0.0   \n",
       "LLY_Adj Close         0.859824     0.000000      0.803080            0.0   \n",
       "MA_Adj Close          0.777947     0.982893      0.000000            0.0   \n",
       "MCD_Adj Close         0.910259     0.811878      0.000000            0.0   \n",
       "META_Adj Close        0.000000     0.000000      0.000000            0.0   \n",
       "MRK_Adj Close         0.761974     0.000000      0.834689            0.0   \n",
       "MSFT_Adj Close        0.876471     0.891767      0.000000            0.0   \n",
       "NFLX_Adj Close        0.000000     0.000000      0.000000            0.0   \n",
       "NKE_Adj Close         0.000000     0.000000      0.000000            0.0   \n",
       "NVDA_Adj Close        0.769826     0.799963      0.000000            0.0   \n",
       "ORCL_Adj Close        0.814599     0.838320      0.000000            0.0   \n",
       "PEP_Adj Close         0.937318     0.782094      0.000000            0.0   \n",
       "PFE_Adj Close         0.000000     0.000000      0.000000            0.0   \n",
       "PG_Adj Close          0.889641     0.869922      0.000000            0.0   \n",
       "QCOM_Adj Close        0.000000     0.000000      0.000000            0.0   \n",
       "TMO_Adj Close         0.879827     0.793966      0.000000            0.0   \n",
       "TSLA_Adj Close        0.815430     0.000000      0.000000            0.0   \n",
       "UNH_Adj Close         1.000000     0.755763      0.000000            0.0   \n",
       "V_Adj Close           0.755763     1.000000      0.000000            0.0   \n",
       "VZ_Adj Close          0.000000     0.000000      1.000000            0.0   \n",
       "WFC_Adj Close         0.000000     0.000000      0.000000            1.0   \n",
       "WMT_Adj Close         0.811382     0.894885      0.000000            0.0   \n",
       "XOM_Adj Close         0.762958     0.000000      0.852376            0.0   \n",
       "\n",
       "                 WMT_Adj Close  XOM_Adj Close  \n",
       "AAPL_Adj Close        0.918141       0.000000  \n",
       "ABBV_Adj Close        0.789635       0.814668  \n",
       "ABT_Adj Close         0.770920       0.000000  \n",
       "ACN_Adj Close         0.835145       0.000000  \n",
       "ADBE_Adj Close        0.000000       0.000000  \n",
       "AMD_Adj Close         0.835794       0.000000  \n",
       "AMGN_Adj Close        0.807236       0.000000  \n",
       "AMZN_Adj Close        0.000000       0.000000  \n",
       "AVGO_Adj Close        0.846596       0.760602  \n",
       "BAC_Adj Close         0.000000       0.000000  \n",
       "BRK-B_Adj Close       0.816245       0.801920  \n",
       "CMCSA_Adj Close       0.000000       0.000000  \n",
       "COST_Adj Close        0.873344       0.000000  \n",
       "CRM_Adj Close         0.000000       0.000000  \n",
       "CSCO_Adj Close        0.000000       0.000000  \n",
       "CVX_Adj Close         0.000000       0.970460  \n",
       "DHR_Adj Close         0.804135       0.000000  \n",
       "DIS_Adj Close         0.000000       0.000000  \n",
       "GOOG_Adj Close        0.801316       0.000000  \n",
       "GOOGL_Adj Close       0.800575       0.000000  \n",
       "HD_Adj Close          0.837979       0.000000  \n",
       "IBM_Adj Close         0.000000       0.828643  \n",
       "INTC_Adj Close        0.000000       0.776011  \n",
       "INTU_Adj Close        0.795016       0.000000  \n",
       "JNJ_Adj Close         0.798881       0.000000  \n",
       "JPM_Adj Close         0.000000       0.000000  \n",
       "KO_Adj Close          0.000000       0.795676  \n",
       "LIN_Adj Close         0.907378       0.000000  \n",
       "LLY_Adj Close         0.792056       0.837922  \n",
       "MA_Adj Close          0.905374       0.000000  \n",
       "MCD_Adj Close         0.829969       0.810765  \n",
       "META_Adj Close        0.000000       0.000000  \n",
       "MRK_Adj Close         0.000000       0.901374  \n",
       "MSFT_Adj Close        0.903860       0.000000  \n",
       "NFLX_Adj Close        0.000000       0.000000  \n",
       "NKE_Adj Close         0.000000       0.000000  \n",
       "NVDA_Adj Close        0.840907       0.000000  \n",
       "ORCL_Adj Close        0.840071       0.000000  \n",
       "PEP_Adj Close         0.831563       0.801176  \n",
       "PFE_Adj Close         0.000000       0.000000  \n",
       "PG_Adj Close          0.917826       0.000000  \n",
       "QCOM_Adj Close        0.000000       0.000000  \n",
       "TMO_Adj Close         0.832604       0.000000  \n",
       "TSLA_Adj Close        0.780372       0.000000  \n",
       "UNH_Adj Close         0.811382       0.762958  \n",
       "V_Adj Close           0.894885       0.000000  \n",
       "VZ_Adj Close          0.000000       0.852376  \n",
       "WFC_Adj Close         0.000000       0.000000  \n",
       "WMT_Adj Close         1.000000       0.000000  \n",
       "XOM_Adj Close         0.000000       1.000000  \n",
       "\n",
       "[50 rows x 50 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute correlation matrix\n",
    "def compute_correlation_matrix(df):\n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = df.corr().abs()\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "correlation_matrix = compute_correlation_matrix(normalized_df)\n",
    "correlation_matrix[correlation_matrix<=0.75] = 0\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# x = torch.tensor(merged_df.iloc[:260,:].values, dtype=torch.float32).T\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnormalized_df\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m260\u001b[39m,:]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      3\u001b[0m A \u001b[38;5;241m=\u001b[39m correlation_matrix\n\u001b[1;32m      4\u001b[0m G \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcoo_matrix(A)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_df' is not defined"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor(merged_df.iloc[:260,:].values, dtype=torch.float32).T\n",
    "x = torch.tensor(normalized_df.iloc[:260,:].values, dtype=torch.float32).T\n",
    "A = correlation_matrix\n",
    "G = sp.coo_matrix(A)\n",
    "edge_index = torch.tensor(np.array([G.row, G.col]), dtype=torch.int64)\n",
    "edge_weight = torch.tensor(G.data).float()\n",
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>1.285640</td>\n",
       "      <td>0.890150</td>\n",
       "      <td>2.104350</td>\n",
       "      <td>0.282281</td>\n",
       "      <td>2.865539</td>\n",
       "      <td>2.772218</td>\n",
       "      <td>1.430447</td>\n",
       "      <td>2.395973</td>\n",
       "      <td>1.108006</td>\n",
       "      <td>0.220242</td>\n",
       "      <td>1.199275</td>\n",
       "      <td>1.078785</td>\n",
       "      <td>2.507982</td>\n",
       "      <td>2.368880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>1.420608</td>\n",
       "      <td>0.801318</td>\n",
       "      <td>2.307554</td>\n",
       "      <td>0.332392</td>\n",
       "      <td>3.178218</td>\n",
       "      <td>3.023480</td>\n",
       "      <td>1.668502</td>\n",
       "      <td>2.622850</td>\n",
       "      <td>1.303321</td>\n",
       "      <td>0.191006</td>\n",
       "      <td>1.419897</td>\n",
       "      <td>1.161971</td>\n",
       "      <td>2.693352</td>\n",
       "      <td>2.609343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>1.550428</td>\n",
       "      <td>0.858722</td>\n",
       "      <td>2.547626</td>\n",
       "      <td>0.392113</td>\n",
       "      <td>3.432771</td>\n",
       "      <td>3.261022</td>\n",
       "      <td>1.759329</td>\n",
       "      <td>2.822245</td>\n",
       "      <td>1.450016</td>\n",
       "      <td>0.172333</td>\n",
       "      <td>1.544350</td>\n",
       "      <td>1.243864</td>\n",
       "      <td>2.920743</td>\n",
       "      <td>2.852250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>1.629912</td>\n",
       "      <td>1.081102</td>\n",
       "      <td>2.854746</td>\n",
       "      <td>0.427096</td>\n",
       "      <td>3.720430</td>\n",
       "      <td>3.477132</td>\n",
       "      <td>1.354926</td>\n",
       "      <td>3.057925</td>\n",
       "      <td>1.684024</td>\n",
       "      <td>0.216958</td>\n",
       "      <td>1.719716</td>\n",
       "      <td>1.465611</td>\n",
       "      <td>3.200760</td>\n",
       "      <td>3.101573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>1.817022</td>\n",
       "      <td>1.188246</td>\n",
       "      <td>3.068851</td>\n",
       "      <td>0.405727</td>\n",
       "      <td>4.022238</td>\n",
       "      <td>3.622661</td>\n",
       "      <td>1.481052</td>\n",
       "      <td>3.368399</td>\n",
       "      <td>1.973449</td>\n",
       "      <td>0.268136</td>\n",
       "      <td>1.878111</td>\n",
       "      <td>1.626147</td>\n",
       "      <td>3.384166</td>\n",
       "      <td>3.452949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>1.952592</td>\n",
       "      <td>1.296958</td>\n",
       "      <td>3.356795</td>\n",
       "      <td>0.394397</td>\n",
       "      <td>4.387786</td>\n",
       "      <td>3.451100</td>\n",
       "      <td>1.568214</td>\n",
       "      <td>3.651092</td>\n",
       "      <td>2.337254</td>\n",
       "      <td>0.337904</td>\n",
       "      <td>2.076104</td>\n",
       "      <td>1.819021</td>\n",
       "      <td>3.701493</td>\n",
       "      <td>3.737410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>2.189504</td>\n",
       "      <td>1.420849</td>\n",
       "      <td>3.698781</td>\n",
       "      <td>0.443717</td>\n",
       "      <td>4.797459</td>\n",
       "      <td>3.729878</td>\n",
       "      <td>1.726631</td>\n",
       "      <td>4.030675</td>\n",
       "      <td>2.745444</td>\n",
       "      <td>0.424015</td>\n",
       "      <td>2.296726</td>\n",
       "      <td>2.023444</td>\n",
       "      <td>4.005467</td>\n",
       "      <td>4.202753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>2.467061</td>\n",
       "      <td>1.617933</td>\n",
       "      <td>4.121442</td>\n",
       "      <td>0.515514</td>\n",
       "      <td>5.019013</td>\n",
       "      <td>4.185208</td>\n",
       "      <td>1.844042</td>\n",
       "      <td>4.707827</td>\n",
       "      <td>3.389619</td>\n",
       "      <td>0.497252</td>\n",
       "      <td>2.511690</td>\n",
       "      <td>1.810989</td>\n",
       "      <td>4.215197</td>\n",
       "      <td>4.630820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>2.698827</td>\n",
       "      <td>1.868001</td>\n",
       "      <td>4.654807</td>\n",
       "      <td>0.555585</td>\n",
       "      <td>5.611000</td>\n",
       "      <td>4.743236</td>\n",
       "      <td>1.989649</td>\n",
       "      <td>5.176015</td>\n",
       "      <td>3.828729</td>\n",
       "      <td>0.546866</td>\n",
       "      <td>2.771910</td>\n",
       "      <td>1.703036</td>\n",
       "      <td>4.574731</td>\n",
       "      <td>5.109364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>3.106212</td>\n",
       "      <td>2.233615</td>\n",
       "      <td>5.302445</td>\n",
       "      <td>0.632902</td>\n",
       "      <td>6.879731</td>\n",
       "      <td>5.443400</td>\n",
       "      <td>2.111117</td>\n",
       "      <td>6.030753</td>\n",
       "      <td>5.070358</td>\n",
       "      <td>0.599812</td>\n",
       "      <td>3.196177</td>\n",
       "      <td>2.133130</td>\n",
       "      <td>5.023466</td>\n",
       "      <td>5.350489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>3.809923</td>\n",
       "      <td>3.195232</td>\n",
       "      <td>6.157733</td>\n",
       "      <td>0.771263</td>\n",
       "      <td>8.939845</td>\n",
       "      <td>6.166626</td>\n",
       "      <td>2.526233</td>\n",
       "      <td>7.285893</td>\n",
       "      <td>6.888676</td>\n",
       "      <td>0.766255</td>\n",
       "      <td>3.908974</td>\n",
       "      <td>2.685781</td>\n",
       "      <td>5.597898</td>\n",
       "      <td>7.259451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>5.309950</td>\n",
       "      <td>4.196787</td>\n",
       "      <td>7.521418</td>\n",
       "      <td>0.802692</td>\n",
       "      <td>9.653955</td>\n",
       "      <td>6.602199</td>\n",
       "      <td>2.940126</td>\n",
       "      <td>8.285307</td>\n",
       "      <td>7.646655</td>\n",
       "      <td>1.079214</td>\n",
       "      <td>5.091276</td>\n",
       "      <td>3.716824</td>\n",
       "      <td>6.068653</td>\n",
       "      <td>9.033006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>5.807765</td>\n",
       "      <td>4.938678</td>\n",
       "      <td>8.151392</td>\n",
       "      <td>0.909803</td>\n",
       "      <td>12.199015</td>\n",
       "      <td>7.741750</td>\n",
       "      <td>2.909016</td>\n",
       "      <td>9.453235</td>\n",
       "      <td>8.314919</td>\n",
       "      <td>1.202917</td>\n",
       "      <td>6.222656</td>\n",
       "      <td>4.660002</td>\n",
       "      <td>6.617146</td>\n",
       "      <td>9.364244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>6.270710</td>\n",
       "      <td>5.854732</td>\n",
       "      <td>9.686036</td>\n",
       "      <td>0.856971</td>\n",
       "      <td>12.587362</td>\n",
       "      <td>7.449541</td>\n",
       "      <td>3.034400</td>\n",
       "      <td>9.329574</td>\n",
       "      <td>9.345152</td>\n",
       "      <td>1.651199</td>\n",
       "      <td>6.284655</td>\n",
       "      <td>5.353928</td>\n",
       "      <td>7.357476</td>\n",
       "      <td>8.980903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>6.586473</td>\n",
       "      <td>6.740579</td>\n",
       "      <td>9.922479</td>\n",
       "      <td>0.973859</td>\n",
       "      <td>13.870729</td>\n",
       "      <td>8.425331</td>\n",
       "      <td>3.588875</td>\n",
       "      <td>10.694611</td>\n",
       "      <td>11.501435</td>\n",
       "      <td>2.122994</td>\n",
       "      <td>5.792196</td>\n",
       "      <td>6.126129</td>\n",
       "      <td>8.175983</td>\n",
       "      <td>9.892721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>7.072356</td>\n",
       "      <td>7.655453</td>\n",
       "      <td>10.252047</td>\n",
       "      <td>0.832485</td>\n",
       "      <td>17.130779</td>\n",
       "      <td>10.756835</td>\n",
       "      <td>4.056031</td>\n",
       "      <td>13.080257</td>\n",
       "      <td>16.159975</td>\n",
       "      <td>2.869872</td>\n",
       "      <td>7.258395</td>\n",
       "      <td>6.801696</td>\n",
       "      <td>9.235466</td>\n",
       "      <td>11.263085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>8.051619</td>\n",
       "      <td>8.460434</td>\n",
       "      <td>11.397395</td>\n",
       "      <td>0.992478</td>\n",
       "      <td>20.756527</td>\n",
       "      <td>14.059708</td>\n",
       "      <td>4.519574</td>\n",
       "      <td>16.344271</td>\n",
       "      <td>16.820011</td>\n",
       "      <td>3.696771</td>\n",
       "      <td>9.512859</td>\n",
       "      <td>9.333132</td>\n",
       "      <td>10.318359</td>\n",
       "      <td>13.757913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>8.952055</td>\n",
       "      <td>9.074131</td>\n",
       "      <td>12.840692</td>\n",
       "      <td>1.064117</td>\n",
       "      <td>23.709158</td>\n",
       "      <td>18.093126</td>\n",
       "      <td>5.504295</td>\n",
       "      <td>19.814241</td>\n",
       "      <td>17.623116</td>\n",
       "      <td>3.611278</td>\n",
       "      <td>14.536759</td>\n",
       "      <td>7.182010</td>\n",
       "      <td>11.221540</td>\n",
       "      <td>19.513748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>10.558072</td>\n",
       "      <td>9.862375</td>\n",
       "      <td>14.358356</td>\n",
       "      <td>1.090378</td>\n",
       "      <td>20.810589</td>\n",
       "      <td>17.317026</td>\n",
       "      <td>5.715989</td>\n",
       "      <td>17.881466</td>\n",
       "      <td>19.434282</td>\n",
       "      <td>4.027340</td>\n",
       "      <td>18.653975</td>\n",
       "      <td>7.417035</td>\n",
       "      <td>12.595055</td>\n",
       "      <td>20.327715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>11.581422</td>\n",
       "      <td>10.370713</td>\n",
       "      <td>14.703852</td>\n",
       "      <td>1.141723</td>\n",
       "      <td>19.773546</td>\n",
       "      <td>16.493515</td>\n",
       "      <td>5.929403</td>\n",
       "      <td>17.739054</td>\n",
       "      <td>18.087572</td>\n",
       "      <td>4.326934</td>\n",
       "      <td>13.053688</td>\n",
       "      <td>6.739010</td>\n",
       "      <td>13.132107</td>\n",
       "      <td>18.748182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>10.580249</td>\n",
       "      <td>7.249433</td>\n",
       "      <td>15.976101</td>\n",
       "      <td>1.284221</td>\n",
       "      <td>18.928057</td>\n",
       "      <td>15.679834</td>\n",
       "      <td>6.447750</td>\n",
       "      <td>18.393764</td>\n",
       "      <td>19.822254</td>\n",
       "      <td>4.846080</td>\n",
       "      <td>11.042919</td>\n",
       "      <td>6.439582</td>\n",
       "      <td>14.272006</td>\n",
       "      <td>20.996119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>11.550381</td>\n",
       "      <td>7.199110</td>\n",
       "      <td>16.670773</td>\n",
       "      <td>1.447108</td>\n",
       "      <td>17.941357</td>\n",
       "      <td>14.779372</td>\n",
       "      <td>6.267415</td>\n",
       "      <td>18.179756</td>\n",
       "      <td>21.018898</td>\n",
       "      <td>5.384462</td>\n",
       "      <td>13.027356</td>\n",
       "      <td>6.263346</td>\n",
       "      <td>15.856971</td>\n",
       "      <td>18.522530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>10.781475</td>\n",
       "      <td>8.060674</td>\n",
       "      <td>17.112682</td>\n",
       "      <td>1.722903</td>\n",
       "      <td>18.700504</td>\n",
       "      <td>15.668835</td>\n",
       "      <td>6.868697</td>\n",
       "      <td>18.774687</td>\n",
       "      <td>22.302481</td>\n",
       "      <td>5.593520</td>\n",
       "      <td>13.805908</td>\n",
       "      <td>7.019733</td>\n",
       "      <td>17.040529</td>\n",
       "      <td>14.067972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>10.891129</td>\n",
       "      <td>9.803727</td>\n",
       "      <td>17.712294</td>\n",
       "      <td>1.674304</td>\n",
       "      <td>26.081884</td>\n",
       "      <td>19.262091</td>\n",
       "      <td>7.355373</td>\n",
       "      <td>26.586882</td>\n",
       "      <td>33.144653</td>\n",
       "      <td>6.451665</td>\n",
       "      <td>9.514732</td>\n",
       "      <td>7.906441</td>\n",
       "      <td>17.985645</td>\n",
       "      <td>16.009354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>11.309879</td>\n",
       "      <td>10.819568</td>\n",
       "      <td>20.241697</td>\n",
       "      <td>1.519626</td>\n",
       "      <td>31.582531</td>\n",
       "      <td>23.863411</td>\n",
       "      <td>8.243018</td>\n",
       "      <td>33.450745</td>\n",
       "      <td>40.380455</td>\n",
       "      <td>8.169622</td>\n",
       "      <td>10.433032</td>\n",
       "      <td>9.100897</td>\n",
       "      <td>19.067951</td>\n",
       "      <td>21.068497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>14.095135</td>\n",
       "      <td>11.768482</td>\n",
       "      <td>23.804567</td>\n",
       "      <td>1.738856</td>\n",
       "      <td>34.445183</td>\n",
       "      <td>29.148975</td>\n",
       "      <td>8.761649</td>\n",
       "      <td>37.016876</td>\n",
       "      <td>48.971714</td>\n",
       "      <td>11.021297</td>\n",
       "      <td>12.842088</td>\n",
       "      <td>9.485611</td>\n",
       "      <td>20.565132</td>\n",
       "      <td>22.692486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>17.898561</td>\n",
       "      <td>15.786166</td>\n",
       "      <td>26.507839</td>\n",
       "      <td>1.936006</td>\n",
       "      <td>34.660358</td>\n",
       "      <td>29.685572</td>\n",
       "      <td>8.745471</td>\n",
       "      <td>38.555180</td>\n",
       "      <td>48.704353</td>\n",
       "      <td>13.635191</td>\n",
       "      <td>15.655859</td>\n",
       "      <td>11.186418</td>\n",
       "      <td>22.156252</td>\n",
       "      <td>23.582655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>18.588226</td>\n",
       "      <td>17.773748</td>\n",
       "      <td>27.856710</td>\n",
       "      <td>2.008877</td>\n",
       "      <td>42.908424</td>\n",
       "      <td>35.011066</td>\n",
       "      <td>9.482141</td>\n",
       "      <td>49.040749</td>\n",
       "      <td>49.946369</td>\n",
       "      <td>15.647401</td>\n",
       "      <td>18.473768</td>\n",
       "      <td>15.731504</td>\n",
       "      <td>23.419134</td>\n",
       "      <td>27.509447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>19.456728</td>\n",
       "      <td>15.585039</td>\n",
       "      <td>28.624285</td>\n",
       "      <td>2.134219</td>\n",
       "      <td>42.911713</td>\n",
       "      <td>36.598030</td>\n",
       "      <td>7.979266</td>\n",
       "      <td>51.739246</td>\n",
       "      <td>57.146263</td>\n",
       "      <td>18.258232</td>\n",
       "      <td>22.142731</td>\n",
       "      <td>15.769633</td>\n",
       "      <td>24.184902</td>\n",
       "      <td>29.507607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>19.429914</td>\n",
       "      <td>14.929020</td>\n",
       "      <td>27.776659</td>\n",
       "      <td>2.376617</td>\n",
       "      <td>47.380760</td>\n",
       "      <td>37.781281</td>\n",
       "      <td>8.514045</td>\n",
       "      <td>54.809071</td>\n",
       "      <td>62.317955</td>\n",
       "      <td>19.631912</td>\n",
       "      <td>25.679611</td>\n",
       "      <td>16.610476</td>\n",
       "      <td>25.607365</td>\n",
       "      <td>32.072525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>18.630455</td>\n",
       "      <td>16.753565</td>\n",
       "      <td>27.060854</td>\n",
       "      <td>2.475795</td>\n",
       "      <td>44.721748</td>\n",
       "      <td>33.994061</td>\n",
       "      <td>8.250760</td>\n",
       "      <td>44.213802</td>\n",
       "      <td>71.012222</td>\n",
       "      <td>21.682810</td>\n",
       "      <td>35.407856</td>\n",
       "      <td>18.837132</td>\n",
       "      <td>26.935709</td>\n",
       "      <td>32.124603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>19.267387</td>\n",
       "      <td>20.879093</td>\n",
       "      <td>27.102228</td>\n",
       "      <td>3.141549</td>\n",
       "      <td>47.127769</td>\n",
       "      <td>36.523636</td>\n",
       "      <td>9.668151</td>\n",
       "      <td>45.636116</td>\n",
       "      <td>79.695602</td>\n",
       "      <td>25.600702</td>\n",
       "      <td>37.322544</td>\n",
       "      <td>13.640844</td>\n",
       "      <td>28.619259</td>\n",
       "      <td>33.503464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>21.973061</td>\n",
       "      <td>29.407007</td>\n",
       "      <td>28.317118</td>\n",
       "      <td>4.088838</td>\n",
       "      <td>54.129810</td>\n",
       "      <td>43.114830</td>\n",
       "      <td>10.643201</td>\n",
       "      <td>48.768375</td>\n",
       "      <td>88.412689</td>\n",
       "      <td>31.286430</td>\n",
       "      <td>25.461447</td>\n",
       "      <td>17.677963</td>\n",
       "      <td>30.003687</td>\n",
       "      <td>37.480606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>23.952869</td>\n",
       "      <td>32.506710</td>\n",
       "      <td>29.469398</td>\n",
       "      <td>4.808445</td>\n",
       "      <td>54.284660</td>\n",
       "      <td>45.531494</td>\n",
       "      <td>11.606688</td>\n",
       "      <td>54.487930</td>\n",
       "      <td>78.493439</td>\n",
       "      <td>33.693233</td>\n",
       "      <td>29.060719</td>\n",
       "      <td>18.946005</td>\n",
       "      <td>31.705681</td>\n",
       "      <td>35.625294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>26.000294</td>\n",
       "      <td>33.759693</td>\n",
       "      <td>30.609203</td>\n",
       "      <td>5.353187</td>\n",
       "      <td>49.119133</td>\n",
       "      <td>50.015213</td>\n",
       "      <td>12.285275</td>\n",
       "      <td>51.559044</td>\n",
       "      <td>71.622925</td>\n",
       "      <td>31.461536</td>\n",
       "      <td>35.385414</td>\n",
       "      <td>19.824451</td>\n",
       "      <td>33.686741</td>\n",
       "      <td>36.878712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>23.854387</td>\n",
       "      <td>33.014500</td>\n",
       "      <td>29.628277</td>\n",
       "      <td>5.728725</td>\n",
       "      <td>50.817101</td>\n",
       "      <td>53.002625</td>\n",
       "      <td>12.447271</td>\n",
       "      <td>52.728695</td>\n",
       "      <td>65.340012</td>\n",
       "      <td>21.167326</td>\n",
       "      <td>37.229847</td>\n",
       "      <td>28.809982</td>\n",
       "      <td>35.592525</td>\n",
       "      <td>33.387856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>23.241333</td>\n",
       "      <td>22.920727</td>\n",
       "      <td>31.703773</td>\n",
       "      <td>6.090297</td>\n",
       "      <td>50.480473</td>\n",
       "      <td>54.104668</td>\n",
       "      <td>13.554157</td>\n",
       "      <td>51.997749</td>\n",
       "      <td>73.911247</td>\n",
       "      <td>27.472380</td>\n",
       "      <td>42.443676</td>\n",
       "      <td>26.769289</td>\n",
       "      <td>37.824638</td>\n",
       "      <td>33.067902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>24.820362</td>\n",
       "      <td>25.053860</td>\n",
       "      <td>34.805099</td>\n",
       "      <td>6.743418</td>\n",
       "      <td>46.169548</td>\n",
       "      <td>53.356583</td>\n",
       "      <td>13.837002</td>\n",
       "      <td>47.606514</td>\n",
       "      <td>79.210320</td>\n",
       "      <td>31.816244</td>\n",
       "      <td>50.057720</td>\n",
       "      <td>28.638039</td>\n",
       "      <td>40.258686</td>\n",
       "      <td>33.119362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>22.642569</td>\n",
       "      <td>21.404823</td>\n",
       "      <td>34.533550</td>\n",
       "      <td>7.456372</td>\n",
       "      <td>46.575840</td>\n",
       "      <td>52.803062</td>\n",
       "      <td>14.340565</td>\n",
       "      <td>48.492760</td>\n",
       "      <td>69.745804</td>\n",
       "      <td>30.241390</td>\n",
       "      <td>53.507973</td>\n",
       "      <td>21.064243</td>\n",
       "      <td>41.558556</td>\n",
       "      <td>29.556974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>23.610397</td>\n",
       "      <td>19.486420</td>\n",
       "      <td>35.547020</td>\n",
       "      <td>8.186515</td>\n",
       "      <td>50.759651</td>\n",
       "      <td>57.191116</td>\n",
       "      <td>15.211943</td>\n",
       "      <td>53.007553</td>\n",
       "      <td>66.686920</td>\n",
       "      <td>34.636189</td>\n",
       "      <td>54.597420</td>\n",
       "      <td>25.083447</td>\n",
       "      <td>42.922050</td>\n",
       "      <td>28.172785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>27.903351</td>\n",
       "      <td>21.337933</td>\n",
       "      <td>41.850822</td>\n",
       "      <td>9.242693</td>\n",
       "      <td>62.360371</td>\n",
       "      <td>65.867653</td>\n",
       "      <td>17.952234</td>\n",
       "      <td>65.498024</td>\n",
       "      <td>72.055191</td>\n",
       "      <td>38.803669</td>\n",
       "      <td>51.572735</td>\n",
       "      <td>32.845673</td>\n",
       "      <td>44.993038</td>\n",
       "      <td>42.998512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>36.667038</td>\n",
       "      <td>25.582922</td>\n",
       "      <td>47.979767</td>\n",
       "      <td>10.885310</td>\n",
       "      <td>71.660561</td>\n",
       "      <td>77.600082</td>\n",
       "      <td>20.949179</td>\n",
       "      <td>75.002136</td>\n",
       "      <td>78.010727</td>\n",
       "      <td>43.798695</td>\n",
       "      <td>55.313957</td>\n",
       "      <td>42.688011</td>\n",
       "      <td>47.980747</td>\n",
       "      <td>55.828728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>41.500374</td>\n",
       "      <td>34.081821</td>\n",
       "      <td>54.822254</td>\n",
       "      <td>12.725822</td>\n",
       "      <td>74.274323</td>\n",
       "      <td>81.499733</td>\n",
       "      <td>24.235149</td>\n",
       "      <td>77.147491</td>\n",
       "      <td>77.027855</td>\n",
       "      <td>51.624733</td>\n",
       "      <td>62.048141</td>\n",
       "      <td>52.862312</td>\n",
       "      <td>51.209023</td>\n",
       "      <td>63.043983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>44.640030</td>\n",
       "      <td>42.337936</td>\n",
       "      <td>61.652500</td>\n",
       "      <td>15.320895</td>\n",
       "      <td>78.452690</td>\n",
       "      <td>86.789284</td>\n",
       "      <td>27.776512</td>\n",
       "      <td>80.939423</td>\n",
       "      <td>73.364105</td>\n",
       "      <td>58.158066</td>\n",
       "      <td>68.971306</td>\n",
       "      <td>58.162098</td>\n",
       "      <td>54.258137</td>\n",
       "      <td>66.315804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>50.997837</td>\n",
       "      <td>53.403313</td>\n",
       "      <td>68.641594</td>\n",
       "      <td>19.764481</td>\n",
       "      <td>89.949265</td>\n",
       "      <td>99.055756</td>\n",
       "      <td>35.943993</td>\n",
       "      <td>91.881248</td>\n",
       "      <td>73.014740</td>\n",
       "      <td>64.751122</td>\n",
       "      <td>74.438316</td>\n",
       "      <td>71.134041</td>\n",
       "      <td>56.844826</td>\n",
       "      <td>72.692596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>63.010288</td>\n",
       "      <td>64.822395</td>\n",
       "      <td>72.575035</td>\n",
       "      <td>25.576424</td>\n",
       "      <td>99.067711</td>\n",
       "      <td>93.889580</td>\n",
       "      <td>35.416939</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>81.415558</td>\n",
       "      <td>57.833492</td>\n",
       "      <td>78.489426</td>\n",
       "      <td>80.439560</td>\n",
       "      <td>58.005859</td>\n",
       "      <td>68.994232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>55.426727</td>\n",
       "      <td>63.719284</td>\n",
       "      <td>64.239624</td>\n",
       "      <td>28.400839</td>\n",
       "      <td>91.311607</td>\n",
       "      <td>77.424530</td>\n",
       "      <td>39.641132</td>\n",
       "      <td>91.334312</td>\n",
       "      <td>84.330162</td>\n",
       "      <td>52.123959</td>\n",
       "      <td>63.644123</td>\n",
       "      <td>67.789742</td>\n",
       "      <td>56.859898</td>\n",
       "      <td>71.967354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>68.554184</td>\n",
       "      <td>84.430656</td>\n",
       "      <td>75.582443</td>\n",
       "      <td>33.887066</td>\n",
       "      <td>89.428513</td>\n",
       "      <td>79.789055</td>\n",
       "      <td>49.499882</td>\n",
       "      <td>88.684334</td>\n",
       "      <td>91.816620</td>\n",
       "      <td>63.174755</td>\n",
       "      <td>74.799194</td>\n",
       "      <td>81.120094</td>\n",
       "      <td>59.101997</td>\n",
       "      <td>91.087799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>83.469055</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>83.806572</td>\n",
       "      <td>42.039047</td>\n",
       "      <td>96.865250</td>\n",
       "      <td>85.393753</td>\n",
       "      <td>53.855347</td>\n",
       "      <td>95.281143</td>\n",
       "      <td>99.374786</td>\n",
       "      <td>69.202301</td>\n",
       "      <td>83.474609</td>\n",
       "      <td>87.574272</td>\n",
       "      <td>61.265022</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>92.332306</td>\n",
       "      <td>94.230911</td>\n",
       "      <td>85.444077</td>\n",
       "      <td>47.498219</td>\n",
       "      <td>90.729576</td>\n",
       "      <td>86.672798</td>\n",
       "      <td>53.990810</td>\n",
       "      <td>86.644096</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>70.594078</td>\n",
       "      <td>84.931740</td>\n",
       "      <td>91.935356</td>\n",
       "      <td>63.834442</td>\n",
       "      <td>94.805984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>94.085716</td>\n",
       "      <td>94.521072</td>\n",
       "      <td>86.296051</td>\n",
       "      <td>53.278290</td>\n",
       "      <td>95.063934</td>\n",
       "      <td>89.234039</td>\n",
       "      <td>54.849987</td>\n",
       "      <td>88.926140</td>\n",
       "      <td>83.099915</td>\n",
       "      <td>75.694565</td>\n",
       "      <td>90.118477</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.148491</td>\n",
       "      <td>87.491592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>87.595413</td>\n",
       "      <td>93.879845</td>\n",
       "      <td>84.387138</td>\n",
       "      <td>58.317238</td>\n",
       "      <td>96.554443</td>\n",
       "      <td>98.166290</td>\n",
       "      <td>60.238476</td>\n",
       "      <td>89.760025</td>\n",
       "      <td>78.072563</td>\n",
       "      <td>81.963249</td>\n",
       "      <td>93.011459</td>\n",
       "      <td>98.030426</td>\n",
       "      <td>68.927017</td>\n",
       "      <td>83.194962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>80.611511</td>\n",
       "      <td>68.887772</td>\n",
       "      <td>72.739494</td>\n",
       "      <td>61.579178</td>\n",
       "      <td>82.464096</td>\n",
       "      <td>93.991241</td>\n",
       "      <td>62.142761</td>\n",
       "      <td>76.251579</td>\n",
       "      <td>70.865326</td>\n",
       "      <td>80.939201</td>\n",
       "      <td>82.865273</td>\n",
       "      <td>90.239594</td>\n",
       "      <td>71.500748</td>\n",
       "      <td>75.667870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>72.015610</td>\n",
       "      <td>68.638596</td>\n",
       "      <td>71.406960</td>\n",
       "      <td>62.535252</td>\n",
       "      <td>83.605980</td>\n",
       "      <td>86.458817</td>\n",
       "      <td>67.791313</td>\n",
       "      <td>77.930275</td>\n",
       "      <td>79.773407</td>\n",
       "      <td>82.835350</td>\n",
       "      <td>76.262398</td>\n",
       "      <td>90.800156</td>\n",
       "      <td>73.421555</td>\n",
       "      <td>70.621094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>79.172302</td>\n",
       "      <td>78.875809</td>\n",
       "      <td>77.074249</td>\n",
       "      <td>68.531837</td>\n",
       "      <td>87.736870</td>\n",
       "      <td>85.940926</td>\n",
       "      <td>78.328033</td>\n",
       "      <td>81.447777</td>\n",
       "      <td>78.612122</td>\n",
       "      <td>89.670959</td>\n",
       "      <td>81.948982</td>\n",
       "      <td>89.683601</td>\n",
       "      <td>76.493607</td>\n",
       "      <td>83.249512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>85.248337</td>\n",
       "      <td>73.272896</td>\n",
       "      <td>80.627426</td>\n",
       "      <td>77.352203</td>\n",
       "      <td>94.356674</td>\n",
       "      <td>92.175186</td>\n",
       "      <td>79.848091</td>\n",
       "      <td>86.850632</td>\n",
       "      <td>80.366539</td>\n",
       "      <td>95.245041</td>\n",
       "      <td>86.438736</td>\n",
       "      <td>81.277672</td>\n",
       "      <td>80.639748</td>\n",
       "      <td>88.205879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>83.096756</td>\n",
       "      <td>71.604584</td>\n",
       "      <td>81.488579</td>\n",
       "      <td>79.495811</td>\n",
       "      <td>92.257652</td>\n",
       "      <td>91.499626</td>\n",
       "      <td>83.767532</td>\n",
       "      <td>83.503113</td>\n",
       "      <td>81.595947</td>\n",
       "      <td>91.190681</td>\n",
       "      <td>89.734200</td>\n",
       "      <td>79.341782</td>\n",
       "      <td>83.969788</td>\n",
       "      <td>84.795212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>79.200783</td>\n",
       "      <td>56.422741</td>\n",
       "      <td>76.996338</td>\n",
       "      <td>81.765869</td>\n",
       "      <td>89.219604</td>\n",
       "      <td>86.617332</td>\n",
       "      <td>78.922440</td>\n",
       "      <td>78.766373</td>\n",
       "      <td>80.492622</td>\n",
       "      <td>90.798065</td>\n",
       "      <td>77.112488</td>\n",
       "      <td>75.202499</td>\n",
       "      <td>82.711075</td>\n",
       "      <td>73.683968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>92.675529</td>\n",
       "      <td>63.055206</td>\n",
       "      <td>93.534409</td>\n",
       "      <td>99.205536</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>93.064201</td>\n",
       "      <td>87.781586</td>\n",
       "      <td>79.803047</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>90.004974</td>\n",
       "      <td>85.512146</td>\n",
       "      <td>91.565628</td>\n",
       "      <td>91.448303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>73.393753</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.084465</td>\n",
       "      <td>98.340660</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>83.466972</td>\n",
       "      <td>67.456894</td>\n",
       "      <td>91.953957</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.590599</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.579262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5    \n",
       "1963    1.285640    0.890150    2.104350    0.282281    2.865539    2.772218  \\\n",
       "1964    1.420608    0.801318    2.307554    0.332392    3.178218    3.023480   \n",
       "1965    1.550428    0.858722    2.547626    0.392113    3.432771    3.261022   \n",
       "1966    1.629912    1.081102    2.854746    0.427096    3.720430    3.477132   \n",
       "1967    1.817022    1.188246    3.068851    0.405727    4.022238    3.622661   \n",
       "1968    1.952592    1.296958    3.356795    0.394397    4.387786    3.451100   \n",
       "1969    2.189504    1.420849    3.698781    0.443717    4.797459    3.729878   \n",
       "1970    2.467061    1.617933    4.121442    0.515514    5.019013    4.185208   \n",
       "1971    2.698827    1.868001    4.654807    0.555585    5.611000    4.743236   \n",
       "1972    3.106212    2.233615    5.302445    0.632902    6.879731    5.443400   \n",
       "1973    3.809923    3.195232    6.157733    0.771263    8.939845    6.166626   \n",
       "1974    5.309950    4.196787    7.521418    0.802692    9.653955    6.602199   \n",
       "1975    5.807765    4.938678    8.151392    0.909803   12.199015    7.741750   \n",
       "1976    6.270710    5.854732    9.686036    0.856971   12.587362    7.449541   \n",
       "1977    6.586473    6.740579    9.922479    0.973859   13.870729    8.425331   \n",
       "1978    7.072356    7.655453   10.252047    0.832485   17.130779   10.756835   \n",
       "1979    8.051619    8.460434   11.397395    0.992478   20.756527   14.059708   \n",
       "1980    8.952055    9.074131   12.840692    1.064117   23.709158   18.093126   \n",
       "1981   10.558072    9.862375   14.358356    1.090378   20.810589   17.317026   \n",
       "1982   11.581422   10.370713   14.703852    1.141723   19.773546   16.493515   \n",
       "1983   10.580249    7.249433   15.976101    1.284221   18.928057   15.679834   \n",
       "1984   11.550381    7.199110   16.670773    1.447108   17.941357   14.779372   \n",
       "1985   10.781475    8.060674   17.112682    1.722903   18.700504   15.668835   \n",
       "1986   10.891129    9.803727   17.712294    1.674304   26.081884   19.262091   \n",
       "1987   11.309879   10.819568   20.241697    1.519626   31.582531   23.863411   \n",
       "1988   14.095135   11.768482   23.804567    1.738856   34.445183   29.148975   \n",
       "1989   17.898561   15.786166   26.507839    1.936006   34.660358   29.685572   \n",
       "1990   18.588226   17.773748   27.856710    2.008877   42.908424   35.011066   \n",
       "1991   19.456728   15.585039   28.624285    2.134219   42.911713   36.598030   \n",
       "1992   19.429914   14.929020   27.776659    2.376617   47.380760   37.781281   \n",
       "1993   18.630455   16.753565   27.060854    2.475795   44.721748   33.994061   \n",
       "1994   19.267387   20.879093   27.102228    3.141549   47.127769   36.523636   \n",
       "1995   21.973061   29.407007   28.317118    4.088838   54.129810   43.114830   \n",
       "1996   23.952869   32.506710   29.469398    4.808445   54.284660   45.531494   \n",
       "1997   26.000294   33.759693   30.609203    5.353187   49.119133   50.015213   \n",
       "1998   23.854387   33.014500   29.628277    5.728725   50.817101   53.002625   \n",
       "1999   23.241333   22.920727   31.703773    6.090297   50.480473   54.104668   \n",
       "2000   24.820362   25.053860   34.805099    6.743418   46.169548   53.356583   \n",
       "2001   22.642569   21.404823   34.533550    7.456372   46.575840   52.803062   \n",
       "2002   23.610397   19.486420   35.547020    8.186515   50.759651   57.191116   \n",
       "2003   27.903351   21.337933   41.850822    9.242693   62.360371   65.867653   \n",
       "2004   36.667038   25.582922   47.979767   10.885310   71.660561   77.600082   \n",
       "2005   41.500374   34.081821   54.822254   12.725822   74.274323   81.499733   \n",
       "2006   44.640030   42.337936   61.652500   15.320895   78.452690   86.789284   \n",
       "2007   50.997837   53.403313   68.641594   19.764481   89.949265   99.055756   \n",
       "2008   63.010288   64.822395   72.575035   25.576424   99.067711   93.889580   \n",
       "2009   55.426727   63.719284   64.239624   28.400839   91.311607   77.424530   \n",
       "2010   68.554184   84.430656   75.582443   33.887066   89.428513   79.789055   \n",
       "2011   83.469055  100.000000   83.806572   42.039047   96.865250   85.393753   \n",
       "2012   92.332306   94.230911   85.444077   47.498219   90.729576   86.672798   \n",
       "2013   94.085716   94.521072   86.296051   53.278290   95.063934   89.234039   \n",
       "2014   87.595413   93.879845   84.387138   58.317238   96.554443   98.166290   \n",
       "2015   80.611511   68.887772   72.739494   61.579178   82.464096   93.991241   \n",
       "2016   72.015610   68.638596   71.406960   62.535252   83.605980   86.458817   \n",
       "2017   79.172302   78.875809   77.074249   68.531837   87.736870   85.940926   \n",
       "2018   85.248337   73.272896   80.627426   77.352203   94.356674   92.175186   \n",
       "2019   83.096756   71.604584   81.488579   79.495811   92.257652   91.499626   \n",
       "2020   79.200783   56.422741   76.996338   81.765869   89.219604   86.617332   \n",
       "2021   92.675529   63.055206   93.534409   99.205536  100.000000  100.000000   \n",
       "2022  100.000000   73.393753  100.000000  100.000000   94.084465   98.340660   \n",
       "\n",
       "              6           7           8           9           10          11   \n",
       "1963    1.430447    2.395973    1.108006    0.220242    1.199275    1.078785  \\\n",
       "1964    1.668502    2.622850    1.303321    0.191006    1.419897    1.161971   \n",
       "1965    1.759329    2.822245    1.450016    0.172333    1.544350    1.243864   \n",
       "1966    1.354926    3.057925    1.684024    0.216958    1.719716    1.465611   \n",
       "1967    1.481052    3.368399    1.973449    0.268136    1.878111    1.626147   \n",
       "1968    1.568214    3.651092    2.337254    0.337904    2.076104    1.819021   \n",
       "1969    1.726631    4.030675    2.745444    0.424015    2.296726    2.023444   \n",
       "1970    1.844042    4.707827    3.389619    0.497252    2.511690    1.810989   \n",
       "1971    1.989649    5.176015    3.828729    0.546866    2.771910    1.703036   \n",
       "1972    2.111117    6.030753    5.070358    0.599812    3.196177    2.133130   \n",
       "1973    2.526233    7.285893    6.888676    0.766255    3.908974    2.685781   \n",
       "1974    2.940126    8.285307    7.646655    1.079214    5.091276    3.716824   \n",
       "1975    2.909016    9.453235    8.314919    1.202917    6.222656    4.660002   \n",
       "1976    3.034400    9.329574    9.345152    1.651199    6.284655    5.353928   \n",
       "1977    3.588875   10.694611   11.501435    2.122994    5.792196    6.126129   \n",
       "1978    4.056031   13.080257   16.159975    2.869872    7.258395    6.801696   \n",
       "1979    4.519574   16.344271   16.820011    3.696771    9.512859    9.333132   \n",
       "1980    5.504295   19.814241   17.623116    3.611278   14.536759    7.182010   \n",
       "1981    5.715989   17.881466   19.434282    4.027340   18.653975    7.417035   \n",
       "1982    5.929403   17.739054   18.087572    4.326934   13.053688    6.739010   \n",
       "1983    6.447750   18.393764   19.822254    4.846080   11.042919    6.439582   \n",
       "1984    6.267415   18.179756   21.018898    5.384462   13.027356    6.263346   \n",
       "1985    6.868697   18.774687   22.302481    5.593520   13.805908    7.019733   \n",
       "1986    7.355373   26.586882   33.144653    6.451665    9.514732    7.906441   \n",
       "1987    8.243018   33.450745   40.380455    8.169622   10.433032    9.100897   \n",
       "1988    8.761649   37.016876   48.971714   11.021297   12.842088    9.485611   \n",
       "1989    8.745471   38.555180   48.704353   13.635191   15.655859   11.186418   \n",
       "1990    9.482141   49.040749   49.946369   15.647401   18.473768   15.731504   \n",
       "1991    7.979266   51.739246   57.146263   18.258232   22.142731   15.769633   \n",
       "1992    8.514045   54.809071   62.317955   19.631912   25.679611   16.610476   \n",
       "1993    8.250760   44.213802   71.012222   21.682810   35.407856   18.837132   \n",
       "1994    9.668151   45.636116   79.695602   25.600702   37.322544   13.640844   \n",
       "1995   10.643201   48.768375   88.412689   31.286430   25.461447   17.677963   \n",
       "1996   11.606688   54.487930   78.493439   33.693233   29.060719   18.946005   \n",
       "1997   12.285275   51.559044   71.622925   31.461536   35.385414   19.824451   \n",
       "1998   12.447271   52.728695   65.340012   21.167326   37.229847   28.809982   \n",
       "1999   13.554157   51.997749   73.911247   27.472380   42.443676   26.769289   \n",
       "2000   13.837002   47.606514   79.210320   31.816244   50.057720   28.638039   \n",
       "2001   14.340565   48.492760   69.745804   30.241390   53.507973   21.064243   \n",
       "2002   15.211943   53.007553   66.686920   34.636189   54.597420   25.083447   \n",
       "2003   17.952234   65.498024   72.055191   38.803669   51.572735   32.845673   \n",
       "2004   20.949179   75.002136   78.010727   43.798695   55.313957   42.688011   \n",
       "2005   24.235149   77.147491   77.027855   51.624733   62.048141   52.862312   \n",
       "2006   27.776512   80.939423   73.364105   58.158066   68.971306   58.162098   \n",
       "2007   35.943993   91.881248   73.014740   64.751122   74.438316   71.134041   \n",
       "2008   35.416939  100.000000   81.415558   57.833492   78.489426   80.439560   \n",
       "2009   39.641132   91.334312   84.330162   52.123959   63.644123   67.789742   \n",
       "2010   49.499882   88.684334   91.816620   63.174755   74.799194   81.120094   \n",
       "2011   53.855347   95.281143   99.374786   69.202301   83.474609   87.574272   \n",
       "2012   53.990810   86.644096  100.000000   70.594078   84.931740   91.935356   \n",
       "2013   54.849987   88.926140   83.099915   75.694565   90.118477  100.000000   \n",
       "2014   60.238476   89.760025   78.072563   81.963249   93.011459   98.030426   \n",
       "2015   62.142761   76.251579   70.865326   80.939201   82.865273   90.239594   \n",
       "2016   67.791313   77.930275   79.773407   82.835350   76.262398   90.800156   \n",
       "2017   78.328033   81.447777   78.612122   89.670959   81.948982   89.683601   \n",
       "2018   79.848091   86.850632   80.366539   95.245041   86.438736   81.277672   \n",
       "2019   83.767532   83.503113   81.595947   91.190681   89.734200   79.341782   \n",
       "2020   78.922440   78.766373   80.492622   90.798065   77.112488   75.202499   \n",
       "2021   93.064201   87.781586   79.803047  100.000000   90.004974   85.512146   \n",
       "2022  100.000000   83.466972   67.456894   91.953957  100.000000   94.590599   \n",
       "\n",
       "              12          13  \n",
       "1963    2.507982    2.368880  \n",
       "1964    2.693352    2.609343  \n",
       "1965    2.920743    2.852250  \n",
       "1966    3.200760    3.101573  \n",
       "1967    3.384166    3.452949  \n",
       "1968    3.701493    3.737410  \n",
       "1969    4.005467    4.202753  \n",
       "1970    4.215197    4.630820  \n",
       "1971    4.574731    5.109364  \n",
       "1972    5.023466    5.350489  \n",
       "1973    5.597898    7.259451  \n",
       "1974    6.068653    9.033006  \n",
       "1975    6.617146    9.364244  \n",
       "1976    7.357476    8.980903  \n",
       "1977    8.175983    9.892721  \n",
       "1978    9.235466   11.263085  \n",
       "1979   10.318359   13.757913  \n",
       "1980   11.221540   19.513748  \n",
       "1981   12.595055   20.327715  \n",
       "1982   13.132107   18.748182  \n",
       "1983   14.272006   20.996119  \n",
       "1984   15.856971   18.522530  \n",
       "1985   17.040529   14.067972  \n",
       "1986   17.985645   16.009354  \n",
       "1987   19.067951   21.068497  \n",
       "1988   20.565132   22.692486  \n",
       "1989   22.156252   23.582655  \n",
       "1990   23.419134   27.509447  \n",
       "1991   24.184902   29.507607  \n",
       "1992   25.607365   32.072525  \n",
       "1993   26.935709   32.124603  \n",
       "1994   28.619259   33.503464  \n",
       "1995   30.003687   37.480606  \n",
       "1996   31.705681   35.625294  \n",
       "1997   33.686741   36.878712  \n",
       "1998   35.592525   33.387856  \n",
       "1999   37.824638   33.067902  \n",
       "2000   40.258686   33.119362  \n",
       "2001   41.558556   29.556974  \n",
       "2002   42.922050   28.172785  \n",
       "2003   44.993038   42.998512  \n",
       "2004   47.980747   55.828728  \n",
       "2005   51.209023   63.043983  \n",
       "2006   54.258137   66.315804  \n",
       "2007   56.844826   72.692596  \n",
       "2008   58.005859   68.994232  \n",
       "2009   56.859898   71.967354  \n",
       "2010   59.101997   91.087799  \n",
       "2011   61.265022  100.000000  \n",
       "2012   63.834442   94.805984  \n",
       "2013   66.148491   87.491592  \n",
       "2014   68.927017   83.194962  \n",
       "2015   71.500748   75.667870  \n",
       "2016   73.421555   70.621094  \n",
       "2017   76.493607   83.249512  \n",
       "2018   80.639748   88.205879  \n",
       "2019   83.969788   84.795212  \n",
       "2020   82.711075   73.683968  \n",
       "2021   91.565628   91.448303  \n",
       "2022  100.000000   88.579262  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/mnt/vstor/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/GDP/GDP.csv', delimiter=',', encoding='utf-8', skiprows=4)\n",
    "\n",
    "# Example: Filtering rows where 'Name' is in a list of names\n",
    "G20_list = ['AUS', 'ARG', 'BRA', 'CAN', 'CHN', 'FRA', 'DEU', 'IND', 'IDN', 'ITA', 'JPN', 'MEX', 'RUS', 'SAU', 'ZAF', 'KOR', 'TUR', 'GBR', 'USA']\n",
    "# filter = ['BRA', 'CAN', 'IND', 'ITA', 'JPN', 'MEX', 'RUS', 'SAU', 'GBR', 'USA']\n",
    "# Applying the filter\n",
    "condition = df['Country Code'].isin(G20_list)\n",
    "filtered_df = df[condition]\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "new_df = filtered_df.T.iloc[4:-1,:]\n",
    "new_df = new_df.dropna(axis=1)\n",
    "num_columns = len(new_df.columns)\n",
    "\n",
    "# Create a dictionary to map current column names to new column names\n",
    "new_column_names = {col: i for i, col in enumerate(new_df.columns)}\n",
    "\n",
    "# Rename the columns\n",
    "new_df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "normalized_df = new_df.div(new_df.max())\n",
    "\n",
    "# Display the resulting normalized DataFrame\n",
    "normalized_df = normalized_df.iloc[3:,:]\n",
    "normalized_df = normalized_df.astype('float32')\n",
    "normalized_df = normalized_df*100\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix:\n",
      "          0         1         2         3         4         5         6    \n",
      "0   1.000000  0.984179  0.989144  0.000000  0.961498  0.961858  0.973649  \\\n",
      "1   0.984179  1.000000  0.975627  0.000000  0.957488  0.951461  0.000000   \n",
      "2   0.989144  0.975627  1.000000  0.000000  0.988583  0.987821  0.950375   \n",
      "3   0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.985018   \n",
      "4   0.961498  0.957488  0.988583  0.000000  1.000000  0.994629  0.000000   \n",
      "5   0.961858  0.951461  0.987821  0.000000  0.994629  1.000000  0.000000   \n",
      "6   0.973649  0.000000  0.950375  0.985018  0.000000  0.000000  1.000000   \n",
      "7   0.000000  0.000000  0.979542  0.000000  0.997484  0.992106  0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.973215  0.962139  0.000000   \n",
      "9   0.986087  0.962403  0.983273  0.000000  0.964281  0.972227  0.973889   \n",
      "10  0.979310  0.967123  0.991979  0.000000  0.984190  0.990922  0.000000   \n",
      "11  0.991615  0.985221  0.984069  0.000000  0.959784  0.964889  0.957878   \n",
      "12  0.976892  0.950730  0.991064  0.000000  0.983473  0.987356  0.955328   \n",
      "13  0.982959  0.979424  0.995337  0.000000  0.989820  0.985055  0.000000   \n",
      "\n",
      "          7         8         9         10        11        12        13  \n",
      "0   0.000000  0.000000  0.986087  0.979310  0.991615  0.976892  0.982959  \n",
      "1   0.000000  0.000000  0.962403  0.967123  0.985221  0.950730  0.979424  \n",
      "2   0.979542  0.000000  0.983273  0.991979  0.984069  0.991064  0.995337  \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4   0.997484  0.973215  0.964281  0.984190  0.959784  0.983473  0.989820  \n",
      "5   0.992106  0.962139  0.972227  0.990922  0.964889  0.987356  0.985055  \n",
      "6   0.000000  0.000000  0.973889  0.000000  0.957878  0.955328  0.000000  \n",
      "7   1.000000  0.979150  0.000000  0.975997  0.000000  0.973228  0.982149  \n",
      "8   0.979150  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  1.000000  0.983378  0.982031  0.986093  0.975802  \n",
      "10  0.975997  0.000000  0.983378  1.000000  0.979791  0.989720  0.986496  \n",
      "11  0.000000  0.000000  0.982031  0.979791  1.000000  0.967426  0.979257  \n",
      "12  0.973228  0.000000  0.986093  0.989720  0.967426  1.000000  0.982041  \n",
      "13  0.982149  0.000000  0.975802  0.986496  0.979257  0.982041  1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample DataFrame\n",
    "# Replace this with your actual DataFrame\n",
    "data = {\n",
    "    'Column1': [1, 2, 3, 4],\n",
    "    'Column2': [5, 6, 7, 8],\n",
    "    'Column3': [9, 10, 11, 12],\n",
    "    # Add more columns as needed\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to compute cosine similarity between columns\n",
    "def compute_cosine_similarity(df):\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(df.T)  # Transpose to get columns as samples\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    adjacency_matrix = pd.DataFrame(cosine_sim_matrix, index=df.columns, columns=df.columns)\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "adjacency_matrix = compute_cosine_similarity(normalized_df)\n",
    "adjacency_matrix[adjacency_matrix<=0.95] = 0\n",
    "# Display the adjacency matrix\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968333</td>\n",
       "      <td>0.984782</td>\n",
       "      <td>0.927076</td>\n",
       "      <td>0.935475</td>\n",
       "      <td>0.927517</td>\n",
       "      <td>0.958933</td>\n",
       "      <td>0.896876</td>\n",
       "      <td>0.776604</td>\n",
       "      <td>0.972980</td>\n",
       "      <td>0.959487</td>\n",
       "      <td>0.984369</td>\n",
       "      <td>0.959365</td>\n",
       "      <td>0.972863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956449</td>\n",
       "      <td>0.831699</td>\n",
       "      <td>0.929011</td>\n",
       "      <td>0.906225</td>\n",
       "      <td>0.883122</td>\n",
       "      <td>0.900823</td>\n",
       "      <td>0.802875</td>\n",
       "      <td>0.926449</td>\n",
       "      <td>0.934672</td>\n",
       "      <td>0.971666</td>\n",
       "      <td>0.902590</td>\n",
       "      <td>0.966803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.984782</td>\n",
       "      <td>0.956449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881727</td>\n",
       "      <td>0.974996</td>\n",
       "      <td>0.969861</td>\n",
       "      <td>0.932045</td>\n",
       "      <td>0.951096</td>\n",
       "      <td>0.841182</td>\n",
       "      <td>0.977102</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>0.980761</td>\n",
       "      <td>0.977839</td>\n",
       "      <td>0.988502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.927076</td>\n",
       "      <td>0.831699</td>\n",
       "      <td>0.881727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.789110</td>\n",
       "      <td>0.792243</td>\n",
       "      <td>0.989065</td>\n",
       "      <td>0.722481</td>\n",
       "      <td>0.579904</td>\n",
       "      <td>0.919240</td>\n",
       "      <td>0.847150</td>\n",
       "      <td>0.882636</td>\n",
       "      <td>0.903470</td>\n",
       "      <td>0.844475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.935475</td>\n",
       "      <td>0.929011</td>\n",
       "      <td>0.974996</td>\n",
       "      <td>0.789110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987235</td>\n",
       "      <td>0.857516</td>\n",
       "      <td>0.992659</td>\n",
       "      <td>0.920040</td>\n",
       "      <td>0.952006</td>\n",
       "      <td>0.970073</td>\n",
       "      <td>0.944741</td>\n",
       "      <td>0.958897</td>\n",
       "      <td>0.976343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.927517</td>\n",
       "      <td>0.906225</td>\n",
       "      <td>0.969861</td>\n",
       "      <td>0.792243</td>\n",
       "      <td>0.987235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858064</td>\n",
       "      <td>0.980502</td>\n",
       "      <td>0.897503</td>\n",
       "      <td>0.958656</td>\n",
       "      <td>0.981182</td>\n",
       "      <td>0.944755</td>\n",
       "      <td>0.967632</td>\n",
       "      <td>0.961796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.958933</td>\n",
       "      <td>0.883122</td>\n",
       "      <td>0.932045</td>\n",
       "      <td>0.989065</td>\n",
       "      <td>0.857516</td>\n",
       "      <td>0.858064</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.801812</td>\n",
       "      <td>0.664532</td>\n",
       "      <td>0.957035</td>\n",
       "      <td>0.900830</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.945888</td>\n",
       "      <td>0.904565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.896876</td>\n",
       "      <td>0.900823</td>\n",
       "      <td>0.951096</td>\n",
       "      <td>0.722481</td>\n",
       "      <td>0.992659</td>\n",
       "      <td>0.980502</td>\n",
       "      <td>0.801812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937546</td>\n",
       "      <td>0.918226</td>\n",
       "      <td>0.949334</td>\n",
       "      <td>0.914010</td>\n",
       "      <td>0.931215</td>\n",
       "      <td>0.955790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.776604</td>\n",
       "      <td>0.802875</td>\n",
       "      <td>0.841182</td>\n",
       "      <td>0.579904</td>\n",
       "      <td>0.920040</td>\n",
       "      <td>0.897503</td>\n",
       "      <td>0.664532</td>\n",
       "      <td>0.937546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818689</td>\n",
       "      <td>0.861152</td>\n",
       "      <td>0.777888</td>\n",
       "      <td>0.845221</td>\n",
       "      <td>0.858978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.972980</td>\n",
       "      <td>0.926449</td>\n",
       "      <td>0.977102</td>\n",
       "      <td>0.919240</td>\n",
       "      <td>0.952006</td>\n",
       "      <td>0.958656</td>\n",
       "      <td>0.957035</td>\n",
       "      <td>0.918226</td>\n",
       "      <td>0.818689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972316</td>\n",
       "      <td>0.965537</td>\n",
       "      <td>0.986048</td>\n",
       "      <td>0.962995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.959487</td>\n",
       "      <td>0.934672</td>\n",
       "      <td>0.981801</td>\n",
       "      <td>0.847150</td>\n",
       "      <td>0.970073</td>\n",
       "      <td>0.981182</td>\n",
       "      <td>0.900830</td>\n",
       "      <td>0.949334</td>\n",
       "      <td>0.861152</td>\n",
       "      <td>0.972316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966284</td>\n",
       "      <td>0.977028</td>\n",
       "      <td>0.969165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.984369</td>\n",
       "      <td>0.971666</td>\n",
       "      <td>0.980761</td>\n",
       "      <td>0.882636</td>\n",
       "      <td>0.944741</td>\n",
       "      <td>0.944755</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.914010</td>\n",
       "      <td>0.777888</td>\n",
       "      <td>0.965537</td>\n",
       "      <td>0.966284</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947079</td>\n",
       "      <td>0.972797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.959365</td>\n",
       "      <td>0.902590</td>\n",
       "      <td>0.977839</td>\n",
       "      <td>0.903470</td>\n",
       "      <td>0.958897</td>\n",
       "      <td>0.967632</td>\n",
       "      <td>0.945888</td>\n",
       "      <td>0.931215</td>\n",
       "      <td>0.845221</td>\n",
       "      <td>0.986048</td>\n",
       "      <td>0.977028</td>\n",
       "      <td>0.947079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.972863</td>\n",
       "      <td>0.966803</td>\n",
       "      <td>0.988502</td>\n",
       "      <td>0.844475</td>\n",
       "      <td>0.976343</td>\n",
       "      <td>0.961796</td>\n",
       "      <td>0.904565</td>\n",
       "      <td>0.955790</td>\n",
       "      <td>0.858978</td>\n",
       "      <td>0.962995</td>\n",
       "      <td>0.969165</td>\n",
       "      <td>0.972797</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \n",
       "0   1.000000  0.968333  0.984782  0.927076  0.935475  0.927517  0.958933  \\\n",
       "1   0.968333  1.000000  0.956449  0.831699  0.929011  0.906225  0.883122   \n",
       "2   0.984782  0.956449  1.000000  0.881727  0.974996  0.969861  0.932045   \n",
       "3   0.927076  0.831699  0.881727  1.000000  0.789110  0.792243  0.989065   \n",
       "4   0.935475  0.929011  0.974996  0.789110  1.000000  0.987235  0.857516   \n",
       "5   0.927517  0.906225  0.969861  0.792243  0.987235  1.000000  0.858064   \n",
       "6   0.958933  0.883122  0.932045  0.989065  0.857516  0.858064  1.000000   \n",
       "7   0.896876  0.900823  0.951096  0.722481  0.992659  0.980502  0.801812   \n",
       "8   0.776604  0.802875  0.841182  0.579904  0.920040  0.897503  0.664532   \n",
       "9   0.972980  0.926449  0.977102  0.919240  0.952006  0.958656  0.957035   \n",
       "10  0.959487  0.934672  0.981801  0.847150  0.970073  0.981182  0.900830   \n",
       "11  0.984369  0.971666  0.980761  0.882636  0.944741  0.944755  0.927603   \n",
       "12  0.959365  0.902590  0.977839  0.903470  0.958897  0.967632  0.945888   \n",
       "13  0.972863  0.966803  0.988502  0.844475  0.976343  0.961796  0.904565   \n",
       "\n",
       "          7         8         9         10        11        12        13  \n",
       "0   0.896876  0.776604  0.972980  0.959487  0.984369  0.959365  0.972863  \n",
       "1   0.900823  0.802875  0.926449  0.934672  0.971666  0.902590  0.966803  \n",
       "2   0.951096  0.841182  0.977102  0.981801  0.980761  0.977839  0.988502  \n",
       "3   0.722481  0.579904  0.919240  0.847150  0.882636  0.903470  0.844475  \n",
       "4   0.992659  0.920040  0.952006  0.970073  0.944741  0.958897  0.976343  \n",
       "5   0.980502  0.897503  0.958656  0.981182  0.944755  0.967632  0.961796  \n",
       "6   0.801812  0.664532  0.957035  0.900830  0.927603  0.945888  0.904565  \n",
       "7   1.000000  0.937546  0.918226  0.949334  0.914010  0.931215  0.955790  \n",
       "8   0.937546  1.000000  0.818689  0.861152  0.777888  0.845221  0.858978  \n",
       "9   0.918226  0.818689  1.000000  0.972316  0.965537  0.986048  0.962995  \n",
       "10  0.949334  0.861152  0.972316  1.000000  0.966284  0.977028  0.969165  \n",
       "11  0.914010  0.777888  0.965537  0.966284  1.000000  0.947079  0.972797  \n",
       "12  0.931215  0.845221  0.986048  0.977028  0.947079  1.000000  0.954545  \n",
       "13  0.955790  0.858978  0.962995  0.969165  0.972797  0.954545  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to compute correlation matrix\n",
    "def compute_correlation_matrix(df):\n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = df.corr().abs()\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "correlation_matrix = compute_correlation_matrix(normalized_df)\n",
    "# correlation_matrix[correlation_matrix<=0.25] = 0\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(normalized_df\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mT\n\u001b[1;32m      <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m A \u001b[39m=\u001b[39m adjacency_matrix\n\u001b[1;32m      <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m G \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcoo_matrix(A)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_df' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(normalized_df.values, dtype=torch.float32).T\n",
    "A = adjacency_matrix\n",
    "G = sp.coo_matrix(A)\n",
    "edge_index = torch.tensor(np.array([G.row, G.col]), dtype=torch.int64)\n",
    "edge_weight = torch.tensor(G.data).float()\n",
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called\n",
      "Called\n",
      "Called\n",
      "Called\n",
      "CalledCalled\n",
      "\n",
      "Called\n",
      "Futures submitted.\n",
      "Waiting for results...\n",
      "Called\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x120 and 60x30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/tmp/job.825246.hpc/ipykernel_1441299/373990601.py\", line 95, in parallel_forward\n    output = model(x, edge_index)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/job.825246.hpc/ipykernel_1441299/373990601.py\", line 48, in forward\n    h_1 = self.conv1(x, edge_index)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch_geometric/nn/conv/cheb_conv.py\", line 163, in forward\n    out = self.lins[0](Tx_0)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch_geometric/nn/dense/linear.py\", line 130, in forward\n    return F.linear(x, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (100x120 and 60x30)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=336'>337</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFutures submitted.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=338'>339</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWaiting for results...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=339'>340</a>\u001b[0m output1, target1 \u001b[39m=\u001b[39m future1\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=340'>341</a>\u001b[0m output2, target2 \u001b[39m=\u001b[39m future2\u001b[39m.\u001b[39mresult()\n\u001b[1;32m    <a href='vscode-notebook-cell://ondemand-pioneer.case.edu/home/yxf451/CSE_MSE_RXF131/cradle-members/sdle/yxf451/Git/21-pv-stgnn-bkp/topics/STGCN_DAE/Para_st_DynGNN_Version_2.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=341'>342</a>\u001b[0m output3, target3 \u001b[39m=\u001b[39m future3\u001b[39m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x120 and 60x30)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "\n",
    "m1 = [[1] for i in range(1,61)]\n",
    "m2 = [[i] for i in range(1,61)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(60) ] for x in range(5) ]\n",
    "for i in range(5):\n",
    "  for j in range(60):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(5) ] for x in range(5) ]\n",
    "for i in range(5):\n",
    "  for j in range(5):\n",
    "    W[i][j] = np.abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(5) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(60, 30, 1)\n",
    "        self.conv2 = GATConv(30, 15)\n",
    "        self.conv3 = ChebConv(15, 30, 1)\n",
    "        self.conv4 = GATConv(30, 60)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(60, 30, 1)\n",
    "        self.conv2 = GATConv(30, 15)\n",
    "        self.conv3 = ChebConv(15, 30, 1)\n",
    "        self.conv4 = GATConv(30, 60)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "target3 = x.clone()\n",
    "target4 = x.clone()\n",
    "target5 = x.clone()\n",
    "target6 = x.clone()\n",
    "target7 = x.clone()\n",
    "target8 = x.clone()\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    print(\"Called\")\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation2(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation3(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "def backpropagation_fluctuation4(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation5(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation6(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation7(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "model3 = GAE2()\n",
    "model4 = GAE2()\n",
    "model5 = GAE2()\n",
    "model6 = GAE2()\n",
    "model7 = GAE2()\n",
    "model8 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
    "optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)\n",
    "optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)\n",
    "optimizer6 = torch.optim.Adam(model6.parameters(), lr=0.001)\n",
    "optimizer7 = torch.optim.Adam(model7.parameters(), lr=0.001)\n",
    "optimizer8 = torch.optim.Adam(model8.parameters(), lr=0.001)\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "with ProcessPoolExecutor(max_workers=7) as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    future3 = executor.submit(parallel_forward, model3, x, edge_index, target3)\n",
    "    future4 = executor.submit(parallel_forward, model4, x, edge_index, target4)\n",
    "    future5 = executor.submit(parallel_forward, model5, x, edge_index, target5)\n",
    "    future6 = executor.submit(parallel_forward, model6, x, edge_index, target6)    \n",
    "    future7 = executor.submit(parallel_forward, model7, x, edge_index, target7)   \n",
    "    future8 = executor.submit(parallel_forward, model8, x, edge_index, target8) \n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    output3, target3 = future3.result()\n",
    "    output4, target4 = future4.result()\n",
    "    output5, target5 = future5.result()\n",
    "    output6, target6 = future6.result()\n",
    "    output7, target7 = future7.result()\n",
    "    output8, target8 = future8.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation2, (model3, optimizer3, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation3, (model4, optimizer4, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation4, (model5, optimizer5, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation5, (model6, optimizer6, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation6, (model7, optimizer7, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation7, (model8, optimizer8, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, TransformerConv, AGNNConv, FusedGATConv, ChebConv, SAGEConv, GraphConv, CuGraphGATConv, TAGConv, GMMConv\n",
    "# from scipy.stats import linregress\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#   def __init__(self):\n",
    "#     super(GCN, self).__init__()\n",
    "#     torch.manual_seed(42)\n",
    "\n",
    "#     # self.conv1 = TransformerConv(260, 130)\n",
    "#     # self.conv2 = GATConv(130, 65)\n",
    "#     # self.conv3 = TransformerConv(65, 130)\n",
    "#     # self.conv4 = GATConv(130, 260)\n",
    "\n",
    "#     self.conv1 = nn.Linear(260, 130)\n",
    "#     self.conv2 = nn.Linear(130, 65)\n",
    "#     self.conv3 = nn.Linear(65, 130)\n",
    "#     self.conv4 = nn.Linear(130, 260)\n",
    "\n",
    "\n",
    "#     self.conv5 = nn.Linear(260, 130)\n",
    "#     self.conv6 = nn.Linear(130, 65)\n",
    "#     self.conv7 = nn.Linear(65, 130)\n",
    "#     self.conv8 = nn.Linear(130, 260)\n",
    "\n",
    "\n",
    "#   def forward(self, x, edge_index):\n",
    "\n",
    "\n",
    "#     h_1 = self.conv1(x)\n",
    "#     h_2 = self.conv2(h_1)\n",
    "#     h_3 = self.conv3(h_2)\n",
    "#     h_a = self.conv4(h_3)\n",
    "\n",
    "#     h_5 = self.conv5(x)\n",
    "#     h_6 = self.conv6(h_5)\n",
    "#     h_7 = self.conv7(h_6)\n",
    "#     h_f = self.conv8(h_7)\n",
    "#     h_a = torch.nn.functional.avg_pool1d(h_a, kernel_size = 11, stride=1, padding = 11 // 2)\n",
    "\n",
    "#     return h_a, h_f\n",
    "\n",
    "# model = GCN()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "Futures submitted.\n",
      "Waiting for results...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (14x60 and 260x130)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/tmp/job.810340.hpc/ipykernel_2886314/2740346304.py\", line 93, in parallel_forward\n    output = model(x, edge_index)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/job.810340.hpc/ipykernel_2886314/2740346304.py\", line 47, in forward\n    h_1 = self.conv1(x, edge_index)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch_geometric/nn/conv/cheb_conv.py\", line 163, in forward\n    out = self.lins[0](Tx_0)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch_geometric/nn/dense/linear.py\", line 130, in forward\n    return F.linear(x, self.weight, self.bias)\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (14x60 and 260x130)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 338\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFutures submitted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 338\u001b[0m output1, target1 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m output2, target2 \u001b[38;5;241m=\u001b[39m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    340\u001b[0m output3, target3 \u001b[38;5;241m=\u001b[39m future3\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14x60 and 260x130)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,261)]\n",
    "m2 = [[i] for i in range(1,261)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "K = 52\n",
    "d = [ [ 0 for y in range(260) ] for x in range(5) ]\n",
    "for i in range(5):\n",
    "  for j in range(260):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(5) ] for x in range(5) ]\n",
    "for i in range(5):\n",
    "  for j in range(5):\n",
    "    W[i][j] = np.abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(5) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(260, 130, 1)\n",
    "        self.conv2 = GATConv(130, 65)\n",
    "        self.conv3 = ChebConv(65, 130, 1)\n",
    "        self.conv4 = GATConv(130, 260)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(260, 130, 1)\n",
    "        self.conv2 = GATConv(130, 65)\n",
    "        self.conv3 = ChebConv(65, 130, 1)\n",
    "        self.conv4 = GATConv(130, 260)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "target3 = x.clone()\n",
    "# target4 = x.clone()\n",
    "# target5 = x.clone()\n",
    "# target6 = x.clone()\n",
    "# target7 = x.clone()\n",
    "# target8 = x.clone()\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation2(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation3(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "# def backpropagation_fluctuation4(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation5(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation6(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation7(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "model3 = GAE2()\n",
    "# model4 = GAE2()\n",
    "# model5 = GAE2()\n",
    "# model6 = GAE2()\n",
    "# model7 = GAE2()\n",
    "# model8 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
    "# optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)\n",
    "# optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)\n",
    "# optimizer6 = torch.optim.Adam(model6.parameters(), lr=0.001)\n",
    "# optimizer7 = torch.optim.Adam(model7.parameters(), lr=0.001)\n",
    "# optimizer8 = torch.optim.Adam(model8.parameters(), lr=0.001)\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    future3 = executor.submit(parallel_forward, model3, x, edge_index, target3)\n",
    "    # future4 = executor.submit(parallel_forward, model4, x, edge_index, target4)\n",
    "    # future5 = executor.submit(parallel_forward, model5, x, edge_index, target5)\n",
    "    # future6 = executor.submit(parallel_forward, model6, x, edge_index, target6)    \n",
    "    # future7 = executor.submit(parallel_forward, model7, x, edge_index, target7)   \n",
    "    # future8 = executor.submit(parallel_forward, model8, x, edge_index, target8) \n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    output3, target3 = future3.result()\n",
    "    # output4, target4 = future4.result()\n",
    "    # output5, target5 = future5.result()\n",
    "    # output6, target6 = future6.result()\n",
    "    # output7, target7 = future7.result()\n",
    "    # output8, target8 = future8.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation2, (model3, optimizer3, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation3, (model4, optimizer4, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation4, (model5, optimizer5, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation5, (model6, optimizer6, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation6, (model7, optimizer7, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation7, (model8, optimizer8, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "called\n",
      "called\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Futures submitted.\n",
      "Waiting for results...\n",
      "Results received.\n",
      "Starting backpropagation...\n",
      "Backpropagation submitted.\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 1/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 1/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 2/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 3/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 2/50, Model: GAE2, Loss: 3197387.25Epoch 4/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 5/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 3/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 4/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 6/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 7/50, Model: GAE1, Loss: 7818.0390625tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "\n",
      "Epoch 5/50, Model: GAE2, Loss: 3197387.25Epoch 8/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "\n",
      "Epoch 6/50, Model: GAE2, Loss: 3197387.25Epoch 9/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 10/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 7/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 11/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 12/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 8/50, Model: GAE2, Loss: 3197387.25Epoch 13/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 9/50, Model: GAE2, Loss: 3197387.25Epoch 14/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 15/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 10/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 16/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 11/50, Model: GAE2, Loss: 3197387.25Epoch 17/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 18/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 12/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 19/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 20/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 13/50, Model: GAE2, Loss: 3197387.25Epoch 21/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 22/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 23/50, Model: GAE1, Loss: 7818.0390625tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "\n",
      "Epoch 14/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 24/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 25/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 15/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 26/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 16/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 27/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 28/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 17/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 29/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 18/50, Model: GAE2, Loss: 3197387.25Epoch 30/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 31/50, Model: GAE1, Loss: 7818.0390625Epoch 19/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 32/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 33/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 34/50, Model: GAE1, Loss: 7818.0390625Epoch 20/50, Model: GAE2, Loss: 3197387.25\n",
      "\n",
      "Epoch 35/50, Model: GAE1, Loss: 7818.0390625tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 36/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 21/50, Model: GAE2, Loss: 3197387.25\n",
      "\n",
      "Epoch 37/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 38/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 39/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 40/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 41/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 22/50, Model: GAE2, Loss: 3197387.25Epoch 42/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 43/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 23/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 44/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Epoch 45/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 46/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 47/50, Model: GAE1, Loss: 7818.0390625\n",
      "Epoch 24/50, Model: GAE2, Loss: 3197387.25Epoch 48/50, Model: GAE1, Loss: 7818.0390625\n",
      "\n",
      "Epoch 49/50, Model: GAE1, Loss: 7818.0390625tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "\n",
      "Epoch 25/50, Model: GAE2, Loss: 3197387.25\n",
      "Epoch 50/50, Model: GAE1, Loss: 7818.0390625\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])Training time aging: 7.38960599899292 seconds\n",
      "\n",
      "Epoch 26/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 27/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 28/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 29/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 30/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 31/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 32/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 33/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 34/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 35/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 36/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 37/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 38/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 39/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 40/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 41/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 42/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 43/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 44/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 45/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 46/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 47/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 48/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 49/50, Model: GAE2, Loss: 3197387.25\n",
      "tensor([[ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        [ 15.7749, -58.2102, -34.8476,  ..., -27.2065, -48.2889, -17.7458],\n",
      "        ...,\n",
      "        [ 14.4217, -50.4337, -32.2782,  ..., -20.6976, -39.1249, -19.9011],\n",
      "        [ 14.3259, -50.5901, -32.2198,  ..., -20.6795, -39.2114, -19.8774],\n",
      "        [ 14.4347, -50.4026, -32.2870,  ..., -20.7025, -39.1107, -19.9015]])\n",
      "Epoch 50/50, Model: GAE2, Loss: 3197387.25\n",
      "Training time fluctuation: 7.799413204193115 seconds\n",
      "Training time: 8.442373752593994 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "# target3 = x.clone()\n",
    "# target4 = x.clone()\n",
    "# target5 = x.clone()\n",
    "# target6 = x.clone()\n",
    "# target7 = x.clone()\n",
    "# target8 = x.clone()\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    print(\"called\")\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation2(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation3(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "# def backpropagation_fluctuation4(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation5(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation6(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation7(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "# model3 = GAE2()\n",
    "# model4 = GAE2()\n",
    "# model5 = GAE2()\n",
    "# model6 = GAE2()\n",
    "# model7 = GAE2()\n",
    "# model8 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "# optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
    "# optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)\n",
    "# optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)\n",
    "# optimizer6 = torch.optim.Adam(model6.parameters(), lr=0.001)\n",
    "# optimizer7 = torch.optim.Adam(model7.parameters(), lr=0.001)\n",
    "# optimizer8 = torch.optim.Adam(model8.parameters(), lr=0.001)\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    # future3 = executor.submit(parallel_forward, model3, x, edge_index, target3)\n",
    "    # future4 = executor.submit(parallel_forward, model4, x, edge_index, target4)\n",
    "    # future5 = executor.submit(parallel_forward, model5, x, edge_index, target5)\n",
    "    # future6 = executor.submit(parallel_forward, model6, x, edge_index, target6)    \n",
    "    # future7 = executor.submit(parallel_forward, model7, x, edge_index, target7)   \n",
    "    # future8 = executor.submit(parallel_forward, model8, x, edge_index, target8) \n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    # output3, target3 = future3.result()\n",
    "    # output4, target4 = future4.result()\n",
    "    # output5, target5 = future5.result()\n",
    "    # output6, target6 = future6.result()\n",
    "    # output7, target7 = future7.result()\n",
    "    # output8, target8 = future8.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation2, (model3, optimizer3, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation3, (model4, optimizer4, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation4, (model5, optimizer5, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation5, (model6, optimizer6, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation6, (model7, optimizer7, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation7, (model8, optimizer8, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "# target3 = x.clone()\n",
    "# target4 = x.clone()\n",
    "# target5 = x.clone()\n",
    "# target6 = x.clone()\n",
    "# target7 = x.clone()\n",
    "# target8 = x.clone()\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    print(\"called\")\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation2(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation3(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "# def backpropagation_fluctuation4(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation5(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation6(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# def backpropagation_fluctuation7(args):\n",
    "#     start_time = time.time()\n",
    "#     model, optimizer, criterion, output, other_model_target = args\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         # print(output)\n",
    "#         # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "#         # loss = criterion(output, target) \n",
    "#         loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "#         torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "# model3 = GAE2()\n",
    "# model4 = GAE2()\n",
    "# model5 = GAE2()\n",
    "# model6 = GAE2()\n",
    "# model7 = GAE2()\n",
    "# model8 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "# optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)\n",
    "# optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)\n",
    "# optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)\n",
    "# optimizer6 = torch.optim.Adam(model6.parameters(), lr=0.001)\n",
    "# optimizer7 = torch.optim.Adam(model7.parameters(), lr=0.001)\n",
    "# optimizer8 = torch.optim.Adam(model8.parameters(), lr=0.001)\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    # future3 = executor.submit(parallel_forward, model3, x, edge_index, target3)\n",
    "    # future4 = executor.submit(parallel_forward, model4, x, edge_index, target4)\n",
    "    # future5 = executor.submit(parallel_forward, model5, x, edge_index, target5)\n",
    "    # future6 = executor.submit(parallel_forward, model6, x, edge_index, target6)    \n",
    "    # future7 = executor.submit(parallel_forward, model7, x, edge_index, target7)   \n",
    "    # future8 = executor.submit(parallel_forward, model8, x, edge_index, target8) \n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    # output3, target3 = future3.result()\n",
    "    # output4, target4 = future4.result()\n",
    "    # output5, target5 = future5.result()\n",
    "    # output6, target6 = future6.result()\n",
    "    # output7, target7 = future7.result()\n",
    "    # output8, target8 = future8.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation2, (model3, optimizer3, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation3, (model4, optimizer4, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation4, (model5, optimizer5, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation5, (model6, optimizer6, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation6, (model7, optimizer7, mse_loss, output1, target1))\n",
    "    # executor.submit(backpropagation_fluctuation7, (model8, optimizer8, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GAE1, Loss: 7830.443359375\n",
      "Model: GAE2, Loss: 3221845.25\n",
      "Model: GAE2, Loss: 1515561.5\n",
      "Model: GAE1, Loss: 4551.16796875\n",
      "Model: GAE2, Loss: 627302.125Model: GAE1, Loss: 2764.150390625\n",
      "\n",
      "Model: GAE2, Loss: 324139.75\n",
      "Model: GAE1, Loss: 1960.453369140625\n",
      "Model: GAE2, Loss: 178554.65625\n",
      "Model: GAE1, Loss: 1614.25732421875\n",
      "Model: GAE2, Loss: 145713.640625\n",
      "Model: GAE1, Loss: 1602.0245361328125\n",
      "Model: GAE1, Loss: 1420.546142578125\n",
      "Model: GAE2, Loss: 182462.515625\n",
      "Model: GAE1, Loss: 1389.2672119140625\n",
      "Model: GAE2, Loss: 160519.515625\n",
      "Model: GAE2, Loss: 239274.375\n",
      "Model: GAE1, Loss: 1203.0057373046875\n",
      "Model: GAE2, Loss: 195066.828125\n",
      "Model: GAE1, Loss: 1159.3184814453125\n",
      "Model: GAE1, Loss: 998.6103515625Model: GAE2, Loss: 293449.09375\n",
      "\n",
      "Model: GAE1, Loss: 939.1134643554688\n",
      "Model: GAE2, Loss: 144955.5625\n",
      "Model: GAE1, Loss: 808.5260009765625\n",
      "Model: GAE2, Loss: 366854.0\n",
      "Model: GAE2, Loss: 152465.015625\n",
      "Model: GAE1, Loss: 759.7921752929688\n",
      "Model: GAE2, Loss: 366912.125Model: GAE1, Loss: 651.8637084960938\n",
      "\n",
      "Model: GAE1, Loss: 626.083740234375Model: GAE2, Loss: 135903.375\n",
      "\n",
      "Model: GAE1, Loss: 524.6671142578125Model: GAE2, Loss: 349516.28125\n",
      "\n",
      "Model: GAE2, Loss: 116794.46875\n",
      "Model: GAE1, Loss: 522.814697265625\n",
      "Model: GAE1, Loss: 472.1716003417969Model: GAE2, Loss: 399031.53125\n",
      "\n",
      "Model: GAE1, Loss: 524.21240234375\n",
      "Model: GAE2, Loss: 172814.0\n",
      "Model: GAE1, Loss: 420.21148681640625Model: GAE2, Loss: 486138.21875\n",
      "\n",
      "Model: GAE1, Loss: 463.1765441894531\n",
      "Model: GAE2, Loss: 157176.140625\n",
      "Model: GAE1, Loss: 361.5503845214844Model: GAE2, Loss: 427810.90625\n",
      "\n",
      "Model: GAE1, Loss: 406.6429138183594\n",
      "Model: GAE2, Loss: 187090.9375\n",
      "Model: GAE1, Loss: 315.6109619140625\n",
      "Model: GAE2, Loss: 404536.65625\n",
      "Model: GAE2, Loss: 247637.921875\n",
      "Model: GAE1, Loss: 360.61700439453125\n",
      "Model: GAE1, Loss: 277.6544189453125\n",
      "Model: GAE2, Loss: 383212.875\n",
      "Model: GAE1, Loss: 324.9615478515625Model: GAE2, Loss: 206263.53125\n",
      "\n",
      "Model: GAE1, Loss: 248.38717651367188Model: GAE2, Loss: 380451.4375\n",
      "\n",
      "Model: GAE2, Loss: 198256.1875\n",
      "Model: GAE1, Loss: 299.3177185058594\n",
      "Model: GAE2, Loss: 401175.625\n",
      "Model: GAE1, Loss: 225.67308044433594\n",
      "Model: GAE2, Loss: 145880.328125Model: GAE1, Loss: 278.46502685546875\n",
      "\n",
      "Model: GAE2, Loss: 375100.09375Model: GAE1, Loss: 207.75253295898438\n",
      "\n",
      "Model: GAE1, Loss: 269.0696105957031\n",
      "Model: GAE2, Loss: 147606.796875\n",
      "Model: GAE2, Loss: 340128.28125Model: GAE1, Loss: 215.06869506835938\n",
      "\n",
      "Model: GAE1, Loss: 272.87713623046875\n",
      "Model: GAE2, Loss: 130171.34375\n",
      "Model: GAE1, Loss: 192.75286865234375Model: GAE2, Loss: 305789.59375\n",
      "\n",
      "Model: GAE1, Loss: 247.94955444335938Model: GAE2, Loss: 114302.6171875\n",
      "\n",
      "Model: GAE2, Loss: 289825.3125\n",
      "Model: GAE1, Loss: 173.8143768310547\n",
      "Model: GAE1, Loss: 231.3769073486328\n",
      "Model: GAE2, Loss: 107231.890625\n",
      "Model: GAE2, Loss: 270613.125\n",
      "Model: GAE1, Loss: 161.07797241210938\n",
      "Model: GAE1, Loss: 219.34011840820312\n",
      "Model: GAE2, Loss: 102889.421875\n",
      "Model: GAE1, Loss: 149.6593475341797\n",
      "Model: GAE2, Loss: 255286.21875\n",
      "Model: GAE2, Loss: 105037.171875Model: GAE1, Loss: 208.88629150390625\n",
      "\n",
      "Model: GAE2, Loss: 238732.21875Model: GAE1, Loss: 144.29928588867188\n",
      "\n",
      "Model: GAE1, Loss: 196.4898681640625Model: GAE2, Loss: 91525.015625\n",
      "\n",
      "Model: GAE1, Loss: 143.42979431152344\n",
      "Model: GAE2, Loss: 268483.5\n",
      "Model: GAE2, Loss: 130661.5703125Model: GAE1, Loss: 195.45516967773438\n",
      "\n",
      "Model: GAE1, Loss: 150.78347778320312Model: GAE2, Loss: 255290.328125\n",
      "\n",
      "Model: GAE2, Loss: 185404.671875Model: GAE1, Loss: 188.77517700195312\n",
      "\n",
      "Training time: 41.22909665107727 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    # start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    # Forward pass with the other model's output\n",
    "    current_output = model(x, edge_index)\n",
    "    loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    current_output = model(x, edge_index)\n",
    "    loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "    torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "        future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "        future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "\n",
    "    \n",
    "        output1, target1 = future1.result()\n",
    "        output2, target2 = future2.result()\n",
    "\n",
    "        executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "        executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, target1))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Model Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from torch_geometric.nn import ChebConv\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Layer.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "        stride (int): Stride for convolution.\n",
    "        padding (int): Padding for convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride: int, padding: int):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "        self.conv_2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "        self.conv_3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass through temporal convolution block.\n",
    "\n",
    "        Args:\n",
    "            X (torch.FloatTensor): Input data of shape (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Output data of shape (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n",
    "\n",
    "\n",
    "\n",
    "class TemporalDeConv1(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Deconvolutional Layer.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "        stride (int): Stride for deconvolution.\n",
    "        padding (int): Padding for deconvolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride: int, padding: int):\n",
    "        super(TemporalDeConv1, self).__init__()\n",
    "        self.conv_1 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size), (1, stride), (0,padding))\n",
    "        self.conv_2 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride), (0,padding))\n",
    "        self.conv_3 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride), (0,padding))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass through temporal convolution block.\n",
    "\n",
    "        Args:\n",
    "            X (torch.FloatTensor): Input data of shape (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Output data of shape (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n",
    "\n",
    "\n",
    "class TemporalDeConv2(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Deconvolutional Layer (Simplified).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "        stride (int): Stride for deconvolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride: int):\n",
    "        super(TemporalDeConv2, self).__init__()\n",
    "        self.conv_1 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride))\n",
    "        self.conv_2 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride))\n",
    "        self.conv_3 = nn.ConvTranspose2d(in_channels, out_channels, (1, kernel_size),(1, stride))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass through temporal convolution block.\n",
    "\n",
    "        Args:\n",
    "            X (torch.FloatTensor): Input data of shape (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Output data of shape (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n",
    "\n",
    "\n",
    "class STConvEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Convolutional Encoder Block.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): Number of nodes in the input graph.\n",
    "        in_channels (int): Number of input features.\n",
    "        hidden_channels (int): Number of hidden features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Length of the temporal kernel.\n",
    "        stride (int): Stride for convolution.\n",
    "        padding (int): Padding for convolution.\n",
    "        K (int): Size of the Chebyshev filter for spatial convolution.\n",
    "        normalization (str, optional): Type of normalization (\"sym\" for symmetric Laplacian or \"rw\" for random walk Laplacian).\n",
    "        bias (bool, optional): Whether to include bias in convolutional layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(STConvEncoder, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "\n",
    "        self._temporal_conv1 = TemporalConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size, stride = stride, padding = padding,\n",
    "        )\n",
    "\n",
    "        self._graph_conv = ChebConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            K=K,\n",
    "            normalization=normalization,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self._temporal_conv2 = TemporalConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size, stride = stride, padding = padding,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,) -> torch.FloatTensor:\n",
    "\n",
    "        r\"\"\"Forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph.\n",
    "        Arg types:\n",
    "            * **X** (PyTorch FloatTensor) - Sequence of node features of shape (Batch size X Input time steps X Num nodes X In channels).\n",
    "            * **edge_index** (PyTorch LongTensor) - Graph edge indices.\n",
    "            * **edge_weight** (PyTorch LongTensor, optional)- Edge weight vector.\n",
    "        Return types:\n",
    "            * **T** (PyTorch FloatTensor) - Sequence of node features.\n",
    "        \"\"\"\n",
    "        #print(X.shape)\n",
    "        T_0 = self._temporal_conv1(X)\n",
    "        #print(T_0.shape)\n",
    "        T = torch.zeros_like(T_0).to(T_0.device)\n",
    "        for b in range(T_0.size(0)):\n",
    "            for t in range(T_0.size(1)):\n",
    "                T[b][t] = self._graph_conv(T_0[b][t], edge_index, edge_weight)\n",
    "\n",
    "        T = F.relu(T)\n",
    "        T = self._temporal_conv2(T)\n",
    "        return T\n",
    "\n",
    "class STConvDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Convolutional Decoder Block.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): Number of nodes in the input graph.\n",
    "        in_channels (int): Number of input features.\n",
    "        hidden_channels (int): Number of hidden features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Length of the temporal kernel for deconvolution.\n",
    "        kernel_size_de (int): Length of the temporal kernel for deconvolution.\n",
    "        stride (int): Stride for deconvolution.\n",
    "        padding (int): Padding for deconvolution.\n",
    "        K (int): Size of the Chebyshev filter for spatial convolution.\n",
    "        normalization (str, optional): Type of normalization (\"sym\" for symmetric Laplacian or \"rw\" for random walk Laplacian).\n",
    "        bias (bool, optional): Whether to include bias in convolutional layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        kernel_size_de: int,\n",
    "        stride: int,\n",
    "        padding: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(STConvDecoder, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "\n",
    "        self._temporal_conv1 = TemporalDeConv1(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size, stride = stride, padding = padding,\n",
    "        )\n",
    "\n",
    "        self._graph_conv = ChebConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            K=K,\n",
    "            normalization=normalization,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self._temporal_conv2 = TemporalDeConv2(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size_de, stride = stride,\n",
    "        )\n",
    "\n",
    "        self._batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,) -> torch.FloatTensor:\n",
    "\n",
    "        r\"\"\"Forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph.\n",
    "        Arg types:\n",
    "            * **X** (PyTorch FloatTensor) - Sequence of node features of shape (Batch size X Input time steps X Num nodes X In channels).\n",
    "            * **edge_index** (PyTorch LongTensor) - Graph edge indices.\n",
    "            * **edge_weight** (PyTorch LongTensor, optional)- Edge weight vector.\n",
    "        Return types:\n",
    "            * **T** (PyTorch FloatTensor) - Sequence of node features.\n",
    "        \"\"\"\n",
    "        T_0 = self._temporal_conv1(X)\n",
    "        T = torch.zeros_like(T_0).to(T_0.device)\n",
    "        for b in range(T_0.size(0)):\n",
    "            for t in range(T_0.size(1)):\n",
    "                T[b][t] = self._graph_conv(T_0[b][t], edge_index, edge_weight)\n",
    "\n",
    "        T = F.relu(T)\n",
    "        T = self._temporal_conv2(T)\n",
    "        return T\n",
    "\n",
    "\n",
    "class STConvAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Convolutional Autoencoder.\n",
    "\n",
    "    Args:\n",
    "        device (str): The PyTorch device to use ('cpu' or 'cuda').\n",
    "        num_nodes (int): Number of nodes in the input graph.\n",
    "        channel_size_list (list): List of channel sizes for each layer.\n",
    "        num_layers (int): Number of STConv blocks.\n",
    "        kernel_size (int): Length of the temporal kernel.\n",
    "        K (int): Size of the Chebyshev filter for spatial convolution.\n",
    "        window_size (int): Number of historical time steps to consider.\n",
    "        kernel_size_de (int): Length of the temporal kernel for deconvolution.\n",
    "        stride (int): Stride for convolution and deconvolution.\n",
    "        padding (int): Padding for convolution and deconvolution.\n",
    "        normalization (str, optional): Type of normalization (\"sym\" for symmetric Laplacian or \"rw\" for random walk Laplacian).\n",
    "        bias (bool, optional): Whether to include bias in convolutional layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, num_nodes, channel_size_list, num_layers,\n",
    "                 kernel_size, K, window_size, kernel_size_de, stride, padding, \\\n",
    "                 normalization='sym', bias=True):\n",
    "\n",
    "        super(STConvAE, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        # add STConv blocks\n",
    "        for l in range(num_layers):\n",
    "            input_size, hidden_size, output_size = channel_size_list[l][0], channel_size_list[l][1], \\\n",
    "                                                   channel_size_list[l][2]\n",
    "            if l == 0:\n",
    "                self.layers.append(\n",
    "                    STConvEncoder(num_nodes, input_size, hidden_size, output_size, kernel_size, stride, padding, K,\n",
    "                                  normalization, bias))\n",
    "            if l == 1:\n",
    "                self.layers.append(\n",
    "                    STConvDecoder(num_nodes, input_size, hidden_size, output_size, kernel_size, kernel_size_de, stride,\n",
    "                                  padding, K, normalization, bias))\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer = layer.to(device)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_weight)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "num_nodes = 100\n",
    "channels = np.array([[1, 4, 8], [8, 4, 1]])\n",
    "kernel_size = 4 # size of temporal kernel\n",
    "kernel_size_de = 2 # size of temporal deconv2\n",
    "stride = 2\n",
    "padding = 1\n",
    "K = 3 # chebyshev filter size\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "num_layers = 2 # number of STConv blocks\n",
    "n_his = 60 # window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 120, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add dimensions using unsqueeze\n",
    "x = x.unsqueeze(0).unsqueeze(-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2., 3., 4., 5., 6., 7., 8., 9., 1., 1., 1., 2., 3., 4., 5., 6.,\n",
       "        7., 8., 2., 1., 1., 1., 2., 3., 4., 5., 6., 7., 3., 2., 1., 1., 1., 2.,\n",
       "        3., 4., 5., 6., 4., 3., 2., 1., 1., 1., 2., 3., 4., 5., 5., 4., 3., 2.,\n",
       "        1., 1., 1., 2., 3., 4., 6., 5., 4., 3., 2., 1., 1., 1., 2., 3., 7., 6.,\n",
       "        5., 4., 3., 2., 1., 1., 1., 2., 8., 7., 6., 5., 4., 3., 2., 1., 1., 1.,\n",
       "        9., 8., 7., 6., 5., 4., 3., 2., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "Futures submitted.\n",
      "Waiting for results...\n",
      "called\n",
      "called\n",
      "Results received.\n",
      "Starting backpropagation...\n",
      "Backpropagation submitted.\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 1/10, Model: STConvAE, Loss: 841.5541381835938\n",
      "Epoch 1/10, Model: STConvAE, Loss: 841.5541381835938\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 2/10, Model: STConvAE, Loss: 814.9131469726562\n",
      "Epoch 2/10, Model: STConvAE, Loss: 801.080078125\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 3/10, Model: STConvAE, Loss: 759.1952514648438Epoch 3/10, Model: STConvAE, Loss: 791.7333374023438\n",
      "\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 4/10, Model: STConvAE, Loss: 773.287109375\n",
      "Epoch 4/10, Model: STConvAE, Loss: 714.0277709960938\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 5/10, Model: STConvAE, Loss: 759.6444091796875\n",
      "Epoch 5/10, Model: STConvAE, Loss: 667.736328125\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 6/10, Model: STConvAE, Loss: 749.4383544921875\n",
      "Epoch 6/10, Model: STConvAE, Loss: 622.2667236328125\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 7/10, Model: STConvAE, Loss: 742.40576171875\n",
      "Epoch 7/10, Model: STConvAE, Loss: 578.757568359375\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 8/10, Model: STConvAE, Loss: 738.9530029296875\n",
      "Epoch 8/10, Model: STConvAE, Loss: 538.1590576171875\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 9/10, Model: STConvAE, Loss: 501.7861633300781\n",
      "Epoch 9/10, Model: STConvAE, Loss: 736.6298217773438\n",
      "tensor([[[[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[11.8053],\n",
      "          [11.0480],\n",
      "          [11.0316],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7163],\n",
      "          [ 1.8308],\n",
      "          [ 1.8736],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.6383],\n",
      "          [ 3.8312],\n",
      "          [ 3.8488],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]],\n",
      "\n",
      "         [[ 3.5697],\n",
      "          [ 3.7343],\n",
      "          [ 3.7657],\n",
      "          ...,\n",
      "          [ 0.0000],\n",
      "          [ 0.0000],\n",
      "          [ 0.0000]]]])\n",
      "Epoch 10/10, Model: STConvAE, Loss: 470.9014892578125\n",
      "Training time aging: 14.571890830993652 seconds\n",
      "Epoch 10/10, Model: STConvAE, Loss: 733.7640991210938\n",
      "Training time fluctuation: 15.100547552108765 seconds\n",
      "Training time: 16.792221546173096 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, edge_weight, target):\n",
    "    print(\"called\")\n",
    "    output = model(x, edge_index, edge_weight)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index, edge_weight)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        loss = criterion(current_output + output, x) \n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index, edge_weight)\n",
    "        print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        # loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        # torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        loss = criterion(current_output + output, x) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True)\n",
    "model2 = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True)\n",
    "\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Parallel forward propagation\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, edge_weight, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, edge_weight, target2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, target1))\n",
    "\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True).to(device)\n",
    "# define loss function\n",
    "loss = nn.MSELoss()\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 60, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,:60,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "called\n",
      "calledcalled\n",
      "called\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Futures submitted.\n",
      "Waiting for results...\n",
      "torch.Size([1, 100, 120, 1])\n",
      "Results received.\n",
      "Starting backpropagation...\n",
      "Backpropagation submitted.\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "Epoch 1/10, Model: STConvAE, Loss: 1691.200439453125\n",
      "Epoch 1/10, Model: STConvAE, Loss: 1691.200439453125Epoch 1/10, Model: STConvAE, Loss: 1395.3995361328125\n",
      "\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "\n",
      "Epoch 2/10, Model: STConvAE, Loss: 1690.2467041015625Epoch 2/10, Model: STConvAE, Loss: 1690.2362060546875\n",
      "\n",
      "Epoch 2/10, Model: STConvAE, Loss: 1356.092529296875\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "\n",
      "Epoch 3/10, Model: STConvAE, Loss: 1689.258056640625\n",
      "Epoch 3/10, Model: STConvAE, Loss: 1689.1080322265625Epoch 3/10, Model: STConvAE, Loss: 1320.9730224609375\n",
      "\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "Epoch 4/10, Model: STConvAE, Loss: 1688.29736328125\n",
      "Epoch 4/10, Model: STConvAE, Loss: 1288.4256591796875Epoch 4/10, Model: STConvAE, Loss: 1687.781005859375\n",
      "\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "Epoch 5/10, Model: STConvAE, Loss: 1687.35595703125\n",
      "Epoch 5/10, Model: STConvAE, Loss: 1258.0386962890625\n",
      "Epoch 5/10, Model: STConvAE, Loss: 1686.2435302734375\n",
      "Epoch 6/10, Model: STConvAE, Loss: 1686.4312744140625\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "\n",
      "Epoch 6/10, Model: STConvAE, Loss: 1229.8792724609375Epoch 6/10, Model: STConvAE, Loss: 1684.49755859375\n",
      "\n",
      "Epoch 7/10, Model: STConvAE, Loss: 1685.5447998046875tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "Epoch 7/10, Model: STConvAE, Loss: 1682.50244140625\n",
      "Epoch 7/10, Model: STConvAE, Loss: 1204.368408203125\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])Epoch 8/10, Model: STConvAE, Loss: 1684.5738525390625\n",
      "\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "Epoch 8/10, Model: STConvAE, Loss: 1680.067626953125\n",
      "Epoch 8/10, Model: STConvAE, Loss: 1182.076416015625\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "Epoch 9/10, Model: STConvAE, Loss: 1683.5074462890625\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])\n",
      "Epoch 9/10, Model: STConvAE, Loss: 1676.96533203125\n",
      "Epoch 9/10, Model: STConvAE, Loss: 1163.29052734375\n",
      "tensor([[[[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[6.6532],\n",
      "          [6.9546],\n",
      "          [6.8715],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]],\n",
      "\n",
      "         [[5.0252],\n",
      "          [5.3105],\n",
      "          [5.2311],\n",
      "          ...,\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000]]]])\n",
      "Epoch 10/10, Model: STConvAE, Loss: 1682.3197021484375\n",
      "Training time aging: 43.29681038856506 seconds\n",
      "tensor([[[[3.5370],\n",
      "          [3.5197],\n",
      "          [3.5524],\n",
      "          ...,\n",
      "          [1.1176],\n",
      "          [1.1224],\n",
      "          [1.1220]],\n",
      "\n",
      "         [[0.0000],\n",
      "          [0.0000],\n",
      "          [0.0000],\n",
      "          ...,\n",
      "          [0.5151],\n",
      "          [0.5224],\n",
      "          [0.5237]],\n",
      "\n",
      "         [[1.4074],\n",
      "          [1.3051],\n",
      "          [1.4686],\n",
      "          ...,\n",
      "          [1.1063],\n",
      "          [1.1159],\n",
      "          [1.1230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0631],\n",
      "          [0.6938],\n",
      "          [0.8019],\n",
      "          ...,\n",
      "          [0.4624],\n",
      "          [0.4597],\n",
      "          [0.4624]],\n",
      "\n",
      "         [[0.1867],\n",
      "          [0.1696],\n",
      "          [0.1746],\n",
      "          ...,\n",
      "          [0.9860],\n",
      "          [0.9979],\n",
      "          [0.9860]],\n",
      "\n",
      "         [[1.0936],\n",
      "          [1.1065],\n",
      "          [1.1028],\n",
      "          ...,\n",
      "          [0.4570],\n",
      "          [0.4590],\n",
      "          [0.4570]]]])Epoch 10/10, Model: STConvAE, Loss: 1673.3389892578125\n",
      "\n",
      "Training time fluctuation: 43.61328935623169 seconds\n",
      "Epoch 10/10, Model: STConvAE, Loss: 1147.84228515625\n",
      "Training time fluctuation: 43.78265023231506 seconds\n",
      "Training time: 48.46286106109619 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss  \n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, edge_weight, target):\n",
    "    print(\"called\")\n",
    "    output = model(x, edge_index, edge_weight)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        print(x.shape)\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x[:,:,60:,:], edge_index, edge_weight)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        loss = criterion(current_output + output, x) \n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "\n",
    "def backpropagation_aging2(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index, edge_weight)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        loss = criterion(current_output + output, x) \n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index, edge_weight)\n",
    "        print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        # loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        # torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        loss = criterion(current_output + output, x) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation2(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index, edge_weight)\n",
    "        print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        # loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        # torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        loss = criterion(current_output + output, x) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model_a1 = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True)\n",
    "model_a2 = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True)\n",
    "model_f1 = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True)\n",
    "model_f2 = STConvAE(device, num_nodes, channels, num_layers, kernel_size, K, n_his, kernel_size_de, stride, padding, normalization = 'sym', bias = True)\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model_a1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model_a2.parameters(), lr=0.001)\n",
    "optimizer3 = torch.optim.Adam(model_f1.parameters(), lr=0.001)\n",
    "optimizer4 = torch.optim.Adam(model_f2.parameters(), lr=0.001)\n",
    "\n",
    "# Parallel forward propagation\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model_a1, x, edge_index, edge_weight, target1)\n",
    "    future2 = executor.submit(parallel_forward, model_a2, x, edge_index, edge_weight, target2)\n",
    "    future3 = executor.submit(parallel_forward, model_f1, x, edge_index, edge_weight, target2)\n",
    "    future4 = executor.submit(parallel_forward, model_f2, x, edge_index, edge_weight, target2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    output3, target3 = future3.result()\n",
    "    output4, target4 = future4.result()\n",
    "\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging1, (model_a1, optimizer1, mse_loss, output3, target2))\n",
    "    executor.submit(backpropagation_aging2, (model_a2, optimizer2, mse_loss, output4, target2))\n",
    "    executor.submit(backpropagation_fluctuation1, (model_f1, optimizer3, mse_loss, output1, target1))\n",
    "    executor.submit(backpropagation_fluctuation2, (model_f2, optimizer4, mse_loss, output2, target1))\n",
    "\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 60, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,60:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 9.984124422073364 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    # start_time = time.time()\n",
    "    model, optimizer, criterion, output, current_output, other_model_target = args\n",
    "    loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    model, optimizer, criterion, output, current_output, other_model_target = args\n",
    "    loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "    torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    for epoch in range(num_epochs):\n",
    "        future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "        future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "\n",
    "    \n",
    "        output1, target1 = future1.result()\n",
    "        output2, target2 = future2.result()\n",
    "\n",
    "        executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, output1, target2))\n",
    "        executor.submit(backpropagation_fluctuation1, (model2, optimizer2, mse_loss, output1, output2, target1))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m     output1, target1 \u001b[38;5;241m=\u001b[39m future1\n\u001b[1;32m    136\u001b[0m     output2, target2 \u001b[38;5;241m=\u001b[39m future2\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mbackpropagation_aging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m:=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     backpropagation_fluctuation1((args\u001b[38;5;241m:=\u001b[39m(model2, optimizer2, mse_loss, output1, output2, target1)))\n\u001b[1;32m    142\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[10], line 93\u001b[0m, in \u001b[0;36mbackpropagation_aging\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(current_output \u001b[38;5;241m+\u001b[39m output, x) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mvar(torch\u001b[38;5;241m.\u001b[39mdiff(current_output))\n\u001b[1;32m     92\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 93\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Print loss at each epoch\u001b[39;00m\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    # start_time = time.time()\n",
    "    model, optimizer, criterion, output, current_output, other_model_target = args\n",
    "    loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation1(args):\n",
    "    model, optimizer, criterion, output, current_output, other_model_target = args\n",
    "    loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "    torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    future1 = parallel_forward(model1, x, edge_index, target1)\n",
    "    future2 = parallel_forward(model2, x, edge_index, target2)\n",
    "\n",
    "\n",
    "    output1, target1 = future1\n",
    "    output2, target2 = future2\n",
    "\n",
    "    backpropagation_aging((args:=(model1, optimizer1, mse_loss, output2, output1, target2)))\n",
    "    backpropagation_fluctuation1((args:=(model2, optimizer2, mse_loss, output1, output2, target1)))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "Futures submitted.\n",
      "Waiting for results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-40:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-5:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-33:\n",
      "Process ForkProcess-34:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-41:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-35:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-36:\n",
      "Process ForkProcess-39:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-27:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-38:\n",
      "Process ForkProcess-22:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-30:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-42:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-14:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-37:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-29:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFutures submitted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m output1, target1 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m output2, target2 \u001b[38;5;241m=\u001b[39m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults received.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # print(output)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-76:\n",
      "Process ForkProcess-65:\n",
      "Process ForkProcess-45:\n",
      "Process ForkProcess-55:\n",
      "Process ForkProcess-72:\n",
      "Process ForkProcess-60:\n",
      "Process ForkProcess-74:\n",
      "Process ForkProcess-82:\n",
      "Process ForkProcess-63:\n",
      "Process ForkProcess-64:\n",
      "Process ForkProcess-61:\n",
      "Process ForkProcess-80:\n",
      "Process ForkProcess-57:\n",
      "Process ForkProcess-47:\n",
      "Process ForkProcess-49:\n",
      "Process ForkProcess-56:\n",
      "Process ForkProcess-69:\n",
      "Process ForkProcess-77:\n",
      "Process ForkProcess-62:\n",
      "Process ForkProcess-81:\n",
      "Process ForkProcess-78:\n",
      "Process ForkProcess-53:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-73:\n",
      "Process ForkProcess-52:\n",
      "Process ForkProcess-67:\n",
      "Process ForkProcess-51:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-71:\n",
      "Process ForkProcess-70:\n",
      "Process ForkProcess-68:\n",
      "Process ForkProcess-66:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-59:\n",
      "Process ForkProcess-46:\n",
      "Process ForkProcess-75:\n",
      "Process ForkProcess-79:\n",
      "Process ForkProcess-50:\n",
      "Process ForkProcess-54:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-48:\n",
      "Process ForkProcess-58:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 166\u001b[0m\n\u001b[1;32m    162\u001b[0m future2 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model2, x, edge_index, target2)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# print(\"Futures submitted.\")\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(\"Waiting for results...\")\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m output1, target1 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m output2, target2 \u001b[38;5;241m=\u001b[39m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# print(\"Results received.\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# print(\"Starting backpropagation...\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    # start_time = time.time()\n",
    "    model, optimizer, criterion, output, current_output, other_model_target, epoch = args\n",
    "    # print(output)\n",
    "    # for epoch in range(num_epochs):\n",
    "    # Forward pass with the other model's output\n",
    "    current_output = model(x, edge_index)\n",
    "    loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output)) + torch.var(current_output)\n",
    "    # loss = criterion(output, target)\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Epoch {epoch+1}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed_time = end_time - start_time\n",
    "    # print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation(args):\n",
    "    # start_time = time.time()\n",
    "    model, optimizer, criterion, output, current_output, other_model_target, epoch = args\n",
    "    # for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "    current_output = model(x, edge_index)\n",
    "    # print(output)\n",
    "    # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "    # loss = criterion(output, target) \n",
    "    loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "    torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss at each epoch\n",
    "    print(f'Epoch {epoch+1}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed_time = end_time - start_time\n",
    "    # print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Parallel forward propagation\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(\"Submitting futures...\")\n",
    "        future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "        future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "        # print(\"Futures submitted.\")\n",
    "    \n",
    "        # print(\"Waiting for results...\")\n",
    "        output1, target1 = future1.result()\n",
    "        output2, target2 = future2.result()\n",
    "        # print(\"Results received.\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        # print(\"Starting backpropagation...\")\n",
    "        executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, output1, target2, epoch))\n",
    "        executor.submit(backpropagation_fluctuation, (model2, optimizer2, mse_loss, output1, output2, target1, epoch))\n",
    "        # print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Alternate between forward and backward propagation for each epoch\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward propagation\n",
    "#     process1 = multiprocessing.Process(target=parallel_forward, args=(model1, input_data, output_queue1, target1))\n",
    "#     process2 = multiprocessing.Process(target=parallel_forward, args=(model2, input_data, output_queue2, target2))\n",
    "\n",
    "#     process1.start()\n",
    "#     process2.start()\n",
    "\n",
    "#     process1.join()\n",
    "#     process2.join()\n",
    "\n",
    "#     # Backward propagation\n",
    "#     backprop_process1 = multiprocessing.Process(target=backpropagation, args=(model1, optimizer1, nn.CrossEntropyLoss(), output_queue2, target2))\n",
    "#     backprop_process2 = multiprocessing.Process(target=backpropagation, args=(model2, optimizer2, nn.CrossEntropyLoss(), output_queue1, target1))\n",
    "\n",
    "#     backprop_process1.start()\n",
    "#     backprop_process2.start()\n",
    "\n",
    "#     backprop_process1.join()\n",
    "#     backprop_process2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "Futures submitted.\n",
      "Waiting for results...\n",
      "Results received.\n",
      "Starting backpropagation...\n",
      "Backpropagation submitted.\n",
      "Epoch 1/100, Model: GAE2, Loss: 3221845.25\n",
      "Epoch 2/100, Model: GAE2, Loss: 1516573.125\n",
      "Epoch 3/100, Model: GAE2, Loss: 655325.5625\n",
      "Epoch 4/100, Model: GAE2, Loss: 267109.09375\n",
      "Epoch 5/100, Model: GAE2, Loss: 230921.734375\n",
      "Epoch 6/100, Model: GAE2, Loss: 386658.0625\n",
      "Epoch 7/100, Model: GAE2, Loss: 678982.625\n",
      "Epoch 8/100, Model: GAE2, Loss: 1020604.9375\n",
      "Epoch 9/100, Model: GAE2, Loss: 1501049.5\n",
      "Epoch 10/100, Model: GAE2, Loss: 2017814.0\n",
      "Epoch 11/100, Model: GAE2, Loss: 2556926.75\n",
      "Epoch 12/100, Model: GAE2, Loss: 3122349.0\n",
      "Epoch 13/100, Model: GAE2, Loss: 3723630.5\n",
      "Epoch 14/100, Model: GAE2, Loss: 4436397.0\n",
      "Epoch 15/100, Model: GAE2, Loss: 4813869.0\n",
      "Epoch 16/100, Model: GAE2, Loss: 5121729.0\n",
      "Epoch 17/100, Model: GAE2, Loss: 5276498.0\n",
      "Epoch 18/100, Model: GAE2, Loss: 5340437.5\n",
      "Epoch 19/100, Model: GAE2, Loss: 5230909.0\n",
      "Epoch 20/100, Model: GAE2, Loss: 5030095.5\n",
      "Epoch 21/100, Model: GAE2, Loss: 4757405.0\n",
      "Epoch 22/100, Model: GAE2, Loss: 4439091.5\n",
      "Epoch 23/100, Model: GAE2, Loss: 4046691.0\n",
      "Epoch 24/100, Model: GAE2, Loss: 3594611.5\n",
      "Epoch 25/100, Model: GAE2, Loss: 3327518.5\n",
      "Epoch 26/100, Model: GAE2, Loss: 2975695.5\n",
      "Epoch 27/100, Model: GAE2, Loss: 2672630.25\n",
      "Epoch 28/100, Model: GAE2, Loss: 2463153.75\n",
      "Epoch 29/100, Model: GAE2, Loss: 2267513.75\n",
      "Epoch 30/100, Model: GAE2, Loss: 2026031.375\n",
      "Epoch 31/100, Model: GAE2, Loss: 1927801.75\n",
      "Epoch 32/100, Model: GAE2, Loss: 1798810.5\n",
      "Epoch 33/100, Model: GAE2, Loss: 1717585.5\n",
      "Epoch 34/100, Model: GAE2, Loss: 1649146.875\n",
      "Epoch 35/100, Model: GAE2, Loss: 1583667.375\n",
      "Epoch 36/100, Model: GAE2, Loss: 1542407.25\n",
      "Epoch 37/100, Model: GAE2, Loss: 1485514.875\n",
      "Epoch 38/100, Model: GAE2, Loss: 1516377.375\n",
      "Epoch 39/100, Model: GAE2, Loss: 1534299.125\n",
      "Epoch 40/100, Model: GAE2, Loss: 1595366.0\n",
      "Epoch 41/100, Model: GAE2, Loss: 1640407.75\n",
      "Epoch 42/100, Model: GAE2, Loss: 1685878.625\n",
      "Epoch 43/100, Model: GAE2, Loss: 1747028.5\n",
      "Epoch 44/100, Model: GAE2, Loss: 1836862.625\n",
      "Epoch 45/100, Model: GAE2, Loss: 1902725.0\n",
      "Epoch 46/100, Model: GAE2, Loss: 1973082.625\n",
      "Epoch 47/100, Model: GAE2, Loss: 1996505.5\n",
      "Epoch 48/100, Model: GAE2, Loss: 2080073.375\n",
      "Epoch 49/100, Model: GAE2, Loss: 2128181.5\n",
      "Epoch 50/100, Model: GAE2, Loss: 2112490.75\n",
      "Epoch 51/100, Model: GAE2, Loss: 2198339.25\n",
      "Epoch 52/100, Model: GAE2, Loss: 2237249.0\n",
      "Epoch 53/100, Model: GAE2, Loss: 2258328.0\n",
      "Epoch 54/100, Model: GAE2, Loss: 2245786.0\n",
      "Epoch 55/100, Model: GAE2, Loss: 2277074.25\n",
      "Epoch 56/100, Model: GAE2, Loss: 2319235.5\n",
      "Epoch 57/100, Model: GAE2, Loss: 2311634.0\n",
      "Epoch 58/100, Model: GAE2, Loss: 2298537.5\n",
      "Epoch 59/100, Model: GAE2, Loss: 2285539.0\n",
      "Epoch 60/100, Model: GAE2, Loss: 2293921.5\n",
      "Epoch 61/100, Model: GAE2, Loss: 2252211.25\n",
      "Epoch 62/100, Model: GAE2, Loss: 2248745.75\n",
      "Epoch 63/100, Model: GAE2, Loss: 2240175.25\n",
      "Epoch 64/100, Model: GAE2, Loss: 2268790.0\n",
      "Epoch 65/100, Model: GAE2, Loss: 2266769.75\n",
      "Epoch 66/100, Model: GAE2, Loss: 2246521.5\n",
      "Epoch 67/100, Model: GAE2, Loss: 2210249.25\n",
      "Epoch 68/100, Model: GAE2, Loss: 2204849.0\n",
      "Epoch 69/100, Model: GAE2, Loss: 2204848.75\n",
      "Epoch 70/100, Model: GAE2, Loss: 2228810.5\n",
      "Epoch 71/100, Model: GAE2, Loss: 2250368.25\n",
      "Epoch 72/100, Model: GAE2, Loss: 2249828.25\n",
      "Epoch 73/100, Model: GAE2, Loss: 2284167.0\n",
      "Epoch 74/100, Model: GAE2, Loss: 2285540.75\n",
      "Epoch 75/100, Model: GAE2, Loss: 2285267.75\n",
      "Epoch 76/100, Model: GAE2, Loss: 2285645.5\n",
      "Epoch 77/100, Model: GAE2, Loss: 2286527.5\n",
      "Epoch 78/100, Model: GAE2, Loss: 2268067.25\n",
      "Epoch 79/100, Model: GAE2, Loss: 2268067.25\n",
      "Epoch 80/100, Model: GAE2, Loss: 2268707.0\n",
      "Epoch 81/100, Model: GAE2, Loss: 2268707.0\n",
      "Epoch 82/100, Model: GAE2, Loss: 2264012.0\n",
      "Epoch 83/100, Model: GAE2, Loss: 2184507.75\n",
      "Epoch 84/100, Model: GAE2, Loss: 2170126.75\n",
      "Epoch 85/100, Model: GAE2, Loss: 2163533.75\n",
      "Epoch 86/100, Model: GAE2, Loss: 2146611.75\n",
      "Epoch 87/100, Model: GAE2, Loss: 2139166.5\n",
      "Epoch 88/100, Model: GAE2, Loss: 2139166.5\n",
      "Epoch 89/100, Model: GAE2, Loss: 2141826.5\n",
      "Epoch 90/100, Model: GAE2, Loss: 2144686.5\n",
      "Epoch 91/100, Model: GAE2, Loss: 2144686.25\n",
      "Epoch 92/100, Model: GAE2, Loss: 2144686.25\n",
      "Epoch 93/100, Model: GAE2, Loss: 2168786.25\n",
      "Epoch 94/100, Model: GAE2, Loss: 2168786.25\n",
      "Epoch 95/100, Model: GAE2, Loss: 2168726.25\n",
      "Epoch 96/100, Model: GAE2, Loss: 2168726.25\n",
      "Epoch 97/100, Model: GAE2, Loss: 2168726.0\n",
      "Epoch 98/100, Model: GAE2, Loss: 2168726.0\n",
      "Epoch 99/100, Model: GAE2, Loss: 2168726.0\n",
      "Epoch 100/100, Model: GAE2, Loss: 2168726.0\n",
      "Training time fluctuation: 1.368708848953247 seconds\n",
      "Training time: 2.378089666366577 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output)) + torch.var(h_a)\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.924871174589607"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(122.00995969772339+123.03065609931946)/127.30234575271606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTqZyRmo8wPe",
    "outputId": "aa1f4f67-5f48-4dda-c43a-8b1b605e4157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAE1(\n",
      "  (conv1): ChebConv(120, 60, K=1, normalization=sym)\n",
      "  (conv2): GATConv(60, 30, heads=1)\n",
      "  (conv3): ChebConv(30, 60, K=1, normalization=sym)\n",
      "  (conv4): GATConv(60, 120, heads=1)\n",
      ")\n",
      "GAE2(\n",
      "  (conv1): ChebConv(120, 60, K=1, normalization=sym)\n",
      "  (conv2): GATConv(60, 30, heads=1)\n",
      "  (conv3): ChebConv(30, 60, K=1, normalization=sym)\n",
      "  (conv4): GATConv(60, 120, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, TransformerConv, AGNNConv, FusedGATConv, ChebConv, SAGEConv, GraphConv, CuGraphGATConv, TAGConv, GMMConv\n",
    "from scipy.stats import linregress\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE1, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_1 = self.conv1(x, edge_index)\n",
    "    h_2 = self.conv2(h_1, edge_index)\n",
    "    h_3 = self.conv3(h_2, edge_index)\n",
    "    h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "    return h_a\n",
    "\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE2, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_5 = self.conv1(x, edge_index)\n",
    "    h_6 = self.conv2(h_5, edge_index)\n",
    "    h_7 = self.conv3(h_6, edge_index)\n",
    "    h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "    return h_f\n",
    "\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "print(model1)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAE1(model, x, edge_index, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        h_a = model(x, edge_index)\n",
    "        loss = torch.var(h_a)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Model {model.__class__.__name__}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time for {model.__class__.__name__}: {elapsed_time} seconds\")\n",
    "    \n",
    "\n",
    "def train_GAE2(model, x, edge_index, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        h_f = model(x, edge_index)\n",
    "        loss = torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])) + torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Model {model.__class__.__name__}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time for {model.__class__.__name__}: {elapsed_time} seconds\")\n",
    "    \n",
    "# def train_GAE2(x, edge_index, Lambda1, Lambda2):\n",
    "#   optimizer.zero_grad()\n",
    "#   h_a, h_f = model(x, edge_index)\n",
    "#   loss = criterion(h_a+h_f, x)  + \\\n",
    "#   10*torch.std(torch.diff(h_a)) +\\\n",
    "#   Lambda1 * torch.var(h_a) +\\\n",
    "#   Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])) +\\\n",
    "#   Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0))\n",
    "#   loss.backward()\n",
    "#   optimizer.step()\n",
    "#   return loss, criterion(h_a+h_f, x), h_a, h_f\n",
    "\n",
    "\n",
    "\n",
    "# def train_model(model, data, epochs):\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         inputs, targets = data\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         print(f\"Model {model.__class__.__name__}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time for {model.__class__.__name__}: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for GAE1: 10.309388399124146 seconds\n",
      "Training time for GAE2: 11.01285719871521 seconds\n",
      "Training time: 11.029053688049316 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from scipy.stats import linregress\n",
    "from concurrent.futures import ThreadPoolExecutor, wait  # Import wait from concurrent.futures\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 500\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create two instances of the GAE models\n",
    "    model1 = GAE1()\n",
    "    model2 = GAE2()\n",
    "\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to the ThreadPoolExecutor\n",
    "        future1 = executor.submit(train_GAE1, model1, x, edge_index, num_epochs)\n",
    "        future2 = executor.submit(train_GAE2, model2, x, edge_index, num_epochs)\n",
    "\n",
    "        # Wait for both tasks to finish\n",
    "        wait([future1, future2])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE1, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_2 = self.conv2(h_1, edge_index)\n",
    "        h_3 = self.conv3(h_2, edge_index)\n",
    "        h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "        return h_a\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAE2, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120)  # Adjust output size for regression\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_5 = self.conv1(x, edge_index)\n",
    "        h_6 = self.conv2(h_5, edge_index)\n",
    "        h_7 = self.conv3(h_6, edge_index)\n",
    "        h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "        return h_f\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "# Assuming input features x, edge_index, and targets for regression\n",
    "# x = torch.randn(100, 120)\n",
    "# edge_index = torch.randint(0, 100, (2, 500))\n",
    "target1 = x.clone()\n",
    "target2 = x.clone()\n",
    "\n",
    "\n",
    "# manager = multiprocessing.Manager()\n",
    "# output_queue1 = manager.Queue()\n",
    "# output_queue2 = manager.Queue()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, target):\n",
    "    output = model(x, edge_index)\n",
    "    # output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "    \n",
    "    return output.detach(), target.detach() \n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time aging: {elapsed_time} seconds\")\n",
    "    \n",
    "# Function for backpropagation\n",
    "def backpropagation_fluctuation(args):\n",
    "    start_time = time.time()\n",
    "    model, optimizer, criterion, output, other_model_target = args\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        # loss = criterion(output, target) \n",
    "        loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training time fluctuation: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# Parallel forward propagation\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, target1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, target2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1, target1 = future1.result()\n",
    "    output2, target2 = future2.result()\n",
    "    print(\"Results received.\")\n",
    "    \n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, (model1, optimizer1, mse_loss, output2, target2))\n",
    "    executor.submit(backpropagation_fluctuation, (model2, optimizer2, mse_loss, output1, target1))\n",
    "    print(\"Backpropagation submitted.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "Futures submitted.\n",
      "Waiting for results...\n",
      "Results received.\n",
      "Starting backpropagation...\n",
      "Backpropagation submitted.\n",
      "Epoch 1/10, Model: GAE1, Loss: 5935.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-508:\n",
      "Process ForkProcess-521:\n",
      "Process ForkProcess-529:\n",
      "Process ForkProcess-525:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-509:\n",
      "Process ForkProcess-523:\n",
      "Process ForkProcess-524:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-507:\n",
      "Process ForkProcess-516:\n",
      "Process ForkProcess-522:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-511:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-518:\n",
      "Process ForkProcess-517:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-520:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-515:\n",
      "Process ForkProcess-514:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-519:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-513:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-496:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-505:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-512:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-506:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-526:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-510:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-527:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-494:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-491:\n",
      "Process ForkProcess-490:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-504:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-498:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-528:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-495:\n",
      "Process ForkProcess-497:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-493:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-502:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-501:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-499:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-503:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-500:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m optimizer1 \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model1\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    119\u001b[0m optimizer2 \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmitting futures...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m     future1 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model1, x, edge_index, output_queue1)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py:780\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_executor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-492:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "import concurrent\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE1, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_1 = self.conv1(x, edge_index)\n",
    "    h_2 = self.conv2(h_1, edge_index)\n",
    "    h_3 = self.conv3(h_2, edge_index)\n",
    "    h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "    return h_a\n",
    "\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE2, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_5 = self.conv1(x, edge_index)\n",
    "    h_6 = self.conv2(h_5, edge_index)\n",
    "    h_7 = self.conv3(h_6, edge_index)\n",
    "    h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "    return h_f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "output_queue1 = manager.Queue()\n",
    "output_queue2 = manager.Queue()\n",
    "\n",
    "# # Function for parallel forward propagation\n",
    "# def parallel_forward(model, x, edge_index, output_queue):\n",
    "#     with torch.no_grad():\n",
    "#         output = model(x, edge_index)\n",
    "\n",
    "#     # Put the output into the queue\n",
    "#     output_queue.put(output)\n",
    "    \n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, output_queue):\n",
    "    output = model(x, edge_index)\n",
    "    output_queue.put(output.detach())\n",
    "    \n",
    "    return output.detach()\n",
    "    \n",
    "        \n",
    "def backpropagation_aging(model, optimizer, criterion, output_queue):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest output from the other model\n",
    "        output = output_queue.get()\n",
    "\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # loss = criterion(current_output + output, x) + torch.var(torch.diff(current_output))\n",
    "        loss = criterion(current_output + output, x)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation(model, optimizer, criterion, output_queue):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest output from the other model\n",
    "        other_model_output = output_queue.get()\n",
    "\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        # loss = criterion(current_output + output, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(current_output, 0, 1))[1])) +\\\n",
    "        # torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(current_output, E.T).long(), L.long()), torch.matmul(E, current_output.T).long()), 0))\n",
    "        loss = criterion(current_output + output, x) \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... (your model instances, optimizers, etc.)\n",
    "criterion = nn.MSELoss() \n",
    "num_epochs = 10\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, output_queue1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, output_queue2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1 = future1.result()\n",
    "    output2 = future2.result()\n",
    "    print(\"Results received.\")\n",
    "\n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation_aging, model1, optimizer1, criterion, output_queue2)\n",
    "    executor.submit(backpropagation_fluctuation, model2, optimizer2, criterion, output_queue1)\n",
    "    print(\"Backpropagation submitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting futures...\n",
      "Futures submitted.\n",
      "Waiting for results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-624:\n",
      "Process ForkProcess-607:\n",
      "Process ForkProcess-628:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-622:\n",
      "Process ForkProcess-618:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-611:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-621:\n",
      "Process ForkProcess-616:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-603:\n",
      "Process ForkProcess-623:\n",
      "Process ForkProcess-608:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-627:\n",
      "Process ForkProcess-625:\n",
      "Process ForkProcess-629:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-630:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-614:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-626:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-604:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-605:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-620:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-609:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-610:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-635:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-613:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-617:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-636:\n",
      "Process ForkProcess-612:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-632:\n",
      "Process ForkProcess-637:\n",
      "Process ForkProcess-606:\n",
      "Process ForkProcess-638:\n",
      "Process ForkProcess-619:\n",
      "Process ForkProcess-631:\n",
      "Process ForkProcess-602:\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-633:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-615:\n",
      "Process ForkProcess-601:\n",
      "Process ForkProcess-634:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFutures submitted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m output1 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m output2 \u001b[38;5;241m=\u001b[39m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults received.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE1, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_1 = self.conv1(x, edge_index)\n",
    "    h_2 = self.conv2(h_1, edge_index)\n",
    "    h_3 = self.conv3(h_2, edge_index)\n",
    "    h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "    return h_a\n",
    "\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE2, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_5 = self.conv1(x, edge_index)\n",
    "    h_6 = self.conv2(h_5, edge_index)\n",
    "    h_7 = self.conv3(h_6, edge_index)\n",
    "    h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "    return h_f\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss() \n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Queue for sharing outputs between models\n",
    "manager = multiprocessing.Manager()\n",
    "output_queue1 = manager.Queue()\n",
    "output_queue2 = manager.Queue()\n",
    "\n",
    "# Lock to synchronize access to the queues\n",
    "queue_lock = multiprocessing.Lock()\n",
    "\n",
    "\n",
    "# def parallel_forward(model, x, edge_index, output_queue):\n",
    "#     with torch.no_grad():\n",
    "#         output = model(x, edge_index)\n",
    "\n",
    "#     # Put the output into the queue\n",
    "#     output_queue.put(output)\n",
    "\n",
    "# # Function for backpropagation\n",
    "# def backpropagation(model, optimizer, criterion, output_queue):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Get the latest output from the other model\n",
    "#         other_model_output = output_queue.get()\n",
    "\n",
    "#         # Forward pass with the other model's output\n",
    "#         current_output = model(x, edge_index)\n",
    "#         loss = criterion(current_output, other_model_output)\n",
    "\n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print loss at each epoch\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "def parallel_forward(model, x, edge_index, output_queue):\n",
    "    with torch.no_grad():\n",
    "        output = model(x, edge_index)\n",
    "\n",
    "    # Put the model parameters into the queue\n",
    "    output_queue.put([param.cpu().detach().numpy() for param in model.parameters()])\n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation(model, optimizer, criterion, output_queue):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest model parameters from the other model\n",
    "        other_model_params = output_queue.get()\n",
    "\n",
    "        # Update the model parameters\n",
    "        for param, other_param in zip(model.parameters(), other_model_params):\n",
    "            param.data = torch.from_numpy(other_param)\n",
    "\n",
    "        # Forward pass with the updated model\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    print(\"Submitting futures...\")\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, output_queue1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, output_queue2)\n",
    "    print(\"Futures submitted.\")\n",
    "\n",
    "    print(\"Waiting for results...\")\n",
    "    output1 = future1.result()\n",
    "    output2 = future2.result()\n",
    "    print(\"Results received.\")\n",
    "\n",
    "    # Backpropagation\n",
    "    print(\"Starting backpropagation...\")\n",
    "    executor.submit(backpropagation, model1, optimizer1, criterion, output_queue2)\n",
    "    executor.submit(backpropagation, model2, optimizer2, criterion, output_queue1)\n",
    "    print(\"Backpropagation submitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GAE1:\n\tMissing key(s) in state_dict: \"conv1.bias\", \"conv1.lins.0.weight\", \"conv2.att_src\", \"conv2.att_dst\", \"conv2.bias\", \"conv2.lin_src.weight\", \"conv2.lin_dst.weight\", \"conv3.bias\", \"conv3.lins.0.weight\", \"conv4.att_src\", \"conv4.att_dst\", \"conv4.bias\", \"conv4.lin_src.weight\", \"conv4.lin_dst.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/tmp/job.793795.hpc/ipykernel_66200/1401072845.py\", line 9, in parallel_forward\n    model.load_state_dict(dict(shared_params))\n  File \"/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py\", line 2152, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for GAE1:\n\tMissing key(s) in state_dict: \"conv1.bias\", \"conv1.lins.0.weight\", \"conv2.att_src\", \"conv2.att_dst\", \"conv2.bias\", \"conv2.lin_src.weight\", \"conv2.lin_dst.weight\", \"conv3.bias\", \"conv3.lins.0.weight\", \"conv4.att_src\", \"conv4.att_dst\", \"conv4.bias\", \"conv4.lin_src.weight\", \"conv4.lin_dst.weight\". \n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m future2 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model2, x, edge_index, shared_params)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Wait for the results\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GAE1:\n\tMissing key(s) in state_dict: \"conv1.bias\", \"conv1.lins.0.weight\", \"conv2.att_src\", \"conv2.att_dst\", \"conv2.bias\", \"conv2.lin_src.weight\", \"conv2.lin_dst.weight\", \"conv3.bias\", \"conv3.lins.0.weight\", \"conv4.att_src\", \"conv4.att_dst\", \"conv4.bias\", \"conv4.lin_src.weight\", \"conv4.lin_dst.weight\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel_forward(model, x, edge_index, shared_params):\n",
    "    with torch.no_grad():\n",
    "        model.load_state_dict(dict(shared_params))\n",
    "\n",
    "        output = model(x, edge_index)\n",
    "\n",
    "    # Put the model parameters into the shared_params dict\n",
    "    shared_params.update(model.state_dict())\n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation(model, optimizer, criterion, shared_params):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Update the model parameters\n",
    "        model.load_state_dict(dict(shared_params))\n",
    "\n",
    "        # Forward pass with the updated model\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the shared_params dict with the new model parameters\n",
    "        shared_params.update(model.state_dict())\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "# Use a shared_dict for storing model parameters\n",
    "shared_params = multiprocessing.Manager().dict()\n",
    "\n",
    "# Use the ProcessPoolExecutor\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Submit futures for parallel forward pass\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, shared_params)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, shared_params)\n",
    "\n",
    "    # Wait for the results\n",
    "    future1.result()\n",
    "    future2.result()\n",
    "\n",
    "    # Backpropagation\n",
    "    executor.submit(backpropagation, model1, optimizer1, criterion, shared_params)\n",
    "    executor.submit(backpropagation, model2, optimizer2, criterion, shared_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Queue objects should only be shared between processes through inheritance",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 244, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/usr/lib/python3.10/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 58, in __getstate__\n    context.assert_spawning(self)\n  File \"/usr/lib/python3.10/multiprocessing/context.py\", line 373, in assert_spawning\n    raise RuntimeError(\nRuntimeError: Queue objects should only be shared between processes through inheritance\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m future1 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model1, x, edge_index, output_queue1)\n\u001b[1;32m    103\u001b[0m future2 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model2, x, edge_index, output_queue2)\n\u001b[0;32m--> 105\u001b[0m output1 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m output2 \u001b[38;5;241m=\u001b[39m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:244\u001b[0m, in \u001b[0;36mQueue._feed\u001b[0;34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wacquire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     send_bytes(obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:58\u001b[0m, in \u001b[0;36mQueue.__getstate__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_spawning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_epipe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maxsize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer,\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wlock, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opid)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py:373\u001b[0m, in \u001b[0;36massert_spawning\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_spawning\u001b[39m(obj):\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_spawning_popen() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    374\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m objects should only be shared between processes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    375\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m through inheritance\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    376\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Queue objects should only be shared between processes through inheritance"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, TransformerConv, AGNNConv, FusedGATConv, ChebConv, SAGEConv, GraphConv, CuGraphGATConv, TAGConv, GMMConv\n",
    "from scipy.stats import linregress\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE1, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_1 = self.conv1(x, edge_index)\n",
    "    h_2 = self.conv2(h_1, edge_index)\n",
    "    h_3 = self.conv3(h_2, edge_index)\n",
    "    h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "    return h_a\n",
    "\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE2, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_5 = self.conv1(x, edge_index)\n",
    "    h_6 = self.conv2(h_5, edge_index)\n",
    "    h_7 = self.conv3(h_6, edge_index)\n",
    "    h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "    return h_f\n",
    "      \n",
    "criterion = nn.MSELoss() \n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "x = torch.randn(120, 10)\n",
    "edge_index = torch.randint(0, 120, (2, 500))\n",
    "\n",
    "# Queue for sharing outputs between models\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "\n",
    "# Lock to synchronize access to the queues\n",
    "queue_lock = multiprocessing.Lock()\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, output_queue):\n",
    "    output = model(x, edge_index)\n",
    "    \n",
    "    # Put the output into the queue\n",
    "    with queue_lock:\n",
    "        output_queue.put(output)\n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation(model, optimizer, criterion, output_queue):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest output from the other model\n",
    "        with queue_lock:\n",
    "            other_model_output = output_queue.get()\n",
    "\n",
    "        # Forward pass with the other model's output\n",
    "        current_output = model(x, edge_index)\n",
    "        loss = criterion(current_output, other_model_output)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "# Parallel forward propagation\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index, output_queue1)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index, output_queue2)\n",
    "\n",
    "    output1 = future1.result()\n",
    "    output2 = future2.result()\n",
    "\n",
    "    # Backpropagation\n",
    "    executor.submit(backpropagation, model1, optimizer1, criterion, output_queue2)\n",
    "    executor.submit(backpropagation, model2, optimizer2, criterion, output_queue1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parallel_forward() missing 1 required positional argument: 'output_queue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\nTypeError: parallel_forward() missing 1 required positional argument: 'output_queue'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m future1 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model1, x, edge_index)\n\u001b[1;32m    123\u001b[0m future2 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(parallel_forward, model2, x, edge_index)\n\u001b[0;32m--> 125\u001b[0m output1 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m output2 \u001b[38;5;241m=\u001b[39m future2\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: parallel_forward() missing 1 required positional argument: 'output_queue'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, TransformerConv, AGNNConv, FusedGATConv, ChebConv, SAGEConv, GraphConv, CuGraphGATConv, TAGConv, GMMConv\n",
    "from scipy.stats import linregress\n",
    "\n",
    "class GAE1(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE1, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_1 = self.conv1(x, edge_index)\n",
    "    h_2 = self.conv2(h_1, edge_index)\n",
    "    h_3 = self.conv3(h_2, edge_index)\n",
    "    h_a = self.conv4(h_3, edge_index)\n",
    "\n",
    "    return h_a\n",
    "\n",
    "\n",
    "class GAE2(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GAE2, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = ChebConv(120, 60, 1)\n",
    "    self.conv2 = GATConv(60, 30)\n",
    "    self.conv3 = ChebConv(30, 60, 1)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    h_5 = self.conv1(x, edge_index)\n",
    "    h_6 = self.conv2(h_5, edge_index)\n",
    "    h_7 = self.conv3(h_6, edge_index)\n",
    "    h_f = self.conv4(h_7, edge_index)\n",
    "\n",
    "    return h_f\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss() \n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Queue for sharing outputs between models\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "\n",
    "# Lock to synchronize access to the queues\n",
    "queue_lock = multiprocessing.Lock()\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, x, edge_index, output_queue):\n",
    "    output = model(x, edge_index)\n",
    "    \n",
    "    # Put the output into the queue\n",
    "    with queue_lock:\n",
    "        output_queue.put((output))\n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation_aging(model, optimizer, criterion, output_queue):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest output from the other model\n",
    "        with queue_lock:\n",
    "            h_f = output_queue.get()\n",
    "\n",
    "        # Forward pass with the other model's output\n",
    "        h_a = model(x, edge_index)\n",
    "        # loss = criterion(output, target1) + criterion(other_model_output, other_model_target)\n",
    "        loss = criterion(h_a+h_f, x) + 10 * torch.std(torch.diff(h_a))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "def backpropagation_fluctuation(model, optimizer, criterion, output_queue):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest output from the other model\n",
    "        with queue_lock:\n",
    "            h_a = output_queue.get()\n",
    "\n",
    "        # Forward pass with the other model's output\n",
    "        output = model(x, edge_index)\n",
    "        # loss = criterion(output, target1) + criterion(other_model_output, other_model_target)\n",
    "        loss = criterion(h_a+h_f, x) + torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])) +\\\n",
    "        torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "# Parallel forward propagation\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    future1 = executor.submit(parallel_forward, model1, x, edge_index)\n",
    "    future2 = executor.submit(parallel_forward, model2, x, edge_index)\n",
    "\n",
    "    output1 = future1.result()\n",
    "    output2 = future2.result()\n",
    "\n",
    "    # Backpropagation\n",
    "    executor.submit(backpropagation, (model1, optimizer1, nn.CrossEntropyLoss(), output2))\n",
    "    executor.submit(backpropagation, (model2, optimizer2, nn.CrossEntropyLoss(), output1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-133:\n",
      "Process Process-134:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m process1\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     91\u001b[0m process2\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m---> 92\u001b[0m \u001b[43mbackprop_process1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m backprop_process2\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:43\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/job.793795.hpc/ipykernel_55350/199444358.py\", line 51, in backpropagation\n",
      "    other_model_output, other_model_target = output_queue.get()\n",
      "  File \"/tmp/job.793795.hpc/ipykernel_55350/199444358.py\", line 50, in backpropagation\n",
      "    with queue_lock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import multiprocessing\n",
    "\n",
    "# Define the models\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "\n",
    "# Dummy data (replace with your actual data)\n",
    "input_data = torch.randn(5, 10)\n",
    "target1 = torch.randint(0, 5, (5,))\n",
    "target2 = torch.randint(0, 5, (5,))\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Queue for sharing outputs between models\n",
    "output_queue1 = multiprocessing.Queue()\n",
    "output_queue2 = multiprocessing.Queue()\n",
    "\n",
    "# Lock to synchronize access to the queues\n",
    "queue_lock = multiprocessing.Lock()\n",
    "\n",
    "# Function for parallel forward propagation\n",
    "def parallel_forward(model, input_data, output_queue, target):\n",
    "    output = model(input_data)\n",
    "    \n",
    "    # Put the output into the queue\n",
    "    with queue_lock:\n",
    "        output_queue.put((output, target))\n",
    "\n",
    "# Function for backpropagation\n",
    "def backpropagation(model, optimizer, criterion, output_queue, other_model_target):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get the latest output from the other model\n",
    "        with queue_lock:\n",
    "            other_model_output, other_model_target = output_queue.get()\n",
    "\n",
    "        # Forward pass with the other model's output\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, target1) + criterion(other_model_output, other_model_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss at each epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Model: {model.__class__.__name__}, Loss: {loss.item()}')\n",
    "\n",
    "# Create instances of the models\n",
    "model1 = Model1()\n",
    "model2 = Model2()\n",
    "\n",
    "# Optimizers for each model\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "# Create processes for parallel learning\n",
    "process1 = multiprocessing.Process(target=parallel_forward, args=(model1, input_data, output_queue1, target1))\n",
    "process2 = multiprocessing.Process(target=parallel_forward, args=(model2, input_data, output_queue2, target2))\n",
    "\n",
    "# Start the processes\n",
    "process1.start()\n",
    "process2.start()\n",
    "\n",
    "# Backpropagation processes\n",
    "backprop_process1 = multiprocessing.Process(target=backpropagation, args=(model1, optimizer1, nn.CrossEntropyLoss(), output_queue2, target2))\n",
    "backprop_process2 = multiprocessing.Process(target=backpropagation, args=(model2, optimizer2, nn.CrossEntropyLoss(), output_queue1, target1))\n",
    "\n",
    "# Start the backpropagation processes\n",
    "backprop_process1.start()\n",
    "backprop_process2.start()\n",
    "\n",
    "# Wait for all processes to finish\n",
    "process1.join()\n",
    "process2.join()\n",
    "backprop_process1.join()\n",
    "backprop_process2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv, GATConv\n",
    "from scipy.stats import linregress\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 100\n",
    "\n",
    "# Create two instances of the GAE models\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks to the ThreadPoolExecutor\n",
    "        future1 = executor.submit(train_GAE1, model1, x, edge_index, num_epochs)\n",
    "        future2 = executor.submit(train_GAE2, model2, x, edge_index, num_epochs)\n",
    "\n",
    "        # Wait for both tasks to finish\n",
    "        concurrent.futures.wait([future1, future2])\n",
    "       # x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Function to train a model\n",
    "# def train_model(model, data, epochs):\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         inputs, targets = data\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         print(f\"Model {model.__class__.__name__}, Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Training time for {model.__class__.__name__}: {elapsed_time} seconds\")\n",
    "\n",
    "# Create two instances of the complex neural network model\n",
    "model1 = GAE1()\n",
    "model2 = GAE2()\n",
    "\n",
    "# # Dummy data for training\n",
    "# data1 = (torch.randn(5, 10), torch.randn(5, 1))\n",
    "# data2 = (torch.randn(5, 10), torch.randn(5, 1))\n",
    "\n",
    "# Number of epochs for training\n",
    "num_epochs = 100\n",
    "\n",
    "# # start_time = time.time()\n",
    "# # Create two processes for training each model\n",
    "# process1 = multiprocessing.Process(target=train_GAE1, args=(model1, x, edge_index, num_epochs))\n",
    "# process2 = multiprocessing.Process(target=train_GAE2, args=(model2, x, edge_index, num_epochs))\n",
    "\n",
    "# # Start the processes\n",
    "# process1.start()\n",
    "# process2.start()\n",
    "\n",
    "# # Wait for both processes to finish\n",
    "# process1.join()\n",
    "process2.join()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create two processes for training each model\n",
    "    process1 = multiprocessing.Process(target=train_GAE1, args=(model1, x, edge_index, num_epochs))\n",
    "    process2 = multiprocessing.Process(target=train_GAE2, args=(model2, x, edge_index, num_epochs))\n",
    "\n",
    "    # Start the processes\n",
    "    process1.start()\n",
    "    process2.start()\n",
    "\n",
    "    # Wait for both processes to finish\n",
    "    process1.join()\n",
    "    process2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "# import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Lambda1 = 0\n",
    "Lambda2 = 100\n",
    "Lambda3 = 5\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "start_time = time.time()\n",
    "\n",
    "def train(x, edge_index, Lambda1, Lambda2):\n",
    "  optimizer.zero_grad()\n",
    "  h_a, h_f = model(x, edge_index)\n",
    "  loss = criterion(h_a+h_f, x)  + \\\n",
    "  10*torch.std(torch.diff(h_a)) +\\\n",
    "  Lambda1 * torch.var(h_a) +\\\n",
    "  Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])) +\\\n",
    "  Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss, criterion(h_a+h_f, x), h_a, h_f\n",
    "\n",
    "epochs = range(1, 500)\n",
    "losses = []\n",
    "embeddings = []\n",
    "min_loss = 1000\n",
    "for epoch in epochs:\n",
    "  loss, loss_2, h_a, h_f= train(x, edge_index, Lambda1, Lambda2)\n",
    "  losses.append(loss)\n",
    "  # print(f\"Epoch: {epoch}\\tLoss: {loss:.4f}\")\n",
    "  # if loss_2 < 100:\n",
    "  #   break\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GATConv(120, 60, heads=1)\n",
      "  (conv2): GATConv(60, 120, heads=1)\n",
      "  (conv3): GATConv(120, 60, heads=1)\n",
      "  (conv4): GATConv(60, 120, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, TransformerConv, AGNNConv, FusedGATConv, ChebConv, SAGEConv, GraphConv, CuGraphGATConv, TAGConv, GMMConv\n",
    "from scipy.stats import linregress\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GCN, self).__init__()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    self.conv1 = GATConv(120, 60)\n",
    "    self.conv2 = GATConv(60, 120)\n",
    "    self.conv3 = GATConv(120, 60)\n",
    "    self.conv4 = GATConv(60, 120)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "\n",
    "\n",
    "    h_1 = self.conv1(x, edge_index)\n",
    "    h_a = self.conv2(h_1, edge_index)\n",
    "\n",
    "\n",
    "    h_2 = self.conv3(x, edge_index)\n",
    "    h_f = self.conv4(h_2, edge_index)\n",
    "\n",
    "    return h_a, h_f\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-NVdyxW_Cs2",
    "outputId": "4c83bce1-f88f-4d3d-e899-e2dc80fec114"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcudart.so.11.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChebConv, GATConv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGCN\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/__init__.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backend and logging should be imported before other modules.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_verbose_logging  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_name, load_backend  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     container,\n\u001b[1;32m     18\u001b[0m     cuda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     storages,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ffi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, DGLError\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/backend/__init__.py:122\u001b[0m\n\u001b[1;32m    118\u001b[0m         set_default_backend(default_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 122\u001b[0m \u001b[43mload_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_preferred_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_enabled\u001b[39m(api):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the api is enabled by the current backend.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m        True if the API is enabled by the current backend.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/backend/__init__.py:51\u001b[0m, in \u001b[0;36mload_backend\u001b[0;34m(mod_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m mod_name)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ffi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tensor_adapter  \u001b[38;5;66;03m# imports DGL C library\u001b[39;00m\n\u001b[1;32m     53\u001b[0m version \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     54\u001b[0m load_tensor_adapter(mod_name, version)\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/_ffi/base.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# library instance of nnvm\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m _LIB, _LIB_NAME, _DIR_NAME \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# The FFI mode of DGL\u001b[39;00m\n\u001b[1;32m     53\u001b[0m _FFI_MODE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDGL_FFI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/_ffi/base.py:39\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load libary by searching possible path.\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39mfind_lib_path()\n\u001b[0;32m---> 39\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m dirname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(lib_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m basename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_path[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcudart.so.11.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn.pytorch import ChebConv, GATConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30, 1)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120, 1)\n",
    "\n",
    "        self.conv5 = ChebConv(120, 60, 1)\n",
    "        self.conv6 = GATConv(60, 30, 1)\n",
    "        self.conv7 = ChebConv(30, 60, 1)\n",
    "        self.conv8 = GATConv(60, 120, 1)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        h_1 = self.conv1(g, x)\n",
    "        h_2 = self.conv2(g, h_1)\n",
    "        h_3 = self.conv3(g, h_2)\n",
    "        h_a = self.conv4(g, h_3)\n",
    "\n",
    "        h_5 = self.conv5(g, x)\n",
    "        h_6 = self.conv6(g, h_5)\n",
    "        h_7 = self.conv7(g, h_6)\n",
    "        h_f = self.conv8(g, h_7)\n",
    "\n",
    "        return h_a, h_f\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzOWwsiOKAy5",
    "outputId": "1484b1b6-4f10-4e23-a6ed-1feff567f2af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GATConv(\n",
      "    (fc): Linear(in_features=120, out_features=60, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv2): GATConv(\n",
      "    (fc): Linear(in_features=60, out_features=120, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): GATConv(\n",
      "    (fc): Linear(in_features=120, out_features=60, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv4): GATConv(\n",
      "    (fc): Linear(in_features=60, out_features=120, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn.pytorch import ChebConv, GATConv, DotGatConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = GATConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 120, 1)\n",
    "        self.conv3 = GATConv(120, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120, 1)\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h_1 = self.conv1(blocks[0], x)\n",
    "        h_a = self.conv2(blocks[1], h_1)\n",
    "        h_2 = self.conv3(blocks[0], x)\n",
    "        h_f = self.conv4(blocks[1], h_2)\n",
    "\n",
    "        return h_a, h_f\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): ChebConv(\n",
      "    (linear): Linear(in_features=120, out_features=60, bias=True)\n",
      "  )\n",
      "  (conv2): GATConv(\n",
      "    (fc): Linear(in_features=60, out_features=30, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv3): ChebConv(\n",
      "    (linear): Linear(in_features=30, out_features=60, bias=True)\n",
      "  )\n",
      "  (conv4): GATConv(\n",
      "    (fc): Linear(in_features=60, out_features=120, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv5): ChebConv(\n",
      "    (linear): Linear(in_features=120, out_features=60, bias=True)\n",
      "  )\n",
      "  (conv6): GATConv(\n",
      "    (fc): Linear(in_features=60, out_features=30, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (conv7): ChebConv(\n",
      "    (linear): Linear(in_features=30, out_features=60, bias=True)\n",
      "  )\n",
      "  (conv8): GATConv(\n",
      "    (fc): Linear(in_features=60, out_features=120, bias=False)\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn.pytorch import ChebConv, GATConv, GraphConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.conv1 = ChebConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 30, 1)\n",
    "        self.conv3 = ChebConv(30, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120, 1)\n",
    "\n",
    "        self.conv5 = ChebConv(120, 60, 1)\n",
    "        self.conv6 = GATConv(60, 30, 1)\n",
    "        self.conv7 = ChebConv(30, 60, 1)\n",
    "        self.conv8 = GATConv(60, 120, 1)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        # h_1 = self.conv1(g, in_feat)\n",
    "        \n",
    "        # h_a = self.conv2(g, h_1).relu()\n",
    "        # h_2 = self.conv3(g, in_feat)\n",
    "        # h_f = self.conv4(g, h_2).relu()\n",
    "\n",
    "        h_1 = self.conv1(g, x)\n",
    "        h_2 = self.conv2(g, h_1).relu()\n",
    "        h_3 = self.conv3(g, h_2)\n",
    "        h_a = self.conv4(g, h_3).relu()\n",
    "\n",
    "        h_5 = self.conv5(g, x)\n",
    "        h_6 = self.conv6(g, h_5).relu()\n",
    "        h_7 = self.conv7(g, h_6)\n",
    "        h_f = self.conv8(g, h_7).relu()\n",
    "\n",
    "\n",
    "        return h_a, h_f\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn.pytorch import ChebConv, GATConv, GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GATConv(120, 60, 1)\n",
    "        self.conv2 = GATConv(60, 120, 1)\n",
    "        self.conv3 = GATConv(120, 60, 1)\n",
    "        self.conv4 = GATConv(60, 120, 1)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jSRG5UBu8wWR"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcudart.so.11.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sampler \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mdataloading\u001b[38;5;241m.\u001b[39mMultiLayerFullNeighborSampler(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/__init__.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backend and logging should be imported before other modules.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_verbose_logging  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_name, load_backend  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     container,\n\u001b[1;32m     18\u001b[0m     cuda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     storages,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ffi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, DGLError\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/backend/__init__.py:122\u001b[0m\n\u001b[1;32m    118\u001b[0m         set_default_backend(default_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 122\u001b[0m \u001b[43mload_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_preferred_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_enabled\u001b[39m(api):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the api is enabled by the current backend.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m        True if the API is enabled by the current backend.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/backend/__init__.py:51\u001b[0m, in \u001b[0;36mload_backend\u001b[0;34m(mod_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m mod_name)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ffi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tensor_adapter  \u001b[38;5;66;03m# imports DGL C library\u001b[39;00m\n\u001b[1;32m     53\u001b[0m version \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     54\u001b[0m load_tensor_adapter(mod_name, version)\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/_ffi/base.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# library instance of nnvm\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m _LIB, _LIB_NAME, _DIR_NAME \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# The FFI mode of DGL\u001b[39;00m\n\u001b[1;32m     53\u001b[0m _FFI_MODE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDGL_FFI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/_ffi/base.py:39\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load libary by searching possible path.\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39mfind_lib_path()\n\u001b[0;32m---> 39\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m dirname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(lib_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m basename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_path[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcudart.so.11.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "\n",
    "pip install dgl-cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -dlefleets (/home/rxf131/ondemand/ubuntu2204/python310)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dlefleets (/home/rxf131/ondemand/ubuntu2204/python310)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement dgl-cu121 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for dgl-cu121\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dlefleets (/home/rxf131/ondemand/ubuntu2204/python310)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dlefleets (/home/rxf131/ondemand/ubuntu2204/python310)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dlefleets (/home/rxf131/ondemand/ubuntu2204/python310)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dgl-cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "D2ixevx99ckh"
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# PyG Data object\n",
    "pyg_data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Extract PyG graph information\n",
    "num_nodes = pyg_data.num_nodes\n",
    "num_edges = pyg_data.num_edges\n",
    "src, dst = pyg_data.edge_index\n",
    "\n",
    "# Create DGL graph\n",
    "dgl_graph = dgl.graph((src, dst), num_nodes=num_nodes)\n",
    "\n",
    "# Optional: Set node features for DGL graph\n",
    "dgl_graph.ndata['x'] = pyg_data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mLtRaGM9scZ",
    "outputId": "d4ddd403-c066-40ed-bf66-74186c50a913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=100, num_edges=2746,\n",
       "      ndata_schemes={'x': Scheme(shape=(120,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4xPhi1fB8wZX"
   },
   "outputs": [],
   "source": [
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    dgl_graph, dgl_graph.nodes().long(), sampler,\n",
    "    batch_size=25,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pOAHNLxUEpvS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.413655757904053 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "# import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Lambda1 = 0\n",
    "Lambda2 = 100\n",
    "Lambda3 = 5\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "start_time = time.time()\n",
    "\n",
    "def train(x, edge_index, Lambda1, Lambda2):\n",
    "  optimizer.zero_grad()\n",
    "  h_a, h_f = model(x, edge_index)\n",
    "  loss = criterion(h_a+h_f, x)  + \\\n",
    "  10*torch.std(torch.diff(h_a)) +\\\n",
    "  Lambda1 * torch.var(h_a) +\\\n",
    "  Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])) +\\\n",
    "  Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0))\n",
    "\n",
    "  # print(criterion(h_a + h_f, x))\n",
    "  # print(torch.std(torch.diff(h_a)))\n",
    "  # print(torch.var(h_a))\n",
    "  # print(torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])))\n",
    "  # print(torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0)))\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss, criterion(h_a+h_f, x), h_a, h_f\n",
    "\n",
    "epochs = range(1, 500)\n",
    "losses = []\n",
    "embeddings = []\n",
    "min_loss = 1000\n",
    "for epoch in epochs:\n",
    "  loss, loss_2, h_a, h_f= train(x, edge_index, Lambda1, Lambda2)\n",
    "  losses.append(loss)\n",
    "  # print(f\"Epoch: {epoch}\\tLoss: {loss:.4f}\")\n",
    "  # if loss_2 < 100:\n",
    "  #   break\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.6694, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5692, grad_fn=<StdBackward0>)\n",
      "tensor(6.3497, grad_fn=<VarBackward0>)\n",
      "tensor(0.0146, grad_fn=<SumBackward0>)\n",
      "tensor(13080)\n"
     ]
    }
   ],
   "source": [
    "  print(criterion(h_a + h_f, x))\n",
    "  print(torch.std(torch.diff(h_a)))\n",
    "  print(torch.var(h_a))\n",
    "  print(torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])))\n",
    "  print(torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "YbuDMxw-OF7T",
    "outputId": "76e62c16-bff1-446a-9f00-c06cc0593abc"
   },
   "outputs": [],
   "source": [
    "blocks[1].dstdata['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WbO8pJs8we9",
    "outputId": "7b328a45-af36-4283-de67-0c354c10ea51"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "# else torch.device(\"cpu\")\n",
    "\n",
    "# model = model.to(device)\n",
    "# m = m.to(device)\n",
    "# E = E.to(device)\n",
    "# L = L.to(device)\n",
    "# # # Move the input features to the same device as the model\n",
    "# # input_features = input_features.to(device)\n",
    "\n",
    "# for input_nodes, output_nodes, blocks in dataloader:\n",
    "#     blocks = [b.to(device) for b in blocks]\n",
    "#     input_features = blocks[0].srcdata['x'].to(device)\n",
    "#     ground_truth = blocks[1].dstdata['x'].to(device)\n",
    "#     h_a, h_f = model(blocks, input_features)\n",
    "#     loss = criterion(torch.squeeze(h_a)+torch.squeeze(h_f), ground_truth)  + \\\n",
    "#     10*torch.std(torch.diff(h_a)) +\\\n",
    "#     Lambda1 * torch.var(h_a) +\\\n",
    "#     Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(torch.squeeze(h_f), 0, 1))[1])) +\\\n",
    "#     Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).float(), L.float()), torch.matmul(E, torch.squeeze(h_f).T).float()), 0))\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[48.5786, 46.4354, 50.6477,  ..., 33.6993, 37.9428, 47.2802],\n",
       "        [48.5926, 46.4799, 50.7236,  ..., 33.2002, 37.3683, 46.5494],\n",
       "        [48.6029, 46.5166, 50.7899,  ..., 32.7940, 36.9030, 45.9515],\n",
       "        ...,\n",
       "        [41.7236, 44.8196, 43.9540,  ..., 40.6397, 42.6384, 40.6262],\n",
       "        [41.7287, 44.8377, 43.9773,  ..., 40.4663, 42.4543, 40.4481],\n",
       "        [41.7256, 44.8226, 43.9507,  ..., 40.6659, 42.6745, 40.6584]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl_graph.ndata[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52CLJdO4Q0Vs",
    "outputId": "2b9cc536-80e2-411f-8f4e-066b03c87ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2485.2222, grad_fn=<MseLossBackward0>)\n",
      "tensor(30.2983, grad_fn=<StdBackward0>)\n",
      "tensor(458.8483, grad_fn=<VarBackward0>)\n",
      "tensor(0.4608, grad_fn=<SumBackward0>)\n",
      "tensor(108331)\n",
      "Epoch: 1\tLoss: 544489.2500\n",
      "tensor(2131.1416, grad_fn=<MseLossBackward0>)\n",
      "tensor(23.0252, grad_fn=<StdBackward0>)\n",
      "tensor(272.3502, grad_fn=<VarBackward0>)\n",
      "tensor(4.5825, grad_fn=<SumBackward0>)\n",
      "tensor(204195)\n",
      "Epoch: 2\tLoss: 1023794.6250\n",
      "tensor(1884.9075, grad_fn=<MseLossBackward0>)\n",
      "tensor(17.6219, grad_fn=<StdBackward0>)\n",
      "tensor(160.7885, grad_fn=<VarBackward0>)\n",
      "tensor(3.8973, grad_fn=<SumBackward0>)\n",
      "tensor(161619)\n",
      "Epoch: 3\tLoss: 810545.8750\n",
      "tensor(1747.4886, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.8711, grad_fn=<StdBackward0>)\n",
      "tensor(99.9352, grad_fn=<VarBackward0>)\n",
      "tensor(2.0982, grad_fn=<SumBackward0>)\n",
      "tensor(95546)\n",
      "Epoch: 4\tLoss: 479826.0312\n",
      "tensor(1677.5245, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.3457, grad_fn=<StdBackward0>)\n",
      "tensor(66.2654, grad_fn=<VarBackward0>)\n",
      "tensor(0.2143, grad_fn=<SumBackward0>)\n",
      "tensor(44440)\n",
      "Epoch: 5\tLoss: 224012.4062\n",
      "tensor(1628.7217, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.0683, grad_fn=<StdBackward0>)\n",
      "tensor(52.6574, grad_fn=<VarBackward0>)\n",
      "tensor(1.4908, grad_fn=<SumBackward0>)\n",
      "tensor(45480)\n",
      "Epoch: 6\tLoss: 229278.4844\n",
      "tensor(1575.1241, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8792, grad_fn=<StdBackward0>)\n",
      "tensor(50.5515, grad_fn=<VarBackward0>)\n",
      "tensor(1.4087, grad_fn=<SumBackward0>)\n",
      "tensor(41673)\n",
      "Epoch: 7\tLoss: 210179.7812\n",
      "tensor(1526.4991, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.4171, grad_fn=<StdBackward0>)\n",
      "tensor(55.3616, grad_fn=<VarBackward0>)\n",
      "tensor(0.4069, grad_fn=<SumBackward0>)\n",
      "tensor(30928)\n",
      "Epoch: 8\tLoss: 156311.3594\n",
      "tensor(1484.5863, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2510, grad_fn=<StdBackward0>)\n",
      "tensor(63.6143, grad_fn=<VarBackward0>)\n",
      "tensor(0.7186, grad_fn=<SumBackward0>)\n",
      "tensor(44419)\n",
      "Epoch: 9\tLoss: 223763.9531\n",
      "tensor(1438.9355, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.2099, grad_fn=<StdBackward0>)\n",
      "tensor(73.8724, grad_fn=<VarBackward0>)\n",
      "tensor(1.0321, grad_fn=<SumBackward0>)\n",
      "tensor(58107)\n",
      "Epoch: 10\tLoss: 292199.2500\n",
      "tensor(1391.0682, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.2025, grad_fn=<StdBackward0>)\n",
      "tensor(85.7402, grad_fn=<VarBackward0>)\n",
      "tensor(0.8326, grad_fn=<SumBackward0>)\n",
      "tensor(81420)\n",
      "Epoch: 11\tLoss: 408706.3438\n",
      "tensor(1341.2737, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.2375, grad_fn=<StdBackward0>)\n",
      "tensor(99.4131, grad_fn=<VarBackward0>)\n",
      "tensor(0.2912, grad_fn=<SumBackward0>)\n",
      "tensor(89280)\n",
      "Epoch: 12\tLoss: 447912.7500\n",
      "tensor(1288.8467, grad_fn=<MseLossBackward0>)\n",
      "tensor(15.1696, grad_fn=<StdBackward0>)\n",
      "tensor(112.4548, grad_fn=<VarBackward0>)\n",
      "tensor(0.5111, grad_fn=<SumBackward0>)\n",
      "tensor(105130)\n",
      "Epoch: 13\tLoss: 527141.6250\n",
      "tensor(1234.8204, grad_fn=<MseLossBackward0>)\n",
      "tensor(15.9901, grad_fn=<StdBackward0>)\n",
      "tensor(124.4605, grad_fn=<VarBackward0>)\n",
      "tensor(0.8606, grad_fn=<SumBackward0>)\n",
      "tensor(95863)\n",
      "Epoch: 14\tLoss: 480795.7812\n",
      "tensor(1182.1774, grad_fn=<MseLossBackward0>)\n",
      "tensor(16.7105, grad_fn=<StdBackward0>)\n",
      "tensor(135.8672, grad_fn=<VarBackward0>)\n",
      "tensor(0.8512, grad_fn=<SumBackward0>)\n",
      "tensor(117090)\n",
      "Epoch: 15\tLoss: 586884.3750\n",
      "tensor(1129.0574, grad_fn=<MseLossBackward0>)\n",
      "tensor(17.4396, grad_fn=<StdBackward0>)\n",
      "tensor(148.1909, grad_fn=<VarBackward0>)\n",
      "tensor(0.5729, grad_fn=<SumBackward0>)\n",
      "tensor(117666)\n",
      "Epoch: 16\tLoss: 589690.7500\n",
      "tensor(1077.5125, grad_fn=<MseLossBackward0>)\n",
      "tensor(18.1616, grad_fn=<StdBackward0>)\n",
      "tensor(161.1470, grad_fn=<VarBackward0>)\n",
      "tensor(0.0887, grad_fn=<SumBackward0>)\n",
      "tensor(128455)\n",
      "Epoch: 17\tLoss: 643543.0000\n",
      "tensor(1026.9691, grad_fn=<MseLossBackward0>)\n",
      "tensor(18.8674, grad_fn=<StdBackward0>)\n",
      "tensor(174.5638, grad_fn=<VarBackward0>)\n",
      "tensor(0.5677, grad_fn=<SumBackward0>)\n",
      "tensor(134790)\n",
      "Epoch: 18\tLoss: 675222.4375\n",
      "tensor(974.0627, grad_fn=<MseLossBackward0>)\n",
      "tensor(19.5422, grad_fn=<StdBackward0>)\n",
      "tensor(188.1280, grad_fn=<VarBackward0>)\n",
      "tensor(0.8515, grad_fn=<SumBackward0>)\n",
      "tensor(144342)\n",
      "Epoch: 19\tLoss: 722964.6250\n",
      "tensor(920.1432, grad_fn=<MseLossBackward0>)\n",
      "tensor(20.1754, grad_fn=<StdBackward0>)\n",
      "tensor(201.7207, grad_fn=<VarBackward0>)\n",
      "tensor(0.8420, grad_fn=<SumBackward0>)\n",
      "tensor(145941)\n",
      "Epoch: 20\tLoss: 730911.0625\n",
      "tensor(866.3296, grad_fn=<MseLossBackward0>)\n",
      "tensor(20.7553, grad_fn=<StdBackward0>)\n",
      "tensor(215.0539, grad_fn=<VarBackward0>)\n",
      "tensor(0.5929, grad_fn=<SumBackward0>)\n",
      "tensor(145391)\n",
      "Epoch: 21\tLoss: 728088.1875\n",
      "tensor(813.5600, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.2688, grad_fn=<StdBackward0>)\n",
      "tensor(227.7552, grad_fn=<VarBackward0>)\n",
      "tensor(0.1610, grad_fn=<SumBackward0>)\n",
      "tensor(143182)\n",
      "Epoch: 22\tLoss: 716952.3750\n",
      "tensor(763.6072, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.7034, grad_fn=<StdBackward0>)\n",
      "tensor(239.4243, grad_fn=<VarBackward0>)\n",
      "tensor(0.4262, grad_fn=<SumBackward0>)\n",
      "tensor(157227)\n",
      "Epoch: 23\tLoss: 787158.2500\n",
      "tensor(716.1927, grad_fn=<MseLossBackward0>)\n",
      "tensor(22.0485, grad_fn=<StdBackward0>)\n",
      "tensor(249.6771, grad_fn=<VarBackward0>)\n",
      "tensor(0.6382, grad_fn=<SumBackward0>)\n",
      "tensor(178770)\n",
      "Epoch: 24\tLoss: 894850.5000\n",
      "tensor(671.7376, grad_fn=<MseLossBackward0>)\n",
      "tensor(22.2902, grad_fn=<StdBackward0>)\n",
      "tensor(258.0586, grad_fn=<VarBackward0>)\n",
      "tensor(0.5057, grad_fn=<SumBackward0>)\n",
      "tensor(161340)\n",
      "Epoch: 25\tLoss: 807645.1875\n",
      "tensor(630.6115, grad_fn=<MseLossBackward0>)\n",
      "tensor(22.4132, grad_fn=<StdBackward0>)\n",
      "tensor(264.0888, grad_fn=<VarBackward0>)\n",
      "tensor(0.0909, grad_fn=<SumBackward0>)\n",
      "tensor(161920)\n",
      "Epoch: 26\tLoss: 810463.8125\n",
      "tensor(591.7213, grad_fn=<MseLossBackward0>)\n",
      "tensor(22.4007, grad_fn=<StdBackward0>)\n",
      "tensor(267.2784, grad_fn=<VarBackward0>)\n",
      "tensor(0.3462, grad_fn=<SumBackward0>)\n",
      "tensor(150415)\n",
      "Epoch: 27\tLoss: 752925.3750\n",
      "tensor(556.8889, grad_fn=<MseLossBackward0>)\n",
      "tensor(22.2457, grad_fn=<StdBackward0>)\n",
      "tensor(267.4172, grad_fn=<VarBackward0>)\n",
      "tensor(0.2832, grad_fn=<SumBackward0>)\n",
      "tensor(139194)\n",
      "Epoch: 28\tLoss: 696777.6875\n",
      "tensor(525.9990, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.9515, grad_fn=<StdBackward0>)\n",
      "tensor(264.5558, grad_fn=<VarBackward0>)\n",
      "tensor(0.2126, grad_fn=<SumBackward0>)\n",
      "tensor(157960)\n",
      "Epoch: 29\tLoss: 790566.7500\n",
      "tensor(492.2212, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.5317, grad_fn=<StdBackward0>)\n",
      "tensor(259.0112, grad_fn=<VarBackward0>)\n",
      "tensor(0.1854, grad_fn=<SumBackward0>)\n",
      "tensor(149762)\n",
      "Epoch: 30\tLoss: 749536.0625\n",
      "tensor(456.9695, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.0136, grad_fn=<StdBackward0>)\n",
      "tensor(251.4921, grad_fn=<VarBackward0>)\n",
      "tensor(0.2594, grad_fn=<SumBackward0>)\n",
      "tensor(148086)\n",
      "Epoch: 31\tLoss: 741123.0625\n",
      "tensor(425.2212, grad_fn=<MseLossBackward0>)\n",
      "tensor(20.4322, grad_fn=<StdBackward0>)\n",
      "tensor(242.8979, grad_fn=<VarBackward0>)\n",
      "tensor(0.2863, grad_fn=<SumBackward0>)\n",
      "tensor(151800)\n",
      "Epoch: 32\tLoss: 759658.1875\n",
      "tensor(396.4524, grad_fn=<MseLossBackward0>)\n",
      "tensor(19.8194, grad_fn=<StdBackward0>)\n",
      "tensor(233.9586, grad_fn=<VarBackward0>)\n",
      "tensor(0.0773, grad_fn=<SumBackward0>)\n",
      "tensor(151660)\n",
      "Epoch: 33\tLoss: 758902.3750\n",
      "tensor(368.7792, grad_fn=<MseLossBackward0>)\n",
      "tensor(19.2011, grad_fn=<StdBackward0>)\n",
      "tensor(225.1624, grad_fn=<VarBackward0>)\n",
      "tensor(0.3801, grad_fn=<SumBackward0>)\n",
      "tensor(159780)\n",
      "Epoch: 34\tLoss: 799498.8125\n",
      "tensor(339.8795, grad_fn=<MseLossBackward0>)\n",
      "tensor(18.5979, grad_fn=<StdBackward0>)\n",
      "tensor(216.8013, grad_fn=<VarBackward0>)\n",
      "tensor(0.2893, grad_fn=<SumBackward0>)\n",
      "tensor(159022)\n",
      "Epoch: 35\tLoss: 795664.8125\n",
      "tensor(311.3047, grad_fn=<MseLossBackward0>)\n",
      "tensor(18.0268, grad_fn=<StdBackward0>)\n",
      "tensor(209.0760, grad_fn=<VarBackward0>)\n",
      "tensor(0.1300, grad_fn=<SumBackward0>)\n",
      "tensor(161403)\n",
      "Epoch: 36\tLoss: 807519.5625\n",
      "tensor(285.8470, grad_fn=<MseLossBackward0>)\n",
      "tensor(17.4977, grad_fn=<StdBackward0>)\n",
      "tensor(202.0573, grad_fn=<VarBackward0>)\n",
      "tensor(0.1388, grad_fn=<SumBackward0>)\n",
      "tensor(140663)\n",
      "Epoch: 37\tLoss: 703789.6875\n",
      "tensor(262.4522, grad_fn=<MseLossBackward0>)\n",
      "tensor(17.0054, grad_fn=<StdBackward0>)\n",
      "tensor(195.5055, grad_fn=<VarBackward0>)\n",
      "tensor(0.0468, grad_fn=<SumBackward0>)\n",
      "tensor(131284)\n",
      "Epoch: 38\tLoss: 656857.1875\n",
      "tensor(240.8344, grad_fn=<MseLossBackward0>)\n",
      "tensor(16.5339, grad_fn=<StdBackward0>)\n",
      "tensor(189.0576, grad_fn=<VarBackward0>)\n",
      "tensor(0.2138, grad_fn=<SumBackward0>)\n",
      "tensor(131041)\n",
      "Epoch: 39\tLoss: 655632.5625\n",
      "tensor(220.3748, grad_fn=<MseLossBackward0>)\n",
      "tensor(16.0648, grad_fn=<StdBackward0>)\n",
      "tensor(182.3830, grad_fn=<VarBackward0>)\n",
      "tensor(0.0738, grad_fn=<SumBackward0>)\n",
      "tensor(131284)\n",
      "Epoch: 40\tLoss: 656808.3750\n",
      "tensor(201.3260, grad_fn=<MseLossBackward0>)\n",
      "tensor(15.5888, grad_fn=<StdBackward0>)\n",
      "tensor(175.4149, grad_fn=<VarBackward0>)\n",
      "tensor(0.3076, grad_fn=<SumBackward0>)\n",
      "tensor(127783)\n",
      "Epoch: 41\tLoss: 639303.0000\n",
      "tensor(183.8872, grad_fn=<MseLossBackward0>)\n",
      "tensor(15.1056, grad_fn=<StdBackward0>)\n",
      "tensor(168.3001, grad_fn=<VarBackward0>)\n",
      "tensor(0.3751, grad_fn=<SumBackward0>)\n",
      "tensor(131963)\n",
      "Epoch: 42\tLoss: 660187.4375\n",
      "tensor(167.8301, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.6160, grad_fn=<StdBackward0>)\n",
      "tensor(161.1819, grad_fn=<VarBackward0>)\n",
      "tensor(0.1689, grad_fn=<SumBackward0>)\n",
      "tensor(131420)\n",
      "Epoch: 43\tLoss: 657430.8750\n",
      "tensor(153.1918, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.1199, grad_fn=<StdBackward0>)\n",
      "tensor(154.1536, grad_fn=<VarBackward0>)\n",
      "tensor(0.2649, grad_fn=<SumBackward0>)\n",
      "tensor(137220)\n",
      "Epoch: 44\tLoss: 686420.8750\n",
      "tensor(139.6532, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.6168, grad_fn=<StdBackward0>)\n",
      "tensor(147.2605, grad_fn=<VarBackward0>)\n",
      "tensor(0.4021, grad_fn=<SumBackward0>)\n",
      "tensor(141575)\n",
      "Epoch: 45\tLoss: 708191.0625\n",
      "tensor(127.1069, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.1091, grad_fn=<StdBackward0>)\n",
      "tensor(140.5867, grad_fn=<VarBackward0>)\n",
      "tensor(0.2819, grad_fn=<SumBackward0>)\n",
      "tensor(134800)\n",
      "Epoch: 46\tLoss: 674286.3750\n",
      "tensor(115.5945, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6027, grad_fn=<StdBackward0>)\n",
      "tensor(134.2372, grad_fn=<VarBackward0>)\n",
      "tensor(0.0826, grad_fn=<SumBackward0>)\n",
      "tensor(117167)\n",
      "Epoch: 47\tLoss: 586084.8750\n",
      "tensor(105.1780, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1056, grad_fn=<StdBackward0>)\n",
      "tensor(128.3069, grad_fn=<VarBackward0>)\n",
      "tensor(0.2323, grad_fn=<SumBackward0>)\n",
      "tensor(121431)\n",
      "Epoch: 48\tLoss: 607404.4375\n",
      "tensor(95.7810, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6248, grad_fn=<StdBackward0>)\n",
      "tensor(122.7997, grad_fn=<VarBackward0>)\n",
      "tensor(0.1403, grad_fn=<SumBackward0>)\n",
      "tensor(123196)\n",
      "Epoch: 49\tLoss: 616206.0625\n",
      "tensor(87.4772, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1643, grad_fn=<StdBackward0>)\n",
      "tensor(117.6077, grad_fn=<VarBackward0>)\n",
      "tensor(0.1796, grad_fn=<SumBackward0>)\n",
      "tensor(125031)\n",
      "Epoch: 50\tLoss: 625372.0625\n",
      "tensor(80.0306, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.7228, grad_fn=<StdBackward0>)\n",
      "tensor(112.5307, grad_fn=<VarBackward0>)\n",
      "tensor(0.2282, grad_fn=<SumBackward0>)\n",
      "tensor(121858)\n",
      "Epoch: 51\tLoss: 609500.0625\n",
      "tensor(73.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.2961, grad_fn=<StdBackward0>)\n",
      "tensor(107.4128, grad_fn=<VarBackward0>)\n",
      "tensor(0.0362, grad_fn=<SumBackward0>)\n",
      "tensor(124080)\n",
      "Epoch: 52\tLoss: 620579.8750\n",
      "tensor(67.3819, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8793, grad_fn=<StdBackward0>)\n",
      "tensor(102.2041, grad_fn=<VarBackward0>)\n",
      "tensor(0.3693, grad_fn=<SumBackward0>)\n",
      "tensor(123990)\n",
      "Epoch: 53\tLoss: 620153.1250\n",
      "tensor(62.2480, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.4687, grad_fn=<StdBackward0>)\n",
      "tensor(96.9654, grad_fn=<VarBackward0>)\n",
      "tensor(0.4990, grad_fn=<SumBackward0>)\n",
      "tensor(128000)\n",
      "Epoch: 54\tLoss: 640206.8125\n",
      "tensor(57.7368, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0608, grad_fn=<StdBackward0>)\n",
      "tensor(91.7693, grad_fn=<VarBackward0>)\n",
      "tensor(0.3822, grad_fn=<SumBackward0>)\n",
      "tensor(127347)\n",
      "Epoch: 55\tLoss: 636921.5625\n",
      "tensor(53.8827, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6540, grad_fn=<StdBackward0>)\n",
      "tensor(86.6806, grad_fn=<VarBackward0>)\n",
      "tensor(0.0733, grad_fn=<SumBackward0>)\n",
      "tensor(118281)\n",
      "Epoch: 56\tLoss: 591552.7500\n",
      "tensor(50.7155, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2495, grad_fn=<StdBackward0>)\n",
      "tensor(81.7431, grad_fn=<VarBackward0>)\n",
      "tensor(0.3877, grad_fn=<SumBackward0>)\n",
      "tensor(121991)\n",
      "Epoch: 57\tLoss: 610127.0000\n",
      "tensor(47.8019, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8519, grad_fn=<StdBackward0>)\n",
      "tensor(76.9793, grad_fn=<VarBackward0>)\n",
      "tensor(0.5434, grad_fn=<SumBackward0>)\n",
      "tensor(116200)\n",
      "Epoch: 58\tLoss: 581180.6875\n",
      "tensor(44.9945, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4697, grad_fn=<StdBackward0>)\n",
      "tensor(72.4292, grad_fn=<VarBackward0>)\n",
      "tensor(0.4555, grad_fn=<SumBackward0>)\n",
      "tensor(117480)\n",
      "Epoch: 59\tLoss: 587565.2500\n",
      "tensor(42.3717, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1118, grad_fn=<StdBackward0>)\n",
      "tensor(68.1310, grad_fn=<VarBackward0>)\n",
      "tensor(0.1611, grad_fn=<SumBackward0>)\n",
      "tensor(117811)\n",
      "Epoch: 60\tLoss: 589184.6250\n",
      "tensor(40.1113, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7834, grad_fn=<StdBackward0>)\n",
      "tensor(64.1066, grad_fn=<VarBackward0>)\n",
      "tensor(0.3078, grad_fn=<SumBackward0>)\n",
      "tensor(118380)\n",
      "Epoch: 61\tLoss: 592038.7500\n",
      "tensor(38.1277, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4828, grad_fn=<StdBackward0>)\n",
      "tensor(60.3602, grad_fn=<VarBackward0>)\n",
      "tensor(0.5074, grad_fn=<SumBackward0>)\n",
      "tensor(110500)\n",
      "Epoch: 62\tLoss: 552653.6875\n",
      "tensor(36.1748, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2028, grad_fn=<StdBackward0>)\n",
      "tensor(56.8534, grad_fn=<VarBackward0>)\n",
      "tensor(0.4707, grad_fn=<SumBackward0>)\n",
      "tensor(108129)\n",
      "Epoch: 63\tLoss: 540790.2500\n",
      "tensor(34.3102, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9349, grad_fn=<StdBackward0>)\n",
      "tensor(53.5491, grad_fn=<VarBackward0>)\n",
      "tensor(0.2326, grad_fn=<SumBackward0>)\n",
      "tensor(107321)\n",
      "Epoch: 64\tLoss: 536721.9375\n",
      "tensor(32.7499, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6729, grad_fn=<StdBackward0>)\n",
      "tensor(50.4263, grad_fn=<VarBackward0>)\n",
      "tensor(0.1735, grad_fn=<SumBackward0>)\n",
      "tensor(122575)\n",
      "Epoch: 65\tLoss: 612981.8125\n",
      "tensor(31.4128, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4124, grad_fn=<StdBackward0>)\n",
      "tensor(47.4677, grad_fn=<VarBackward0>)\n",
      "tensor(0.3356, grad_fn=<SumBackward0>)\n",
      "tensor(118728)\n",
      "Epoch: 66\tLoss: 593759.1250\n",
      "tensor(30.1510, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1505, grad_fn=<StdBackward0>)\n",
      "tensor(44.6847, grad_fn=<VarBackward0>)\n",
      "tensor(0.2870, grad_fn=<SumBackward0>)\n",
      "tensor(110440)\n",
      "Epoch: 67\tLoss: 552310.3750\n",
      "tensor(28.9451, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8869, grad_fn=<StdBackward0>)\n",
      "tensor(42.0944, grad_fn=<VarBackward0>)\n",
      "tensor(0.0591, grad_fn=<SumBackward0>)\n",
      "tensor(110440)\n",
      "Epoch: 68\tLoss: 552283.7500\n",
      "tensor(27.9199, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6252, grad_fn=<StdBackward0>)\n",
      "tensor(39.7069, grad_fn=<VarBackward0>)\n",
      "tensor(0.3187, grad_fn=<SumBackward0>)\n",
      "tensor(97152)\n",
      "Epoch: 69\tLoss: 485866.0312\n",
      "tensor(27.0395, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3705, grad_fn=<StdBackward0>)\n",
      "tensor(37.5174, grad_fn=<VarBackward0>)\n",
      "tensor(0.4752, grad_fn=<SumBackward0>)\n",
      "tensor(90369)\n",
      "Epoch: 70\tLoss: 451963.2500\n",
      "tensor(26.1834, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1255, grad_fn=<StdBackward0>)\n",
      "tensor(35.4728, grad_fn=<VarBackward0>)\n",
      "tensor(0.4427, grad_fn=<SumBackward0>)\n",
      "tensor(91940)\n",
      "Epoch: 71\tLoss: 459811.7188\n",
      "tensor(25.3877, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8895, grad_fn=<StdBackward0>)\n",
      "tensor(33.5022, grad_fn=<VarBackward0>)\n",
      "tensor(0.2508, grad_fn=<SumBackward0>)\n",
      "tensor(79180)\n",
      "Epoch: 72\tLoss: 395989.3750\n",
      "tensor(24.7671, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6619, grad_fn=<StdBackward0>)\n",
      "tensor(31.5526, grad_fn=<VarBackward0>)\n",
      "tensor(0.0739, grad_fn=<SumBackward0>)\n",
      "tensor(80640)\n",
      "Epoch: 73\tLoss: 403268.7812\n",
      "tensor(24.2415, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4434, grad_fn=<StdBackward0>)\n",
      "tensor(29.6008, grad_fn=<VarBackward0>)\n",
      "tensor(0.2028, grad_fn=<SumBackward0>)\n",
      "tensor(80640)\n",
      "Epoch: 74\tLoss: 403278.9375\n",
      "tensor(23.7012, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2339, grad_fn=<StdBackward0>)\n",
      "tensor(27.6693, grad_fn=<VarBackward0>)\n",
      "tensor(0.1658, grad_fn=<SumBackward0>)\n",
      "tensor(69640)\n",
      "Epoch: 75\tLoss: 348272.6250\n",
      "tensor(23.1713, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0323, grad_fn=<StdBackward0>)\n",
      "tensor(25.8027, grad_fn=<VarBackward0>)\n",
      "tensor(0.0405, grad_fn=<SumBackward0>)\n",
      "tensor(68463)\n",
      "Epoch: 76\tLoss: 342372.5625\n",
      "tensor(22.7782, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8415, grad_fn=<StdBackward0>)\n",
      "tensor(24.0618, grad_fn=<VarBackward0>)\n",
      "tensor(0.0863, grad_fn=<SumBackward0>)\n",
      "tensor(77677)\n",
      "Epoch: 77\tLoss: 388444.8125\n",
      "tensor(22.4598, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6668, grad_fn=<StdBackward0>)\n",
      "tensor(22.4861, grad_fn=<VarBackward0>)\n",
      "tensor(0.0733, grad_fn=<SumBackward0>)\n",
      "tensor(72462)\n",
      "Epoch: 78\tLoss: 362366.4688\n",
      "tensor(22.2029, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5062, grad_fn=<StdBackward0>)\n",
      "tensor(21.0734, grad_fn=<VarBackward0>)\n",
      "tensor(0.0347, grad_fn=<SumBackward0>)\n",
      "tensor(70360)\n",
      "Epoch: 79\tLoss: 351850.7500\n",
      "tensor(21.9806, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3514, grad_fn=<StdBackward0>)\n",
      "tensor(19.7895, grad_fn=<VarBackward0>)\n",
      "tensor(0.0443, grad_fn=<SumBackward0>)\n",
      "tensor(69818)\n",
      "Epoch: 80\tLoss: 349139.9062\n",
      "tensor(21.7901, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2000, grad_fn=<StdBackward0>)\n",
      "tensor(18.6171, grad_fn=<VarBackward0>)\n",
      "tensor(0.0764, grad_fn=<SumBackward0>)\n",
      "tensor(68000)\n",
      "Epoch: 81\tLoss: 340051.4375\n",
      "tensor(21.7087, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0525, grad_fn=<StdBackward0>)\n",
      "tensor(17.5510, grad_fn=<VarBackward0>)\n",
      "tensor(0.0919, grad_fn=<SumBackward0>)\n",
      "tensor(58040)\n",
      "Epoch: 82\tLoss: 290251.4375\n",
      "tensor(21.7333, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9072, grad_fn=<StdBackward0>)\n",
      "tensor(16.5674, grad_fn=<VarBackward0>)\n",
      "tensor(0.0288, grad_fn=<SumBackward0>)\n",
      "tensor(58031)\n",
      "Epoch: 83\tLoss: 290198.6875\n",
      "tensor(21.8876, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7694, grad_fn=<StdBackward0>)\n",
      "tensor(15.6472, grad_fn=<VarBackward0>)\n",
      "tensor(0.1803, grad_fn=<SumBackward0>)\n",
      "tensor(59140)\n",
      "Epoch: 84\tLoss: 295757.5938\n",
      "tensor(21.9919, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6420, grad_fn=<StdBackward0>)\n",
      "tensor(14.7720, grad_fn=<VarBackward0>)\n",
      "tensor(0.2015, grad_fn=<SumBackward0>)\n",
      "tensor(59222)\n",
      "Epoch: 85\tLoss: 296168.5625\n",
      "tensor(22.0476, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5244, grad_fn=<StdBackward0>)\n",
      "tensor(13.9459, grad_fn=<VarBackward0>)\n",
      "tensor(0.0985, grad_fn=<SumBackward0>)\n",
      "tensor(57638)\n",
      "Epoch: 86\tLoss: 288237.1250\n",
      "tensor(22.1320, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4214, grad_fn=<StdBackward0>)\n",
      "tensor(13.1987, grad_fn=<VarBackward0>)\n",
      "tensor(0.0959, grad_fn=<SumBackward0>)\n",
      "tensor(54474)\n",
      "Epoch: 87\tLoss: 272415.9375\n",
      "tensor(22.2407, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3316, grad_fn=<StdBackward0>)\n",
      "tensor(12.5572, grad_fn=<VarBackward0>)\n",
      "tensor(0.1568, grad_fn=<SumBackward0>)\n",
      "tensor(47778)\n",
      "Epoch: 88\tLoss: 238941.2344\n",
      "tensor(22.3199, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2529, grad_fn=<StdBackward0>)\n",
      "tensor(12.0166, grad_fn=<VarBackward0>)\n",
      "tensor(0.1086, grad_fn=<SumBackward0>)\n",
      "tensor(42953)\n",
      "Epoch: 89\tLoss: 214810.7031\n",
      "tensor(22.3853, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1772, grad_fn=<StdBackward0>)\n",
      "tensor(11.5369, grad_fn=<VarBackward0>)\n",
      "tensor(0.0296, grad_fn=<SumBackward0>)\n",
      "tensor(40897)\n",
      "Epoch: 90\tLoss: 204522.1094\n",
      "tensor(22.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1045, grad_fn=<StdBackward0>)\n",
      "tensor(11.1090, grad_fn=<VarBackward0>)\n",
      "tensor(0.0926, grad_fn=<SumBackward0>)\n",
      "tensor(34726)\n",
      "Epoch: 91\tLoss: 173672.7969\n",
      "tensor(22.5688, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0380, grad_fn=<StdBackward0>)\n",
      "tensor(10.7673, grad_fn=<VarBackward0>)\n",
      "tensor(0.0557, grad_fn=<SumBackward0>)\n",
      "tensor(31820)\n",
      "Epoch: 92\tLoss: 159138.5156\n",
      "tensor(22.6095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9848, grad_fn=<StdBackward0>)\n",
      "tensor(10.5122, grad_fn=<VarBackward0>)\n",
      "tensor(0.0724, grad_fn=<SumBackward0>)\n",
      "tensor(33707)\n",
      "Epoch: 93\tLoss: 168574.7031\n",
      "tensor(22.5267, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9446, grad_fn=<StdBackward0>)\n",
      "tensor(10.2917, grad_fn=<VarBackward0>)\n",
      "tensor(0.1183, grad_fn=<SumBackward0>)\n",
      "tensor(37920)\n",
      "Epoch: 94\tLoss: 189643.8125\n",
      "tensor(22.2887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9170, grad_fn=<StdBackward0>)\n",
      "tensor(10.1026, grad_fn=<VarBackward0>)\n",
      "tensor(0.0763, grad_fn=<SumBackward0>)\n",
      "tensor(39340)\n",
      "Epoch: 95\tLoss: 196739.0938\n",
      "tensor(21.8916, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9019, grad_fn=<StdBackward0>)\n",
      "tensor(9.9742, grad_fn=<VarBackward0>)\n",
      "tensor(0.0230, grad_fn=<SumBackward0>)\n",
      "tensor(38120)\n",
      "Epoch: 96\tLoss: 190633.2031\n",
      "tensor(21.4256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8952, grad_fn=<StdBackward0>)\n",
      "tensor(9.8753, grad_fn=<VarBackward0>)\n",
      "tensor(0.0533, grad_fn=<SumBackward0>)\n",
      "tensor(39520)\n",
      "Epoch: 97\tLoss: 197635.7031\n",
      "tensor(20.9990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8904, grad_fn=<StdBackward0>)\n",
      "tensor(9.7778, grad_fn=<VarBackward0>)\n",
      "tensor(0.0279, grad_fn=<SumBackward0>)\n",
      "tensor(39978)\n",
      "Epoch: 98\tLoss: 199922.7031\n",
      "tensor(20.5730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8843, grad_fn=<StdBackward0>)\n",
      "tensor(9.6908, grad_fn=<VarBackward0>)\n",
      "tensor(0.0256, grad_fn=<SumBackward0>)\n",
      "tensor(42290)\n",
      "Epoch: 99\tLoss: 211481.9688\n",
      "tensor(20.1356, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8806, grad_fn=<StdBackward0>)\n",
      "tensor(9.6266, grad_fn=<VarBackward0>)\n",
      "tensor(0.0555, grad_fn=<SumBackward0>)\n",
      "tensor(39730)\n",
      "Epoch: 100\tLoss: 198684.5000\n",
      "tensor(19.7683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8779, grad_fn=<StdBackward0>)\n",
      "tensor(9.5751, grad_fn=<VarBackward0>)\n",
      "tensor(0.0235, grad_fn=<SumBackward0>)\n",
      "tensor(35037)\n",
      "Epoch: 101\tLoss: 175215.9062\n",
      "tensor(19.4545, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8765, grad_fn=<StdBackward0>)\n",
      "tensor(9.5279, grad_fn=<VarBackward0>)\n",
      "tensor(0.0437, grad_fn=<SumBackward0>)\n",
      "tensor(35435)\n",
      "Epoch: 102\tLoss: 177207.5938\n",
      "tensor(19.1635, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8733, grad_fn=<StdBackward0>)\n",
      "tensor(9.4925, grad_fn=<VarBackward0>)\n",
      "tensor(0.0401, grad_fn=<SumBackward0>)\n",
      "tensor(31887)\n",
      "Epoch: 103\tLoss: 159466.9062\n",
      "tensor(18.9184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8652, grad_fn=<StdBackward0>)\n",
      "tensor(9.4378, grad_fn=<VarBackward0>)\n",
      "tensor(0.0316, grad_fn=<SumBackward0>)\n",
      "tensor(27946)\n",
      "Epoch: 104\tLoss: 139760.7344\n",
      "tensor(18.7543, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8555, grad_fn=<StdBackward0>)\n",
      "tensor(9.3531, grad_fn=<VarBackward0>)\n",
      "tensor(0.0265, grad_fn=<SumBackward0>)\n",
      "tensor(25541)\n",
      "Epoch: 105\tLoss: 127734.9609\n",
      "tensor(18.5661, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8473, grad_fn=<StdBackward0>)\n",
      "tensor(9.2736, grad_fn=<VarBackward0>)\n",
      "tensor(0.0255, grad_fn=<SumBackward0>)\n",
      "tensor(22080)\n",
      "Epoch: 106\tLoss: 110429.5938\n",
      "tensor(18.4054, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8396, grad_fn=<StdBackward0>)\n",
      "tensor(9.1934, grad_fn=<VarBackward0>)\n",
      "tensor(0.0258, grad_fn=<SumBackward0>)\n",
      "tensor(22821)\n",
      "Epoch: 107\tLoss: 114134.3828\n",
      "tensor(18.3046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8291, grad_fn=<StdBackward0>)\n",
      "tensor(9.1125, grad_fn=<VarBackward0>)\n",
      "tensor(0.0231, grad_fn=<SumBackward0>)\n",
      "tensor(22840)\n",
      "Epoch: 108\tLoss: 114228.9062\n",
      "tensor(18.2474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8152, grad_fn=<StdBackward0>)\n",
      "tensor(9.0342, grad_fn=<VarBackward0>)\n",
      "tensor(0.0309, grad_fn=<SumBackward0>)\n",
      "tensor(23458)\n",
      "Epoch: 109\tLoss: 117319.4922\n",
      "tensor(18.1796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8001, grad_fn=<StdBackward0>)\n",
      "tensor(8.9694, grad_fn=<VarBackward0>)\n",
      "tensor(0.0219, grad_fn=<SumBackward0>)\n",
      "tensor(22453)\n",
      "Epoch: 110\tLoss: 112293.3750\n",
      "tensor(18.1312, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7845, grad_fn=<StdBackward0>)\n",
      "tensor(8.9002, grad_fn=<VarBackward0>)\n",
      "tensor(0.0230, grad_fn=<SumBackward0>)\n",
      "tensor(22700)\n",
      "Epoch: 111\tLoss: 113528.2734\n",
      "tensor(18.0977, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7702, grad_fn=<StdBackward0>)\n",
      "tensor(8.8520, grad_fn=<VarBackward0>)\n",
      "tensor(0.0324, grad_fn=<SumBackward0>)\n",
      "tensor(21360)\n",
      "Epoch: 112\tLoss: 106829.0391\n",
      "tensor(18.0257, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7584, grad_fn=<StdBackward0>)\n",
      "tensor(8.8036, grad_fn=<VarBackward0>)\n",
      "tensor(0.0265, grad_fn=<SumBackward0>)\n",
      "tensor(21360)\n",
      "Epoch: 113\tLoss: 106828.2656\n",
      "tensor(17.9467, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7481, grad_fn=<StdBackward0>)\n",
      "tensor(8.7629, grad_fn=<VarBackward0>)\n",
      "tensor(0.0417, grad_fn=<SumBackward0>)\n",
      "tensor(21360)\n",
      "Epoch: 114\tLoss: 106829.6016\n",
      "tensor(17.9318, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7416, grad_fn=<StdBackward0>)\n",
      "tensor(8.7103, grad_fn=<VarBackward0>)\n",
      "tensor(0.0225, grad_fn=<SumBackward0>)\n",
      "tensor(21360)\n",
      "Epoch: 115\tLoss: 106827.6016\n",
      "tensor(17.9410, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7544, grad_fn=<StdBackward0>)\n",
      "tensor(8.7141, grad_fn=<VarBackward0>)\n",
      "tensor(0.0426, grad_fn=<SumBackward0>)\n",
      "tensor(21740)\n",
      "Epoch: 116\tLoss: 108729.7422\n",
      "tensor(17.9320, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8112, grad_fn=<StdBackward0>)\n",
      "tensor(8.6795, grad_fn=<VarBackward0>)\n",
      "tensor(0.0486, grad_fn=<SumBackward0>)\n",
      "tensor(21740)\n",
      "Epoch: 117\tLoss: 108730.9062\n",
      "tensor(17.7957, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8686, grad_fn=<StdBackward0>)\n",
      "tensor(8.7576, grad_fn=<VarBackward0>)\n",
      "tensor(0.0316, grad_fn=<SumBackward0>)\n",
      "tensor(21949)\n",
      "Epoch: 118\tLoss: 109774.6484\n",
      "tensor(17.5648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7888, grad_fn=<StdBackward0>)\n",
      "tensor(8.6913, grad_fn=<VarBackward0>)\n",
      "tensor(0.0471, grad_fn=<SumBackward0>)\n",
      "tensor(22243)\n",
      "Epoch: 119\tLoss: 111245.1562\n",
      "tensor(17.3877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7229, grad_fn=<StdBackward0>)\n",
      "tensor(8.6457, grad_fn=<VarBackward0>)\n",
      "tensor(0.0419, grad_fn=<SumBackward0>)\n",
      "tensor(22360)\n",
      "Epoch: 120\tLoss: 111828.8047\n",
      "tensor(17.3597, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7410, grad_fn=<StdBackward0>)\n",
      "tensor(8.6392, grad_fn=<VarBackward0>)\n",
      "tensor(0.0343, grad_fn=<SumBackward0>)\n",
      "tensor(22940)\n",
      "Epoch: 121\tLoss: 114728.2031\n",
      "tensor(17.3590, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8010, grad_fn=<StdBackward0>)\n",
      "tensor(8.6521, grad_fn=<VarBackward0>)\n",
      "tensor(0.0473, grad_fn=<SumBackward0>)\n",
      "tensor(23120)\n",
      "Epoch: 122\tLoss: 115630.1016\n",
      "tensor(17.1923, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7804, grad_fn=<StdBackward0>)\n",
      "tensor(8.6734, grad_fn=<VarBackward0>)\n",
      "tensor(0.0335, grad_fn=<SumBackward0>)\n",
      "tensor(23120)\n",
      "Epoch: 123\tLoss: 115628.3438\n",
      "tensor(17.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7163, grad_fn=<StdBackward0>)\n",
      "tensor(8.6435, grad_fn=<VarBackward0>)\n",
      "tensor(0.0308, grad_fn=<SumBackward0>)\n",
      "tensor(22260)\n",
      "Epoch: 124\tLoss: 111327.2500\n",
      "tensor(16.9267, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7168, grad_fn=<StdBackward0>)\n",
      "tensor(8.6647, grad_fn=<VarBackward0>)\n",
      "tensor(0.0204, grad_fn=<SumBackward0>)\n",
      "tensor(20380)\n",
      "Epoch: 125\tLoss: 101926.1406\n",
      "tensor(16.9596, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7600, grad_fn=<StdBackward0>)\n",
      "tensor(8.6929, grad_fn=<VarBackward0>)\n",
      "tensor(0.0499, grad_fn=<SumBackward0>)\n",
      "tensor(14203)\n",
      "Epoch: 126\tLoss: 71044.5547\n",
      "tensor(16.9168, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7543, grad_fn=<StdBackward0>)\n",
      "tensor(8.6637, grad_fn=<VarBackward0>)\n",
      "tensor(0.0506, grad_fn=<SumBackward0>)\n",
      "tensor(12006)\n",
      "Epoch: 127\tLoss: 60059.5195\n",
      "tensor(16.7998, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7034, grad_fn=<StdBackward0>)\n",
      "tensor(8.6817, grad_fn=<VarBackward0>)\n",
      "tensor(0.0096, grad_fn=<SumBackward0>)\n",
      "tensor(14203)\n",
      "Epoch: 128\tLoss: 71039.7891\n",
      "tensor(16.7777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6873, grad_fn=<StdBackward0>)\n",
      "tensor(8.6815, grad_fn=<VarBackward0>)\n",
      "tensor(0.0580, grad_fn=<SumBackward0>)\n",
      "tensor(14203)\n",
      "Epoch: 129\tLoss: 71044.4453\n",
      "tensor(16.9431, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7103, grad_fn=<StdBackward0>)\n",
      "tensor(8.6541, grad_fn=<VarBackward0>)\n",
      "tensor(0.0345, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 130\tLoss: 50927.5000\n",
      "tensor(17.1942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7227, grad_fn=<StdBackward0>)\n",
      "tensor(8.6469, grad_fn=<VarBackward0>)\n",
      "tensor(0.0433, grad_fn=<SumBackward0>)\n",
      "tensor(10237)\n",
      "Epoch: 131\tLoss: 51213.7461\n",
      "tensor(17.2762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6926, grad_fn=<StdBackward0>)\n",
      "tensor(8.5730, grad_fn=<VarBackward0>)\n",
      "tensor(0.0458, grad_fn=<SumBackward0>)\n",
      "tensor(12726)\n",
      "Epoch: 132\tLoss: 63658.7773\n",
      "tensor(17.2671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6605, grad_fn=<StdBackward0>)\n",
      "tensor(8.5100, grad_fn=<VarBackward0>)\n",
      "tensor(0.0212, grad_fn=<SumBackward0>)\n",
      "tensor(10237)\n",
      "Epoch: 133\tLoss: 51210.9922\n",
      "tensor(17.3028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6568, grad_fn=<StdBackward0>)\n",
      "tensor(8.4595, grad_fn=<VarBackward0>)\n",
      "tensor(0.0353, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 134\tLoss: 50927.3984\n",
      "tensor(17.3881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6720, grad_fn=<StdBackward0>)\n",
      "tensor(8.4060, grad_fn=<VarBackward0>)\n",
      "tensor(0.0144, grad_fn=<SumBackward0>)\n",
      "tensor(12726)\n",
      "Epoch: 135\tLoss: 63655.5508\n",
      "tensor(17.3896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6781, grad_fn=<StdBackward0>)\n",
      "tensor(8.3780, grad_fn=<VarBackward0>)\n",
      "tensor(0.0149, grad_fn=<SumBackward0>)\n",
      "tensor(12726)\n",
      "Epoch: 136\tLoss: 63655.6602\n",
      "tensor(17.2842, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6593, grad_fn=<StdBackward0>)\n",
      "tensor(8.3240, grad_fn=<VarBackward0>)\n",
      "tensor(0.0261, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 137\tLoss: 50926.4844\n",
      "tensor(17.2354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6317, grad_fn=<StdBackward0>)\n",
      "tensor(8.2865, grad_fn=<VarBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 138\tLoss: 50924.6602\n",
      "tensor(17.2175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6160, grad_fn=<StdBackward0>)\n",
      "tensor(8.2532, grad_fn=<VarBackward0>)\n",
      "tensor(0.0554, grad_fn=<SumBackward0>)\n",
      "tensor(10237)\n",
      "Epoch: 139\tLoss: 51213.9180\n",
      "tensor(17.1238, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6109, grad_fn=<StdBackward0>)\n",
      "tensor(8.2227, grad_fn=<VarBackward0>)\n",
      "tensor(0.0446, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 140\tLoss: 50927.6953\n",
      "tensor(16.9882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6144, grad_fn=<StdBackward0>)\n",
      "tensor(8.1928, grad_fn=<VarBackward0>)\n",
      "tensor(0.0288, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 141\tLoss: 50926.0156\n",
      "tensor(16.9686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6279, grad_fn=<StdBackward0>)\n",
      "tensor(8.1707, grad_fn=<VarBackward0>)\n",
      "tensor(0.0273, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 142\tLoss: 50925.9727\n",
      "tensor(17.0299, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6396, grad_fn=<StdBackward0>)\n",
      "tensor(8.1438, grad_fn=<VarBackward0>)\n",
      "tensor(0.0297, grad_fn=<SumBackward0>)\n",
      "tensor(10180)\n",
      "Epoch: 143\tLoss: 50926.3906\n",
      "tensor(16.9889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6362, grad_fn=<StdBackward0>)\n",
      "tensor(8.1010, grad_fn=<VarBackward0>)\n",
      "tensor(0.0243, grad_fn=<SumBackward0>)\n",
      "tensor(10123)\n",
      "Epoch: 144\tLoss: 50640.7812\n",
      "tensor(16.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6182, grad_fn=<StdBackward0>)\n",
      "tensor(8.0689, grad_fn=<VarBackward0>)\n",
      "tensor(0.0309, grad_fn=<SumBackward0>)\n",
      "tensor(10066)\n",
      "Epoch: 145\tLoss: 50356.1289\n",
      "tensor(16.7790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5990, grad_fn=<StdBackward0>)\n",
      "tensor(8.0262, grad_fn=<VarBackward0>)\n",
      "tensor(0.0299, grad_fn=<SumBackward0>)\n",
      "tensor(9040)\n",
      "Epoch: 146\tLoss: 45225.7617\n",
      "tensor(16.7840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5850, grad_fn=<StdBackward0>)\n",
      "tensor(7.9905, grad_fn=<VarBackward0>)\n",
      "tensor(0.0150, grad_fn=<SumBackward0>)\n",
      "tensor(9040)\n",
      "Epoch: 147\tLoss: 45224.1367\n",
      "tensor(16.7827, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5760, grad_fn=<StdBackward0>)\n",
      "tensor(7.9562, grad_fn=<VarBackward0>)\n",
      "tensor(0.0257, grad_fn=<SumBackward0>)\n",
      "tensor(9040)\n",
      "Epoch: 148\tLoss: 45225.1094\n",
      "tensor(16.7243, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5721, grad_fn=<StdBackward0>)\n",
      "tensor(7.9206, grad_fn=<VarBackward0>)\n",
      "tensor(0.0101, grad_fn=<SumBackward0>)\n",
      "tensor(7900)\n",
      "Epoch: 149\tLoss: 39523.4531\n",
      "tensor(16.6746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5726, grad_fn=<StdBackward0>)\n",
      "tensor(7.8899, grad_fn=<VarBackward0>)\n",
      "tensor(0.0432, grad_fn=<SumBackward0>)\n",
      "tensor(7900)\n",
      "Epoch: 150\tLoss: 39526.7148\n",
      "tensor(16.6740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5779, grad_fn=<StdBackward0>)\n",
      "tensor(7.8510, grad_fn=<VarBackward0>)\n",
      "tensor(0.0287, grad_fn=<SumBackward0>)\n",
      "tensor(7900)\n",
      "Epoch: 151\tLoss: 39525.3281\n",
      "tensor(16.7456, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5930, grad_fn=<StdBackward0>)\n",
      "tensor(7.8250, grad_fn=<VarBackward0>)\n",
      "tensor(0.0248, grad_fn=<SumBackward0>)\n",
      "tensor(7900)\n",
      "Epoch: 152\tLoss: 39525.1562\n",
      "tensor(16.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6199, grad_fn=<StdBackward0>)\n",
      "tensor(7.8158, grad_fn=<VarBackward0>)\n",
      "tensor(0.0301, grad_fn=<SumBackward0>)\n",
      "tensor(7900)\n",
      "Epoch: 153\tLoss: 39525.9609\n",
      "tensor(16.6646, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6348, grad_fn=<StdBackward0>)\n",
      "tensor(7.8037, grad_fn=<VarBackward0>)\n",
      "tensor(0.0121, grad_fn=<SumBackward0>)\n",
      "tensor(7539)\n",
      "Epoch: 154\tLoss: 37719.2227\n",
      "tensor(16.5886, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6179, grad_fn=<StdBackward0>)\n",
      "tensor(7.8083, grad_fn=<VarBackward0>)\n",
      "tensor(0.0164, grad_fn=<SumBackward0>)\n",
      "tensor(7121)\n",
      "Epoch: 155\tLoss: 35629.4102\n",
      "tensor(16.6024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5884, grad_fn=<StdBackward0>)\n",
      "tensor(7.7790, grad_fn=<VarBackward0>)\n",
      "tensor(0.0196, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 156\tLoss: 31924.4473\n",
      "tensor(16.5501, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5716, grad_fn=<StdBackward0>)\n",
      "tensor(7.7812, grad_fn=<VarBackward0>)\n",
      "tensor(0.0105, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 157\tLoss: 31923.3203\n",
      "tensor(16.4640, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5663, grad_fn=<StdBackward0>)\n",
      "tensor(7.7780, grad_fn=<VarBackward0>)\n",
      "tensor(0.0335, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 158\tLoss: 31925.4746\n",
      "tensor(16.4820, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5679, grad_fn=<StdBackward0>)\n",
      "tensor(7.7795, grad_fn=<VarBackward0>)\n",
      "tensor(0.0258, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 159\tLoss: 31924.7422\n",
      "tensor(16.5636, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5785, grad_fn=<StdBackward0>)\n",
      "tensor(7.7797, grad_fn=<VarBackward0>)\n",
      "tensor(0.0204, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 160\tLoss: 31924.3887\n",
      "tensor(16.5803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5970, grad_fn=<StdBackward0>)\n",
      "tensor(7.7761, grad_fn=<VarBackward0>)\n",
      "tensor(0.0188, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 161\tLoss: 31924.4336\n",
      "tensor(16.4960, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6158, grad_fn=<StdBackward0>)\n",
      "tensor(7.7897, grad_fn=<VarBackward0>)\n",
      "tensor(0.0242, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 162\tLoss: 31925.0723\n",
      "tensor(16.4699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6071, grad_fn=<StdBackward0>)\n",
      "tensor(7.7739, grad_fn=<VarBackward0>)\n",
      "tensor(0.0230, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 163\tLoss: 31924.8438\n",
      "tensor(16.4525, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5850, grad_fn=<StdBackward0>)\n",
      "tensor(7.7666, grad_fn=<VarBackward0>)\n",
      "tensor(0.0152, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 164\tLoss: 31923.8203\n",
      "tensor(16.4363, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5630, grad_fn=<StdBackward0>)\n",
      "tensor(7.7432, grad_fn=<VarBackward0>)\n",
      "tensor(0.0123, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 165\tLoss: 31923.2988\n",
      "tensor(16.4080, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5509, grad_fn=<StdBackward0>)\n",
      "tensor(7.7292, grad_fn=<VarBackward0>)\n",
      "tensor(0.0155, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 166\tLoss: 31923.4648\n",
      "tensor(16.4401, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5432, grad_fn=<StdBackward0>)\n",
      "tensor(7.7170, grad_fn=<VarBackward0>)\n",
      "tensor(0.0104, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 167\tLoss: 31922.9082\n",
      "tensor(16.4734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5395, grad_fn=<StdBackward0>)\n",
      "tensor(7.6879, grad_fn=<VarBackward0>)\n",
      "tensor(0.0110, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 168\tLoss: 31922.9668\n",
      "tensor(16.4818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5446, grad_fn=<StdBackward0>)\n",
      "tensor(7.6725, grad_fn=<VarBackward0>)\n",
      "tensor(0.0103, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 169\tLoss: 31922.9570\n",
      "tensor(16.5024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5610, grad_fn=<StdBackward0>)\n",
      "tensor(7.6464, grad_fn=<VarBackward0>)\n",
      "tensor(0.0138, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 170\tLoss: 31923.4902\n",
      "tensor(16.5331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5928, grad_fn=<StdBackward0>)\n",
      "tensor(7.6510, grad_fn=<VarBackward0>)\n",
      "tensor(0.0131, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 171\tLoss: 31923.7676\n",
      "tensor(16.5398, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6080, grad_fn=<StdBackward0>)\n",
      "tensor(7.6247, grad_fn=<VarBackward0>)\n",
      "tensor(0.0282, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 172\tLoss: 31925.4434\n",
      "tensor(16.4449, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5862, grad_fn=<StdBackward0>)\n",
      "tensor(7.6299, grad_fn=<VarBackward0>)\n",
      "tensor(0.0115, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 173\tLoss: 31923.4551\n",
      "tensor(16.3871, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5507, grad_fn=<StdBackward0>)\n",
      "tensor(7.5977, grad_fn=<VarBackward0>)\n",
      "tensor(0.0180, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 174\tLoss: 31923.6934\n",
      "tensor(16.3808, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5298, grad_fn=<StdBackward0>)\n",
      "tensor(7.5790, grad_fn=<VarBackward0>)\n",
      "tensor(0.0084, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 175\tLoss: 31922.5215\n",
      "tensor(16.4032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5194, grad_fn=<StdBackward0>)\n",
      "tensor(7.5749, grad_fn=<VarBackward0>)\n",
      "tensor(0.0114, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 176\tLoss: 31922.7363\n",
      "tensor(16.4172, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5159, grad_fn=<StdBackward0>)\n",
      "tensor(7.5499, grad_fn=<VarBackward0>)\n",
      "tensor(0.0170, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 177\tLoss: 31923.2754\n",
      "tensor(16.4248, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5163, grad_fn=<StdBackward0>)\n",
      "tensor(7.5680, grad_fn=<VarBackward0>)\n",
      "tensor(0.0107, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 178\tLoss: 31922.6582\n",
      "tensor(16.4479, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5280, grad_fn=<StdBackward0>)\n",
      "tensor(7.5153, grad_fn=<VarBackward0>)\n",
      "tensor(0.0061, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 179\tLoss: 31922.3398\n",
      "tensor(16.4969, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5554, grad_fn=<StdBackward0>)\n",
      "tensor(7.5728, grad_fn=<VarBackward0>)\n",
      "tensor(0.0203, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 180\tLoss: 31924.0820\n",
      "tensor(16.4608, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5883, grad_fn=<StdBackward0>)\n",
      "tensor(7.5103, grad_fn=<VarBackward0>)\n",
      "tensor(0.0101, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 181\tLoss: 31923.3496\n",
      "tensor(16.3894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5760, grad_fn=<StdBackward0>)\n",
      "tensor(7.5858, grad_fn=<VarBackward0>)\n",
      "tensor(0.0319, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 182\tLoss: 31925.3359\n",
      "tensor(16.3166, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5470, grad_fn=<StdBackward0>)\n",
      "tensor(7.5444, grad_fn=<VarBackward0>)\n",
      "tensor(0.0356, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 183\tLoss: 31925.3516\n",
      "tensor(16.2996, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5374, grad_fn=<StdBackward0>)\n",
      "tensor(7.5409, grad_fn=<VarBackward0>)\n",
      "tensor(0.0084, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 184\tLoss: 30022.5137\n",
      "tensor(16.3653, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5475, grad_fn=<StdBackward0>)\n",
      "tensor(7.6040, grad_fn=<VarBackward0>)\n",
      "tensor(0.0478, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 185\tLoss: 30026.6250\n",
      "tensor(16.3353, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5655, grad_fn=<StdBackward0>)\n",
      "tensor(7.5500, grad_fn=<VarBackward0>)\n",
      "tensor(0.0636, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 186\tLoss: 30028.3516\n",
      "tensor(16.3021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5483, grad_fn=<StdBackward0>)\n",
      "tensor(7.6531, grad_fn=<VarBackward0>)\n",
      "tensor(0.0427, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 187\tLoss: 30026.0508\n",
      "tensor(16.2417, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5242, grad_fn=<StdBackward0>)\n",
      "tensor(7.5882, grad_fn=<VarBackward0>)\n",
      "tensor(0.0050, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 188\tLoss: 30021.9824\n",
      "tensor(16.2762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5096, grad_fn=<StdBackward0>)\n",
      "tensor(7.5978, grad_fn=<VarBackward0>)\n",
      "tensor(0.0283, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 189\tLoss: 30024.1992\n",
      "tensor(16.3526, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5095, grad_fn=<StdBackward0>)\n",
      "tensor(7.6019, grad_fn=<VarBackward0>)\n",
      "tensor(0.0196, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 190\tLoss: 30023.4102\n",
      "tensor(16.4385, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5197, grad_fn=<StdBackward0>)\n",
      "tensor(7.5485, grad_fn=<VarBackward0>)\n",
      "tensor(0.0191, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 191\tLoss: 30023.5410\n",
      "tensor(16.4995, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5264, grad_fn=<StdBackward0>)\n",
      "tensor(7.6015, grad_fn=<VarBackward0>)\n",
      "tensor(0.0233, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 192\tLoss: 30024.0957\n",
      "tensor(16.5236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5182, grad_fn=<StdBackward0>)\n",
      "tensor(7.5014, grad_fn=<VarBackward0>)\n",
      "tensor(0.0065, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 193\tLoss: 30022.3535\n",
      "tensor(16.5331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5036, grad_fn=<StdBackward0>)\n",
      "tensor(7.5388, grad_fn=<VarBackward0>)\n",
      "tensor(0.0314, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 194\tLoss: 30024.7090\n",
      "tensor(16.5512, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4969, grad_fn=<StdBackward0>)\n",
      "tensor(7.4739, grad_fn=<VarBackward0>)\n",
      "tensor(0.0238, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 195\tLoss: 30023.8945\n",
      "tensor(16.5922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5169, grad_fn=<StdBackward0>)\n",
      "tensor(7.4807, grad_fn=<VarBackward0>)\n",
      "tensor(0.0095, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 196\tLoss: 30022.7129\n",
      "tensor(16.6218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5673, grad_fn=<StdBackward0>)\n",
      "tensor(7.4910, grad_fn=<VarBackward0>)\n",
      "tensor(0.0230, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 197\tLoss: 30024.5977\n",
      "tensor(16.5391, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5898, grad_fn=<StdBackward0>)\n",
      "tensor(7.4724, grad_fn=<VarBackward0>)\n",
      "tensor(0.0085, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 198\tLoss: 30023.2871\n",
      "tensor(16.4097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5450, grad_fn=<StdBackward0>)\n",
      "tensor(7.4955, grad_fn=<VarBackward0>)\n",
      "tensor(0.0305, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 199\tLoss: 30024.9102\n",
      "tensor(16.3281, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5131, grad_fn=<StdBackward0>)\n",
      "tensor(7.4599, grad_fn=<VarBackward0>)\n",
      "tensor(0.0330, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 200\tLoss: 30024.7578\n",
      "tensor(16.3025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5085, grad_fn=<StdBackward0>)\n",
      "tensor(7.4659, grad_fn=<VarBackward0>)\n",
      "tensor(0.0107, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 201\tLoss: 30022.4531\n",
      "tensor(16.3616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5067, grad_fn=<StdBackward0>)\n",
      "tensor(7.4585, grad_fn=<VarBackward0>)\n",
      "tensor(0.0238, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 202\tLoss: 30023.8066\n",
      "tensor(16.3526, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5073, grad_fn=<StdBackward0>)\n",
      "tensor(7.4498, grad_fn=<VarBackward0>)\n",
      "tensor(0.0254, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 203\tLoss: 30023.9668\n",
      "tensor(16.4186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5014, grad_fn=<StdBackward0>)\n",
      "tensor(7.4516, grad_fn=<VarBackward0>)\n",
      "tensor(0.0201, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 204\tLoss: 30023.4414\n",
      "tensor(16.4080, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5137, grad_fn=<StdBackward0>)\n",
      "tensor(7.4421, grad_fn=<VarBackward0>)\n",
      "tensor(0.0176, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 205\tLoss: 30023.3008\n",
      "tensor(16.3967, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5447, grad_fn=<StdBackward0>)\n",
      "tensor(7.4540, grad_fn=<VarBackward0>)\n",
      "tensor(0.0261, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 206\tLoss: 30024.4492\n",
      "tensor(16.3879, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5634, grad_fn=<StdBackward0>)\n",
      "tensor(7.4592, grad_fn=<VarBackward0>)\n",
      "tensor(0.0248, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 207\tLoss: 30024.5000\n",
      "tensor(16.2983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5374, grad_fn=<StdBackward0>)\n",
      "tensor(7.4743, grad_fn=<VarBackward0>)\n",
      "tensor(0.0096, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 208\tLoss: 30022.6328\n",
      "tensor(16.2581, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4995, grad_fn=<StdBackward0>)\n",
      "tensor(7.4616, grad_fn=<VarBackward0>)\n",
      "tensor(0.0070, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 209\tLoss: 30021.9551\n",
      "tensor(16.2553, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4812, grad_fn=<StdBackward0>)\n",
      "tensor(7.4692, grad_fn=<VarBackward0>)\n",
      "tensor(0.0059, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 210\tLoss: 30021.6621\n",
      "tensor(16.2674, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4779, grad_fn=<StdBackward0>)\n",
      "tensor(7.4565, grad_fn=<VarBackward0>)\n",
      "tensor(0.0107, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 211\tLoss: 30022.1191\n",
      "tensor(16.3163, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4842, grad_fn=<StdBackward0>)\n",
      "tensor(7.4645, grad_fn=<VarBackward0>)\n",
      "tensor(0.0057, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 212\tLoss: 30021.7266\n",
      "tensor(16.3593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5034, grad_fn=<StdBackward0>)\n",
      "tensor(7.4532, grad_fn=<VarBackward0>)\n",
      "tensor(0.0148, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 213\tLoss: 30022.8691\n",
      "tensor(16.4177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5236, grad_fn=<StdBackward0>)\n",
      "tensor(7.4444, grad_fn=<VarBackward0>)\n",
      "tensor(0.0085, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 214\tLoss: 31922.5020\n",
      "tensor(16.4177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5380, grad_fn=<StdBackward0>)\n",
      "tensor(7.4481, grad_fn=<VarBackward0>)\n",
      "tensor(0.0156, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 215\tLoss: 31923.3594\n",
      "tensor(16.3854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5293, grad_fn=<StdBackward0>)\n",
      "tensor(7.4294, grad_fn=<VarBackward0>)\n",
      "tensor(0.0278, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 216\tLoss: 30024.4551\n",
      "tensor(16.3666, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5118, grad_fn=<StdBackward0>)\n",
      "tensor(7.4484, grad_fn=<VarBackward0>)\n",
      "tensor(0.0109, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 217\tLoss: 30022.5781\n",
      "tensor(16.3115, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4992, grad_fn=<StdBackward0>)\n",
      "tensor(7.4137, grad_fn=<VarBackward0>)\n",
      "tensor(0.0254, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 218\tLoss: 30023.8418\n",
      "tensor(16.3208, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4831, grad_fn=<StdBackward0>)\n",
      "tensor(7.4241, grad_fn=<VarBackward0>)\n",
      "tensor(0.0254, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 219\tLoss: 30023.6914\n",
      "tensor(16.3578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4708, grad_fn=<StdBackward0>)\n",
      "tensor(7.3950, grad_fn=<VarBackward0>)\n",
      "tensor(0.0060, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 220\tLoss: 30021.6699\n",
      "tensor(16.4217, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4587, grad_fn=<StdBackward0>)\n",
      "tensor(7.3870, grad_fn=<VarBackward0>)\n",
      "tensor(0.0176, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 221\tLoss: 16022.7656\n",
      "tensor(16.4646, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4496, grad_fn=<StdBackward0>)\n",
      "tensor(7.3716, grad_fn=<VarBackward0>)\n",
      "tensor(0.0161, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 222\tLoss: 16022.5664\n",
      "tensor(16.5098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4416, grad_fn=<StdBackward0>)\n",
      "tensor(7.3415, grad_fn=<VarBackward0>)\n",
      "tensor(0.0119, grad_fn=<SumBackward0>)\n",
      "tensor(3900)\n",
      "Epoch: 223\tLoss: 19522.1113\n",
      "tensor(16.5704, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4435, grad_fn=<StdBackward0>)\n",
      "tensor(7.3301, grad_fn=<VarBackward0>)\n",
      "tensor(0.0227, grad_fn=<SumBackward0>)\n",
      "tensor(5300)\n",
      "Epoch: 224\tLoss: 26523.2715\n",
      "tensor(16.6627, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4754, grad_fn=<StdBackward0>)\n",
      "tensor(7.3131, grad_fn=<VarBackward0>)\n",
      "tensor(0.0071, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 225\tLoss: 16022.1260\n",
      "tensor(16.7334, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5510, grad_fn=<StdBackward0>)\n",
      "tensor(7.3402, grad_fn=<VarBackward0>)\n",
      "tensor(0.0198, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 226\tLoss: 16024.2197\n",
      "tensor(16.6475, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5530, grad_fn=<StdBackward0>)\n",
      "tensor(7.3356, grad_fn=<VarBackward0>)\n",
      "tensor(0.0072, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 227\tLoss: 16022.8926\n",
      "tensor(16.4862, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4844, grad_fn=<StdBackward0>)\n",
      "tensor(7.3135, grad_fn=<VarBackward0>)\n",
      "tensor(0.0354, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 228\tLoss: 30024.8711\n",
      "tensor(16.4242, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4568, grad_fn=<StdBackward0>)\n",
      "tensor(7.3291, grad_fn=<VarBackward0>)\n",
      "tensor(0.0426, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 229\tLoss: 30025.2500\n",
      "tensor(16.4499, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4647, grad_fn=<StdBackward0>)\n",
      "tensor(7.2823, grad_fn=<VarBackward0>)\n",
      "tensor(0.0252, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 230\tLoss: 30023.6133\n",
      "tensor(16.5026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4949, grad_fn=<StdBackward0>)\n",
      "tensor(7.3691, grad_fn=<VarBackward0>)\n",
      "tensor(0.0194, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 231\tLoss: 16023.3936\n",
      "tensor(16.4760, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5088, grad_fn=<StdBackward0>)\n",
      "tensor(7.3036, grad_fn=<VarBackward0>)\n",
      "tensor(0.0269, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 232\tLoss: 16024.2520\n",
      "tensor(16.3846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4837, grad_fn=<StdBackward0>)\n",
      "tensor(7.3882, grad_fn=<VarBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 233\tLoss: 16022.3320\n",
      "tensor(16.3321, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4595, grad_fn=<StdBackward0>)\n",
      "tensor(7.3496, grad_fn=<VarBackward0>)\n",
      "tensor(0.0208, grad_fn=<SumBackward0>)\n",
      "tensor(5300)\n",
      "Epoch: 234\tLoss: 26523.0098\n",
      "tensor(16.3631, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4575, grad_fn=<StdBackward0>)\n",
      "tensor(7.3500, grad_fn=<VarBackward0>)\n",
      "tensor(0.0253, grad_fn=<SumBackward0>)\n",
      "tensor(5300)\n",
      "Epoch: 235\tLoss: 26523.4707\n",
      "tensor(16.4636, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4830, grad_fn=<StdBackward0>)\n",
      "tensor(7.3886, grad_fn=<VarBackward0>)\n",
      "tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "tensor(5265)\n",
      "Epoch: 236\tLoss: 26347.0898\n",
      "tensor(16.5429, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5251, grad_fn=<StdBackward0>)\n",
      "tensor(7.3247, grad_fn=<VarBackward0>)\n",
      "tensor(0.0296, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 237\tLoss: 16024.7529\n",
      "tensor(16.5305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5140, grad_fn=<StdBackward0>)\n",
      "tensor(7.3924, grad_fn=<VarBackward0>)\n",
      "tensor(0.0395, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 238\tLoss: 16025.6211\n",
      "tensor(16.4378, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4808, grad_fn=<StdBackward0>)\n",
      "tensor(7.3254, grad_fn=<VarBackward0>)\n",
      "tensor(0.0241, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 239\tLoss: 16023.6572\n",
      "tensor(16.4173, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4685, grad_fn=<StdBackward0>)\n",
      "tensor(7.3325, grad_fn=<VarBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "tensor(5335)\n",
      "Epoch: 240\tLoss: 26697.2090\n",
      "tensor(16.3910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4822, grad_fn=<StdBackward0>)\n",
      "tensor(7.3526, grad_fn=<VarBackward0>)\n",
      "tensor(0.0210, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 241\tLoss: 30023.3145\n",
      "tensor(16.4474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4836, grad_fn=<StdBackward0>)\n",
      "tensor(7.3084, grad_fn=<VarBackward0>)\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "tensor(6000)\n",
      "Epoch: 242\tLoss: 30022.2012\n",
      "tensor(16.4237, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4745, grad_fn=<StdBackward0>)\n",
      "tensor(7.3552, grad_fn=<VarBackward0>)\n",
      "tensor(0.0225, grad_fn=<SumBackward0>)\n",
      "tensor(5645)\n",
      "Epoch: 243\tLoss: 28248.4160\n",
      "tensor(16.4495, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4523, grad_fn=<StdBackward0>)\n",
      "tensor(7.3097, grad_fn=<VarBackward0>)\n",
      "tensor(0.0286, grad_fn=<SumBackward0>)\n",
      "tensor(4280)\n",
      "Epoch: 244\tLoss: 21423.8320\n",
      "tensor(16.4346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4436, grad_fn=<StdBackward0>)\n",
      "tensor(7.3357, grad_fn=<VarBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "tensor(5750)\n",
      "Epoch: 245\tLoss: 28771.9844\n",
      "tensor(16.4833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4398, grad_fn=<StdBackward0>)\n",
      "tensor(7.3229, grad_fn=<VarBackward0>)\n",
      "tensor(0.0210, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 246\tLoss: 31922.9785\n",
      "tensor(16.5519, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4420, grad_fn=<StdBackward0>)\n",
      "tensor(7.2910, grad_fn=<VarBackward0>)\n",
      "tensor(0.0210, grad_fn=<SumBackward0>)\n",
      "tensor(6380)\n",
      "Epoch: 247\tLoss: 31923.0723\n",
      "tensor(16.6135, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4524, grad_fn=<StdBackward0>)\n",
      "tensor(7.3096, grad_fn=<VarBackward0>)\n",
      "tensor(0.0055, grad_fn=<SumBackward0>)\n",
      "tensor(5680)\n",
      "Epoch: 248\tLoss: 28421.6836\n",
      "tensor(16.6920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4596, grad_fn=<StdBackward0>)\n",
      "tensor(7.2411, grad_fn=<VarBackward0>)\n",
      "tensor(0.0122, grad_fn=<SumBackward0>)\n",
      "tensor(4280)\n",
      "Epoch: 249\tLoss: 21422.5117\n",
      "tensor(16.6836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4641, grad_fn=<StdBackward0>)\n",
      "tensor(7.3008, grad_fn=<VarBackward0>)\n",
      "tensor(0.0145, grad_fn=<SumBackward0>)\n",
      "tensor(3900)\n",
      "Epoch: 250\tLoss: 19522.7715\n",
      "tensor(16.6740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4660, grad_fn=<StdBackward0>)\n",
      "tensor(7.2454, grad_fn=<VarBackward0>)\n",
      "tensor(0.0145, grad_fn=<SumBackward0>)\n",
      "tensor(3900)\n",
      "Epoch: 251\tLoss: 19522.7871\n",
      "tensor(16.6924, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4843, grad_fn=<StdBackward0>)\n",
      "tensor(7.2898, grad_fn=<VarBackward0>)\n",
      "tensor(0.0176, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 252\tLoss: 16023.2998\n",
      "tensor(16.5968, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5260, grad_fn=<StdBackward0>)\n",
      "tensor(7.3002, grad_fn=<VarBackward0>)\n",
      "tensor(0.0094, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 253\tLoss: 16022.7930\n",
      "tensor(16.5750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5015, grad_fn=<StdBackward0>)\n",
      "tensor(7.3099, grad_fn=<VarBackward0>)\n",
      "tensor(0.0161, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 254\tLoss: 16023.2012\n",
      "tensor(16.4210, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4678, grad_fn=<StdBackward0>)\n",
      "tensor(7.3427, grad_fn=<VarBackward0>)\n",
      "tensor(0.0133, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 255\tLoss: 12522.4238\n",
      "tensor(16.4212, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4380, grad_fn=<StdBackward0>)\n",
      "tensor(7.3472, grad_fn=<VarBackward0>)\n",
      "tensor(0.0173, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 256\tLoss: 12522.5283\n",
      "tensor(16.4024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4292, grad_fn=<StdBackward0>)\n",
      "tensor(7.3566, grad_fn=<VarBackward0>)\n",
      "tensor(0.0117, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 257\tLoss: 12521.8662\n",
      "tensor(16.4353, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4282, grad_fn=<StdBackward0>)\n",
      "tensor(7.3699, grad_fn=<VarBackward0>)\n",
      "tensor(0.0161, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 258\tLoss: 12522.3252\n",
      "tensor(16.4779, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4407, grad_fn=<StdBackward0>)\n",
      "tensor(7.3390, grad_fn=<VarBackward0>)\n",
      "tensor(0.0115, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 259\tLoss: 12522.0391\n",
      "tensor(16.5011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4765, grad_fn=<StdBackward0>)\n",
      "tensor(7.3772, grad_fn=<VarBackward0>)\n",
      "tensor(0.0172, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 260\tLoss: 12522.9863\n",
      "tensor(16.5186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5047, grad_fn=<StdBackward0>)\n",
      "tensor(7.3478, grad_fn=<VarBackward0>)\n",
      "tensor(0.0121, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 261\tLoss: 12522.7715\n",
      "tensor(16.4232, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4910, grad_fn=<StdBackward0>)\n",
      "tensor(7.3703, grad_fn=<VarBackward0>)\n",
      "tensor(0.0123, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 262\tLoss: 16022.5664\n",
      "tensor(16.3779, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4637, grad_fn=<StdBackward0>)\n",
      "tensor(7.3819, grad_fn=<VarBackward0>)\n",
      "tensor(0.0128, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 263\tLoss: 16022.2939\n",
      "tensor(16.3485, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4636, grad_fn=<StdBackward0>)\n",
      "tensor(7.3365, grad_fn=<VarBackward0>)\n",
      "tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 264\tLoss: 12521.7852\n",
      "tensor(16.3755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4694, grad_fn=<StdBackward0>)\n",
      "tensor(7.4089, grad_fn=<VarBackward0>)\n",
      "tensor(0.0078, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 265\tLoss: 12521.8496\n",
      "tensor(16.3027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4750, grad_fn=<StdBackward0>)\n",
      "tensor(7.3456, grad_fn=<VarBackward0>)\n",
      "tensor(0.0064, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 266\tLoss: 16021.6885\n",
      "tensor(16.3077, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4552, grad_fn=<StdBackward0>)\n",
      "tensor(7.3996, grad_fn=<VarBackward0>)\n",
      "tensor(0.0105, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 267\tLoss: 16021.9072\n",
      "tensor(16.2537, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4470, grad_fn=<StdBackward0>)\n",
      "tensor(7.3607, grad_fn=<VarBackward0>)\n",
      "tensor(0.0060, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 268\tLoss: 16021.3213\n",
      "tensor(16.3213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4347, grad_fn=<StdBackward0>)\n",
      "tensor(7.3374, grad_fn=<VarBackward0>)\n",
      "tensor(0.0160, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 269\tLoss: 12522.2656\n",
      "tensor(16.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4330, grad_fn=<StdBackward0>)\n",
      "tensor(7.3272, grad_fn=<VarBackward0>)\n",
      "tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 270\tLoss: 16021.7969\n",
      "tensor(16.4067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4304, grad_fn=<StdBackward0>)\n",
      "tensor(7.2743, grad_fn=<VarBackward0>)\n",
      "tensor(0.0110, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 271\tLoss: 16021.8145\n",
      "tensor(16.4472, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4332, grad_fn=<StdBackward0>)\n",
      "tensor(7.2741, grad_fn=<VarBackward0>)\n",
      "tensor(0.0134, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 272\tLoss: 16022.1172\n",
      "tensor(16.4899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4384, grad_fn=<StdBackward0>)\n",
      "tensor(7.2225, grad_fn=<VarBackward0>)\n",
      "tensor(0.0059, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 273\tLoss: 16021.4639\n",
      "tensor(16.4981, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4513, grad_fn=<StdBackward0>)\n",
      "tensor(7.2269, grad_fn=<VarBackward0>)\n",
      "tensor(0.0130, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 274\tLoss: 12522.3076\n",
      "tensor(16.4693, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4759, grad_fn=<StdBackward0>)\n",
      "tensor(7.2297, grad_fn=<VarBackward0>)\n",
      "tensor(0.0042, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 275\tLoss: 12521.6465\n",
      "tensor(16.3903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4968, grad_fn=<StdBackward0>)\n",
      "tensor(7.2210, grad_fn=<VarBackward0>)\n",
      "tensor(0.0226, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 276\tLoss: 16023.6221\n",
      "tensor(16.3094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4829, grad_fn=<StdBackward0>)\n",
      "tensor(7.2675, grad_fn=<VarBackward0>)\n",
      "tensor(0.0284, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 277\tLoss: 16023.9736\n",
      "tensor(16.2219, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4547, grad_fn=<StdBackward0>)\n",
      "tensor(7.2301, grad_fn=<VarBackward0>)\n",
      "tensor(0.0157, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 278\tLoss: 16022.3350\n",
      "tensor(16.2228, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4307, grad_fn=<StdBackward0>)\n",
      "tensor(7.2678, grad_fn=<VarBackward0>)\n",
      "tensor(0.0125, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 279\tLoss: 12521.7783\n",
      "tensor(16.2258, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4182, grad_fn=<StdBackward0>)\n",
      "tensor(7.2457, grad_fn=<VarBackward0>)\n",
      "tensor(0.0200, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 280\tLoss: 12522.4043\n",
      "tensor(16.2710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4084, grad_fn=<StdBackward0>)\n",
      "tensor(7.2515, grad_fn=<VarBackward0>)\n",
      "tensor(0.0097, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 281\tLoss: 12521.3252\n",
      "tensor(16.2925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4094, grad_fn=<StdBackward0>)\n",
      "tensor(7.2377, grad_fn=<VarBackward0>)\n",
      "tensor(0.0162, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 282\tLoss: 12522.0029\n",
      "tensor(16.3671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4281, grad_fn=<StdBackward0>)\n",
      "tensor(7.2344, grad_fn=<VarBackward0>)\n",
      "tensor(0.0222, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 283\tLoss: 12522.8682\n",
      "tensor(16.3927, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4837, grad_fn=<StdBackward0>)\n",
      "tensor(7.2501, grad_fn=<VarBackward0>)\n",
      "tensor(0.0084, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 284\tLoss: 12522.0752\n",
      "tensor(16.4213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4965, grad_fn=<StdBackward0>)\n",
      "tensor(7.2534, grad_fn=<VarBackward0>)\n",
      "tensor(0.0264, grad_fn=<SumBackward0>)\n",
      "tensor(2000)\n",
      "Epoch: 285\tLoss: 10024.0303\n",
      "tensor(16.3104, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4560, grad_fn=<StdBackward0>)\n",
      "tensor(7.2713, grad_fn=<VarBackward0>)\n",
      "tensor(0.0281, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 286\tLoss: 12523.6797\n",
      "tensor(16.2592, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4299, grad_fn=<StdBackward0>)\n",
      "tensor(7.2549, grad_fn=<VarBackward0>)\n",
      "tensor(0.0062, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 287\tLoss: 12521.1816\n",
      "tensor(16.2934, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4336, grad_fn=<StdBackward0>)\n",
      "tensor(7.2904, grad_fn=<VarBackward0>)\n",
      "tensor(0.0308, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 288\tLoss: 16023.7129\n",
      "tensor(16.2486, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4709, grad_fn=<StdBackward0>)\n",
      "tensor(7.2678, grad_fn=<VarBackward0>)\n",
      "tensor(0.0263, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 289\tLoss: 16023.5869\n",
      "tensor(16.2874, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4642, grad_fn=<StdBackward0>)\n",
      "tensor(7.3120, grad_fn=<VarBackward0>)\n",
      "tensor(0.0100, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 290\tLoss: 16021.9307\n",
      "tensor(16.2066, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4439, grad_fn=<StdBackward0>)\n",
      "tensor(7.2710, grad_fn=<VarBackward0>)\n",
      "tensor(0.0169, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 291\tLoss: 12522.3350\n",
      "tensor(16.2568, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4186, grad_fn=<StdBackward0>)\n",
      "tensor(7.2661, grad_fn=<VarBackward0>)\n",
      "tensor(0.0207, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 292\tLoss: 12522.5166\n",
      "tensor(16.3727, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4065, grad_fn=<StdBackward0>)\n",
      "tensor(7.2388, grad_fn=<VarBackward0>)\n",
      "tensor(0.0132, grad_fn=<SumBackward0>)\n",
      "tensor(3200)\n",
      "Epoch: 293\tLoss: 16021.7578\n",
      "tensor(16.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4006, grad_fn=<StdBackward0>)\n",
      "tensor(7.2040, grad_fn=<VarBackward0>)\n",
      "tensor(0.0145, grad_fn=<SumBackward0>)\n",
      "tensor(3580)\n",
      "Epoch: 294\tLoss: 17921.9805\n",
      "tensor(16.6475, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4096, grad_fn=<StdBackward0>)\n",
      "tensor(7.1934, grad_fn=<VarBackward0>)\n",
      "tensor(0.0120, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 295\tLoss: 16221.9424\n",
      "tensor(16.8005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4320, grad_fn=<StdBackward0>)\n",
      "tensor(7.1456, grad_fn=<VarBackward0>)\n",
      "tensor(0.0108, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 296\tLoss: 16222.1982\n",
      "tensor(16.8706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4557, grad_fn=<StdBackward0>)\n",
      "tensor(7.1535, grad_fn=<VarBackward0>)\n",
      "tensor(0.0151, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 297\tLoss: 16222.9326\n",
      "tensor(16.8977, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4344, grad_fn=<StdBackward0>)\n",
      "tensor(7.1109, grad_fn=<VarBackward0>)\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 298\tLoss: 16222.1592\n",
      "tensor(16.9028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4047, grad_fn=<StdBackward0>)\n",
      "tensor(7.0961, grad_fn=<VarBackward0>)\n",
      "tensor(0.0096, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 299\tLoss: 16221.9092\n",
      "tensor(16.9218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3889, grad_fn=<StdBackward0>)\n",
      "tensor(7.0661, grad_fn=<VarBackward0>)\n",
      "tensor(0.0102, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 300\tLoss: 16221.8271\n",
      "tensor(16.9374, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3864, grad_fn=<StdBackward0>)\n",
      "tensor(7.0292, grad_fn=<VarBackward0>)\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 301\tLoss: 16221.7227\n",
      "tensor(16.9248, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4094, grad_fn=<StdBackward0>)\n",
      "tensor(7.0497, grad_fn=<VarBackward0>)\n",
      "tensor(0.0060, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 302\tLoss: 16221.6191\n",
      "tensor(16.9247, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4578, grad_fn=<StdBackward0>)\n",
      "tensor(7.0213, grad_fn=<VarBackward0>)\n",
      "tensor(0.0057, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 303\tLoss: 16222.0762\n",
      "tensor(16.8558, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4885, grad_fn=<StdBackward0>)\n",
      "tensor(7.0791, grad_fn=<VarBackward0>)\n",
      "tensor(0.0115, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 304\tLoss: 16222.8877\n",
      "tensor(16.6920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4661, grad_fn=<StdBackward0>)\n",
      "tensor(7.0685, grad_fn=<VarBackward0>)\n",
      "tensor(0.0050, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 305\tLoss: 16221.8545\n",
      "tensor(16.6689, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4328, grad_fn=<StdBackward0>)\n",
      "tensor(7.0783, grad_fn=<VarBackward0>)\n",
      "tensor(0.0109, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 306\tLoss: 16222.0830\n",
      "tensor(16.5810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4327, grad_fn=<StdBackward0>)\n",
      "tensor(7.1181, grad_fn=<VarBackward0>)\n",
      "tensor(0.0064, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 307\tLoss: 16221.5449\n",
      "tensor(16.5765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4223, grad_fn=<StdBackward0>)\n",
      "tensor(7.1348, grad_fn=<VarBackward0>)\n",
      "tensor(0.0104, grad_fn=<SumBackward0>)\n",
      "tensor(3240)\n",
      "Epoch: 308\tLoss: 16221.8398\n",
      "tensor(16.5208, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4218, grad_fn=<StdBackward0>)\n",
      "tensor(7.1348, grad_fn=<VarBackward0>)\n",
      "tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 309\tLoss: 12521.5371\n",
      "tensor(16.4317, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4267, grad_fn=<StdBackward0>)\n",
      "tensor(7.2030, grad_fn=<VarBackward0>)\n",
      "tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 310\tLoss: 12521.8184\n",
      "tensor(16.4580, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4174, grad_fn=<StdBackward0>)\n",
      "tensor(7.1524, grad_fn=<VarBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 311\tLoss: 12521.7393\n",
      "tensor(16.4343, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4124, grad_fn=<StdBackward0>)\n",
      "tensor(7.2163, grad_fn=<VarBackward0>)\n",
      "tensor(0.0142, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 312\tLoss: 12521.9805\n",
      "tensor(16.5246, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4016, grad_fn=<StdBackward0>)\n",
      "tensor(7.1720, grad_fn=<VarBackward0>)\n",
      "tensor(0.0254, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 313\tLoss: 12523.0781\n",
      "tensor(16.5047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4269, grad_fn=<StdBackward0>)\n",
      "tensor(7.1910, grad_fn=<VarBackward0>)\n",
      "tensor(0.0135, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 314\tLoss: 12522.1211\n",
      "tensor(16.4982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4824, grad_fn=<StdBackward0>)\n",
      "tensor(7.2304, grad_fn=<VarBackward0>)\n",
      "tensor(0.0181, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 315\tLoss: 12523.1348\n",
      "tensor(16.4088, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4861, grad_fn=<StdBackward0>)\n",
      "tensor(7.1841, grad_fn=<VarBackward0>)\n",
      "tensor(0.0206, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 316\tLoss: 12523.3301\n",
      "tensor(16.2927, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4320, grad_fn=<StdBackward0>)\n",
      "tensor(7.2248, grad_fn=<VarBackward0>)\n",
      "tensor(0.0112, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 317\tLoss: 12521.7334\n",
      "tensor(16.2562, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3998, grad_fn=<StdBackward0>)\n",
      "tensor(7.1802, grad_fn=<VarBackward0>)\n",
      "tensor(0.0210, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 318\tLoss: 12522.3564\n",
      "tensor(16.3266, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3889, grad_fn=<StdBackward0>)\n",
      "tensor(7.1750, grad_fn=<VarBackward0>)\n",
      "tensor(0.0107, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 319\tLoss: 12521.2900\n",
      "tensor(16.3786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3926, grad_fn=<StdBackward0>)\n",
      "tensor(7.1746, grad_fn=<VarBackward0>)\n",
      "tensor(0.0144, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 320\tLoss: 12521.7422\n",
      "tensor(16.4213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4051, grad_fn=<StdBackward0>)\n",
      "tensor(7.1493, grad_fn=<VarBackward0>)\n",
      "tensor(0.0059, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 321\tLoss: 12521.0635\n",
      "tensor(16.4105, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4275, grad_fn=<StdBackward0>)\n",
      "tensor(7.1642, grad_fn=<VarBackward0>)\n",
      "tensor(0.0101, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 322\tLoss: 12521.6934\n",
      "tensor(16.3890, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4322, grad_fn=<StdBackward0>)\n",
      "tensor(7.1426, grad_fn=<VarBackward0>)\n",
      "tensor(0.0091, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 323\tLoss: 12521.6250\n",
      "tensor(16.3593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4179, grad_fn=<StdBackward0>)\n",
      "tensor(7.1296, grad_fn=<VarBackward0>)\n",
      "tensor(0.0100, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 324\tLoss: 12521.5410\n",
      "tensor(16.2657, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4191, grad_fn=<StdBackward0>)\n",
      "tensor(7.1365, grad_fn=<VarBackward0>)\n",
      "tensor(0.0085, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 325\tLoss: 12521.3105\n",
      "tensor(16.2408, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4382, grad_fn=<StdBackward0>)\n",
      "tensor(7.1187, grad_fn=<VarBackward0>)\n",
      "tensor(0.0240, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 326\tLoss: 12523.0186\n",
      "tensor(16.1593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4782, grad_fn=<StdBackward0>)\n",
      "tensor(7.1273, grad_fn=<VarBackward0>)\n",
      "tensor(0.0084, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 327\tLoss: 12521.7783\n",
      "tensor(16.1215, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4584, grad_fn=<StdBackward0>)\n",
      "tensor(7.1658, grad_fn=<VarBackward0>)\n",
      "tensor(0.0241, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 328\tLoss: 12523.1123\n",
      "tensor(16.0422, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4240, grad_fn=<StdBackward0>)\n",
      "tensor(7.1097, grad_fn=<VarBackward0>)\n",
      "tensor(0.0227, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 329\tLoss: 12522.5488\n",
      "tensor(15.9613, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4059, grad_fn=<StdBackward0>)\n",
      "tensor(7.1611, grad_fn=<VarBackward0>)\n",
      "tensor(0.0063, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 330\tLoss: 12520.6523\n",
      "tensor(16.0138, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3918, grad_fn=<StdBackward0>)\n",
      "tensor(7.1094, grad_fn=<VarBackward0>)\n",
      "tensor(0.0251, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 331\tLoss: 12522.4463\n",
      "tensor(16.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3960, grad_fn=<StdBackward0>)\n",
      "tensor(7.1380, grad_fn=<VarBackward0>)\n",
      "tensor(0.0117, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 332\tLoss: 12521.2002\n",
      "tensor(16.1210, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4237, grad_fn=<StdBackward0>)\n",
      "tensor(7.1284, grad_fn=<VarBackward0>)\n",
      "tensor(0.0150, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 333\tLoss: 12521.8574\n",
      "tensor(16.1585, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4598, grad_fn=<StdBackward0>)\n",
      "tensor(7.1243, grad_fn=<VarBackward0>)\n",
      "tensor(0.0125, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 334\tLoss: 12522.0049\n",
      "tensor(16.1099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4552, grad_fn=<StdBackward0>)\n",
      "tensor(7.1346, grad_fn=<VarBackward0>)\n",
      "tensor(0.0111, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 335\tLoss: 12521.7676\n",
      "tensor(16.1143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4201, grad_fn=<StdBackward0>)\n",
      "tensor(7.0861, grad_fn=<VarBackward0>)\n",
      "tensor(0.0097, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 336\tLoss: 12521.2812\n",
      "tensor(16.1703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4129, grad_fn=<StdBackward0>)\n",
      "tensor(7.1077, grad_fn=<VarBackward0>)\n",
      "tensor(0.0120, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 337\tLoss: 12521.4961\n",
      "tensor(16.1706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4355, grad_fn=<StdBackward0>)\n",
      "tensor(7.0697, grad_fn=<VarBackward0>)\n",
      "tensor(0.0204, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 338\tLoss: 12522.5693\n",
      "tensor(16.1088, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4375, grad_fn=<StdBackward0>)\n",
      "tensor(7.1062, grad_fn=<VarBackward0>)\n",
      "tensor(0.0100, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 339\tLoss: 12521.4834\n",
      "tensor(15.9348, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4189, grad_fn=<StdBackward0>)\n",
      "tensor(7.0551, grad_fn=<VarBackward0>)\n",
      "tensor(0.0196, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 340\tLoss: 12522.0820\n",
      "tensor(15.8818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3941, grad_fn=<StdBackward0>)\n",
      "tensor(7.0777, grad_fn=<VarBackward0>)\n",
      "tensor(0.0104, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 341\tLoss: 12520.8643\n",
      "tensor(15.9177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3798, grad_fn=<StdBackward0>)\n",
      "tensor(7.0353, grad_fn=<VarBackward0>)\n",
      "tensor(0.0277, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 342\tLoss: 12522.4863\n",
      "tensor(15.9645, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3776, grad_fn=<StdBackward0>)\n",
      "tensor(7.0547, grad_fn=<VarBackward0>)\n",
      "tensor(0.0313, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 343\tLoss: 12522.8721\n",
      "tensor(16.0205, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3983, grad_fn=<StdBackward0>)\n",
      "tensor(7.0237, grad_fn=<VarBackward0>)\n",
      "tensor(0.0053, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 344\tLoss: 12520.5352\n",
      "tensor(15.9716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4604, grad_fn=<StdBackward0>)\n",
      "tensor(7.0822, grad_fn=<VarBackward0>)\n",
      "tensor(0.0255, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 345\tLoss: 12523.1299\n",
      "tensor(15.8934, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4611, grad_fn=<StdBackward0>)\n",
      "tensor(7.0616, grad_fn=<VarBackward0>)\n",
      "tensor(0.0233, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 346\tLoss: 12522.8369\n",
      "tensor(15.8130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4084, grad_fn=<StdBackward0>)\n",
      "tensor(7.0706, grad_fn=<VarBackward0>)\n",
      "tensor(0.0023, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 347\tLoss: 12520.1270\n",
      "tensor(15.7381, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3868, grad_fn=<StdBackward0>)\n",
      "tensor(7.0762, grad_fn=<VarBackward0>)\n",
      "tensor(0.0036, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 348\tLoss: 12519.9609\n",
      "tensor(15.7566, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3846, grad_fn=<StdBackward0>)\n",
      "tensor(7.0649, grad_fn=<VarBackward0>)\n",
      "tensor(0.0179, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 349\tLoss: 12521.3936\n",
      "tensor(15.6997, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4110, grad_fn=<StdBackward0>)\n",
      "tensor(7.0877, grad_fn=<VarBackward0>)\n",
      "tensor(0.0141, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 350\tLoss: 12521.2168\n",
      "tensor(15.7067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4225, grad_fn=<StdBackward0>)\n",
      "tensor(7.1002, grad_fn=<VarBackward0>)\n",
      "tensor(0.0122, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 351\tLoss: 12521.1533\n",
      "tensor(15.5889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4200, grad_fn=<StdBackward0>)\n",
      "tensor(7.0862, grad_fn=<VarBackward0>)\n",
      "tensor(0.0134, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 352\tLoss: 12521.1270\n",
      "tensor(15.5143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4025, grad_fn=<StdBackward0>)\n",
      "tensor(7.1259, grad_fn=<VarBackward0>)\n",
      "tensor(0.0080, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 353\tLoss: 12520.3428\n",
      "tensor(15.5257, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3929, grad_fn=<StdBackward0>)\n",
      "tensor(7.0707, grad_fn=<VarBackward0>)\n",
      "tensor(0.0046, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 354\tLoss: 12519.9111\n",
      "tensor(15.5348, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4105, grad_fn=<StdBackward0>)\n",
      "tensor(7.1309, grad_fn=<VarBackward0>)\n",
      "tensor(0.0211, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 355\tLoss: 12521.7520\n",
      "tensor(15.5380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4369, grad_fn=<StdBackward0>)\n",
      "tensor(7.0966, grad_fn=<VarBackward0>)\n",
      "tensor(0.0210, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 356\tLoss: 12522.0059\n",
      "tensor(15.4651, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4354, grad_fn=<StdBackward0>)\n",
      "tensor(7.1311, grad_fn=<VarBackward0>)\n",
      "tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 357\tLoss: 12520.1182\n",
      "tensor(15.3441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4118, grad_fn=<StdBackward0>)\n",
      "tensor(7.1307, grad_fn=<VarBackward0>)\n",
      "tensor(0.0284, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 358\tLoss: 12522.2988\n",
      "tensor(15.3477, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3838, grad_fn=<StdBackward0>)\n",
      "tensor(7.1060, grad_fn=<VarBackward0>)\n",
      "tensor(0.0275, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 359\tLoss: 12521.9355\n",
      "tensor(15.3216, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3768, grad_fn=<StdBackward0>)\n",
      "tensor(7.1136, grad_fn=<VarBackward0>)\n",
      "tensor(0.0062, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 360\tLoss: 12519.7090\n",
      "tensor(15.4272, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3707, grad_fn=<StdBackward0>)\n",
      "tensor(7.0820, grad_fn=<VarBackward0>)\n",
      "tensor(0.0204, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 361\tLoss: 12521.1768\n",
      "tensor(15.4239, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3967, grad_fn=<StdBackward0>)\n",
      "tensor(7.0776, grad_fn=<VarBackward0>)\n",
      "tensor(0.0199, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 362\tLoss: 12521.3799\n",
      "tensor(15.4679, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4337, grad_fn=<StdBackward0>)\n",
      "tensor(7.1004, grad_fn=<VarBackward0>)\n",
      "tensor(0.0035, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 363\tLoss: 12520.1543\n",
      "tensor(15.3852, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4424, grad_fn=<StdBackward0>)\n",
      "tensor(7.0558, grad_fn=<VarBackward0>)\n",
      "tensor(0.0071, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 364\tLoss: 12520.5195\n",
      "tensor(15.3220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3995, grad_fn=<StdBackward0>)\n",
      "tensor(7.0893, grad_fn=<VarBackward0>)\n",
      "tensor(0.0049, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 365\tLoss: 12519.8037\n",
      "tensor(15.3101, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3716, grad_fn=<StdBackward0>)\n",
      "tensor(7.0300, grad_fn=<VarBackward0>)\n",
      "tensor(0.0040, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 366\tLoss: 12519.4229\n",
      "tensor(15.3072, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3723, grad_fn=<StdBackward0>)\n",
      "tensor(7.0533, grad_fn=<VarBackward0>)\n",
      "tensor(0.0040, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 367\tLoss: 12519.4307\n",
      "tensor(15.3310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3931, grad_fn=<StdBackward0>)\n",
      "tensor(7.0347, grad_fn=<VarBackward0>)\n",
      "tensor(0.0094, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 368\tLoss: 12520.2061\n",
      "tensor(15.2806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4298, grad_fn=<StdBackward0>)\n",
      "tensor(7.0742, grad_fn=<VarBackward0>)\n",
      "tensor(0.0058, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 369\tLoss: 12520.1543\n",
      "tensor(15.1994, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4206, grad_fn=<StdBackward0>)\n",
      "tensor(7.0645, grad_fn=<VarBackward0>)\n",
      "tensor(0.0109, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 370\tLoss: 12520.5000\n",
      "tensor(15.1460, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3854, grad_fn=<StdBackward0>)\n",
      "tensor(7.0513, grad_fn=<VarBackward0>)\n",
      "tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 371\tLoss: 12519.2998\n",
      "tensor(15.0929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3743, grad_fn=<StdBackward0>)\n",
      "tensor(7.0580, grad_fn=<VarBackward0>)\n",
      "tensor(0.0175, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 372\tLoss: 12520.5879\n",
      "tensor(15.1659, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3767, grad_fn=<StdBackward0>)\n",
      "tensor(7.0423, grad_fn=<VarBackward0>)\n",
      "tensor(0.0092, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 373\tLoss: 12519.8545\n",
      "tensor(15.1004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4175, grad_fn=<StdBackward0>)\n",
      "tensor(7.0500, grad_fn=<VarBackward0>)\n",
      "tensor(0.0211, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 374\tLoss: 12521.3896\n",
      "tensor(15.1105, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4242, grad_fn=<StdBackward0>)\n",
      "tensor(7.0653, grad_fn=<VarBackward0>)\n",
      "tensor(0.0258, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 375\tLoss: 12521.9307\n",
      "tensor(14.9928, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4055, grad_fn=<StdBackward0>)\n",
      "tensor(7.0175, grad_fn=<VarBackward0>)\n",
      "tensor(0.0075, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 376\tLoss: 12519.8027\n",
      "tensor(14.9483, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3786, grad_fn=<StdBackward0>)\n",
      "tensor(7.0561, grad_fn=<VarBackward0>)\n",
      "tensor(0.0317, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 377\tLoss: 12521.9082\n",
      "tensor(14.9826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3631, grad_fn=<StdBackward0>)\n",
      "tensor(6.9892, grad_fn=<VarBackward0>)\n",
      "tensor(0.0438, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 378\tLoss: 12522.9951\n",
      "tensor(14.9882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3686, grad_fn=<StdBackward0>)\n",
      "tensor(7.0421, grad_fn=<VarBackward0>)\n",
      "tensor(0.0311, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 379\tLoss: 12521.7871\n",
      "tensor(15.0509, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3829, grad_fn=<StdBackward0>)\n",
      "tensor(6.9832, grad_fn=<VarBackward0>)\n",
      "tensor(0.0053, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 380\tLoss: 12519.4150\n",
      "tensor(15.0374, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4155, grad_fn=<StdBackward0>)\n",
      "tensor(7.0516, grad_fn=<VarBackward0>)\n",
      "tensor(0.0194, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 381\tLoss: 12521.1348\n",
      "tensor(14.9929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4209, grad_fn=<StdBackward0>)\n",
      "tensor(7.0132, grad_fn=<VarBackward0>)\n",
      "tensor(0.0118, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 382\tLoss: 12520.3789\n",
      "tensor(14.9889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3892, grad_fn=<StdBackward0>)\n",
      "tensor(7.0185, grad_fn=<VarBackward0>)\n",
      "tensor(0.0155, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 383\tLoss: 12520.4316\n",
      "tensor(14.9216, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3776, grad_fn=<StdBackward0>)\n",
      "tensor(7.0335, grad_fn=<VarBackward0>)\n",
      "tensor(0.0191, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 384\tLoss: 12520.6084\n",
      "tensor(15.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3623, grad_fn=<StdBackward0>)\n",
      "tensor(7.0045, grad_fn=<VarBackward0>)\n",
      "tensor(0.0037, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 385\tLoss: 12518.9932\n",
      "tensor(14.9298, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3744, grad_fn=<StdBackward0>)\n",
      "tensor(7.0388, grad_fn=<VarBackward0>)\n",
      "tensor(0.0278, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 386\tLoss: 12521.4590\n",
      "tensor(15.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3787, grad_fn=<StdBackward0>)\n",
      "tensor(7.0149, grad_fn=<VarBackward0>)\n",
      "tensor(0.0326, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 387\tLoss: 12522.0527\n",
      "tensor(14.9364, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4088, grad_fn=<StdBackward0>)\n",
      "tensor(7.0112, grad_fn=<VarBackward0>)\n",
      "tensor(0.0175, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 388\tLoss: 12520.7764\n",
      "tensor(14.9523, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4072, grad_fn=<StdBackward0>)\n",
      "tensor(7.0384, grad_fn=<VarBackward0>)\n",
      "tensor(0.0154, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 389\tLoss: 12520.5664\n",
      "tensor(14.9140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3901, grad_fn=<StdBackward0>)\n",
      "tensor(6.9703, grad_fn=<VarBackward0>)\n",
      "tensor(0.0249, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 390\tLoss: 12521.3037\n",
      "tensor(14.8839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3729, grad_fn=<StdBackward0>)\n",
      "tensor(7.0389, grad_fn=<VarBackward0>)\n",
      "tensor(0.0130, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 391\tLoss: 12519.9131\n",
      "tensor(14.9064, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3603, grad_fn=<StdBackward0>)\n",
      "tensor(6.9635, grad_fn=<VarBackward0>)\n",
      "tensor(0.0192, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 392\tLoss: 12520.4336\n",
      "tensor(14.8940, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3639, grad_fn=<StdBackward0>)\n",
      "tensor(7.0307, grad_fn=<VarBackward0>)\n",
      "tensor(0.0275, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 393\tLoss: 12521.2812\n",
      "tensor(14.8844, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3812, grad_fn=<StdBackward0>)\n",
      "tensor(6.9647, grad_fn=<VarBackward0>)\n",
      "tensor(0.0147, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 394\tLoss: 12520.1709\n",
      "tensor(14.8788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4001, grad_fn=<StdBackward0>)\n",
      "tensor(7.0299, grad_fn=<VarBackward0>)\n",
      "tensor(0.0175, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 395\tLoss: 12520.6279\n",
      "tensor(14.7820, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4048, grad_fn=<StdBackward0>)\n",
      "tensor(6.9948, grad_fn=<VarBackward0>)\n",
      "tensor(0.0257, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 396\tLoss: 12521.4004\n",
      "tensor(14.7982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3708, grad_fn=<StdBackward0>)\n",
      "tensor(7.0144, grad_fn=<VarBackward0>)\n",
      "tensor(0.0118, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 397\tLoss: 12519.6904\n",
      "tensor(14.7227, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3612, grad_fn=<StdBackward0>)\n",
      "tensor(7.0150, grad_fn=<VarBackward0>)\n",
      "tensor(0.0212, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 398\tLoss: 12520.4551\n",
      "tensor(14.7965, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3491, grad_fn=<StdBackward0>)\n",
      "tensor(7.0073, grad_fn=<VarBackward0>)\n",
      "tensor(0.0303, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 399\tLoss: 12521.3193\n",
      "tensor(14.7458, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3692, grad_fn=<StdBackward0>)\n",
      "tensor(7.0064, grad_fn=<VarBackward0>)\n",
      "tensor(0.0184, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 400\tLoss: 12520.2812\n",
      "tensor(14.8170, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3850, grad_fn=<StdBackward0>)\n",
      "tensor(7.0176, grad_fn=<VarBackward0>)\n",
      "tensor(0.0128, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 401\tLoss: 12519.9512\n",
      "tensor(14.7186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4071, grad_fn=<StdBackward0>)\n",
      "tensor(6.9918, grad_fn=<VarBackward0>)\n",
      "tensor(0.0203, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 402\tLoss: 12520.8232\n",
      "tensor(14.7163, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3760, grad_fn=<StdBackward0>)\n",
      "tensor(7.0291, grad_fn=<VarBackward0>)\n",
      "tensor(0.0061, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 403\tLoss: 12519.0820\n",
      "tensor(14.6751, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3584, grad_fn=<StdBackward0>)\n",
      "tensor(6.9702, grad_fn=<VarBackward0>)\n",
      "tensor(0.0272, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 404\tLoss: 12520.9766\n",
      "tensor(14.6878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3542, grad_fn=<StdBackward0>)\n",
      "tensor(7.0235, grad_fn=<VarBackward0>)\n",
      "tensor(0.0366, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 405\tLoss: 12521.8906\n",
      "tensor(14.7130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3668, grad_fn=<StdBackward0>)\n",
      "tensor(6.9609, grad_fn=<VarBackward0>)\n",
      "tensor(0.0250, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 406\tLoss: 12520.8809\n",
      "tensor(14.7074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3884, grad_fn=<StdBackward0>)\n",
      "tensor(7.0299, grad_fn=<VarBackward0>)\n",
      "tensor(0.0055, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 407\tLoss: 12519.1377\n",
      "tensor(14.6647, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3929, grad_fn=<StdBackward0>)\n",
      "tensor(6.9843, grad_fn=<VarBackward0>)\n",
      "tensor(0.0128, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 408\tLoss: 12519.8711\n",
      "tensor(14.6412, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3765, grad_fn=<StdBackward0>)\n",
      "tensor(7.0023, grad_fn=<VarBackward0>)\n",
      "tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 409\tLoss: 12518.7031\n",
      "tensor(14.5719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3770, grad_fn=<StdBackward0>)\n",
      "tensor(7.0204, grad_fn=<VarBackward0>)\n",
      "tensor(0.0086, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 410\tLoss: 12519.1982\n",
      "tensor(14.6084, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3715, grad_fn=<StdBackward0>)\n",
      "tensor(6.9710, grad_fn=<VarBackward0>)\n",
      "tensor(0.0047, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 411\tLoss: 12518.7891\n",
      "tensor(14.5387, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3807, grad_fn=<StdBackward0>)\n",
      "tensor(7.0365, grad_fn=<VarBackward0>)\n",
      "tensor(0.0026, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 412\tLoss: 12518.6006\n",
      "tensor(14.5352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3751, grad_fn=<StdBackward0>)\n",
      "tensor(6.9715, grad_fn=<VarBackward0>)\n",
      "tensor(0.0018, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 413\tLoss: 12518.4678\n",
      "tensor(14.5172, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3754, grad_fn=<StdBackward0>)\n",
      "tensor(7.0145, grad_fn=<VarBackward0>)\n",
      "tensor(0.0121, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 414\tLoss: 12519.4795\n",
      "tensor(14.4424, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3854, grad_fn=<StdBackward0>)\n",
      "tensor(6.9921, grad_fn=<VarBackward0>)\n",
      "tensor(0.0030, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 415\tLoss: 12518.5977\n",
      "tensor(14.4776, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3703, grad_fn=<StdBackward0>)\n",
      "tensor(6.9807, grad_fn=<VarBackward0>)\n",
      "tensor(0.0268, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 416\tLoss: 12520.8584\n",
      "tensor(14.3699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3709, grad_fn=<StdBackward0>)\n",
      "tensor(6.9850, grad_fn=<VarBackward0>)\n",
      "tensor(0.0322, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 417\tLoss: 12521.2998\n",
      "tensor(14.4341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3559, grad_fn=<StdBackward0>)\n",
      "tensor(6.9595, grad_fn=<VarBackward0>)\n",
      "tensor(0.0152, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 418\tLoss: 12519.5166\n",
      "tensor(14.3816, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3774, grad_fn=<StdBackward0>)\n",
      "tensor(6.9716, grad_fn=<VarBackward0>)\n",
      "tensor(0.0223, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 419\tLoss: 12520.3877\n",
      "tensor(14.4130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4002, grad_fn=<StdBackward0>)\n",
      "tensor(6.9705, grad_fn=<VarBackward0>)\n",
      "tensor(0.0339, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 420\tLoss: 12521.8008\n",
      "tensor(14.3723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4044, grad_fn=<StdBackward0>)\n",
      "tensor(6.9787, grad_fn=<VarBackward0>)\n",
      "tensor(0.0215, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 421\tLoss: 12520.5713\n",
      "tensor(14.2991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3864, grad_fn=<StdBackward0>)\n",
      "tensor(6.9720, grad_fn=<VarBackward0>)\n",
      "tensor(0.0120, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 422\tLoss: 12519.3613\n",
      "tensor(14.3058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3707, grad_fn=<StdBackward0>)\n",
      "tensor(7.0003, grad_fn=<VarBackward0>)\n",
      "tensor(0.0199, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 423\tLoss: 12520.0010\n",
      "tensor(14.2496, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3849, grad_fn=<StdBackward0>)\n",
      "tensor(6.9606, grad_fn=<VarBackward0>)\n",
      "tensor(0.0051, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 424\tLoss: 12518.6055\n",
      "tensor(14.2737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3816, grad_fn=<StdBackward0>)\n",
      "tensor(7.0303, grad_fn=<VarBackward0>)\n",
      "tensor(0.0306, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 425\tLoss: 12521.1514\n",
      "tensor(14.2129, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3740, grad_fn=<StdBackward0>)\n",
      "tensor(6.9554, grad_fn=<VarBackward0>)\n",
      "tensor(0.0403, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 426\tLoss: 12521.9824\n",
      "tensor(14.2321, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3529, grad_fn=<StdBackward0>)\n",
      "tensor(7.0172, grad_fn=<VarBackward0>)\n",
      "tensor(0.0262, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 427\tLoss: 12520.3828\n",
      "tensor(14.2222, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3496, grad_fn=<StdBackward0>)\n",
      "tensor(6.9597, grad_fn=<VarBackward0>)\n",
      "tensor(0.0087, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 428\tLoss: 12518.5898\n",
      "tensor(14.2800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3544, grad_fn=<StdBackward0>)\n",
      "tensor(7.0061, grad_fn=<VarBackward0>)\n",
      "tensor(0.0179, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 429\tLoss: 12519.6162\n",
      "tensor(14.2578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3800, grad_fn=<StdBackward0>)\n",
      "tensor(6.9609, grad_fn=<VarBackward0>)\n",
      "tensor(0.0043, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 430\tLoss: 12518.4854\n",
      "tensor(14.2910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3734, grad_fn=<StdBackward0>)\n",
      "tensor(7.0036, grad_fn=<VarBackward0>)\n",
      "tensor(0.0303, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 431\tLoss: 12521.0547\n",
      "tensor(14.2161, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3641, grad_fn=<StdBackward0>)\n",
      "tensor(6.9693, grad_fn=<VarBackward0>)\n",
      "tensor(0.0390, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 432\tLoss: 12521.7598\n",
      "tensor(14.2491, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3475, grad_fn=<StdBackward0>)\n",
      "tensor(6.9866, grad_fn=<VarBackward0>)\n",
      "tensor(0.0243, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 433\tLoss: 12520.1514\n",
      "tensor(14.2140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3640, grad_fn=<StdBackward0>)\n",
      "tensor(6.9819, grad_fn=<VarBackward0>)\n",
      "tensor(0.0110, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 434\tLoss: 12518.9531\n",
      "tensor(14.2319, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3830, grad_fn=<StdBackward0>)\n",
      "tensor(6.9865, grad_fn=<VarBackward0>)\n",
      "tensor(0.0207, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 435\tLoss: 12520.1279\n",
      "tensor(14.1846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3895, grad_fn=<StdBackward0>)\n",
      "tensor(6.9903, grad_fn=<VarBackward0>)\n",
      "tensor(0.0077, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 436\tLoss: 12518.8516\n",
      "tensor(14.1232, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3735, grad_fn=<StdBackward0>)\n",
      "tensor(6.9887, grad_fn=<VarBackward0>)\n",
      "tensor(0.0257, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 437\tLoss: 12520.4307\n",
      "tensor(14.1363, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3626, grad_fn=<StdBackward0>)\n",
      "tensor(6.9805, grad_fn=<VarBackward0>)\n",
      "tensor(0.0339, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 438\tLoss: 12521.1553\n",
      "tensor(14.0904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3782, grad_fn=<StdBackward0>)\n",
      "tensor(6.9973, grad_fn=<VarBackward0>)\n",
      "tensor(0.0192, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 439\tLoss: 12519.7920\n",
      "tensor(14.1236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3712, grad_fn=<StdBackward0>)\n",
      "tensor(6.9824, grad_fn=<VarBackward0>)\n",
      "tensor(0.0158, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 440\tLoss: 12519.4131\n",
      "tensor(14.0610, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3632, grad_fn=<StdBackward0>)\n",
      "tensor(6.9956, grad_fn=<VarBackward0>)\n",
      "tensor(0.0256, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 441\tLoss: 12520.2500\n",
      "tensor(14.0632, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3481, grad_fn=<StdBackward0>)\n",
      "tensor(6.9871, grad_fn=<VarBackward0>)\n",
      "tensor(0.0129, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 442\tLoss: 12518.8330\n",
      "tensor(14.0670, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3471, grad_fn=<StdBackward0>)\n",
      "tensor(6.9851, grad_fn=<VarBackward0>)\n",
      "tensor(0.0202, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 443\tLoss: 12519.5537\n",
      "tensor(14.0652, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3590, grad_fn=<StdBackward0>)\n",
      "tensor(6.9861, grad_fn=<VarBackward0>)\n",
      "tensor(0.0282, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 444\tLoss: 12520.4727\n",
      "tensor(14.0897, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3664, grad_fn=<StdBackward0>)\n",
      "tensor(6.9842, grad_fn=<VarBackward0>)\n",
      "tensor(0.0135, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 445\tLoss: 12519.0996\n",
      "tensor(14.0397, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3673, grad_fn=<StdBackward0>)\n",
      "tensor(6.9713, grad_fn=<VarBackward0>)\n",
      "tensor(0.0214, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 446\tLoss: 12519.8496\n",
      "tensor(14.0336, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3528, grad_fn=<StdBackward0>)\n",
      "tensor(6.9849, grad_fn=<VarBackward0>)\n",
      "tensor(0.0311, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 447\tLoss: 12520.6729\n",
      "tensor(14.0148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3532, grad_fn=<StdBackward0>)\n",
      "tensor(6.9435, grad_fn=<VarBackward0>)\n",
      "tensor(0.0183, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 448\tLoss: 12519.3750\n",
      "tensor(14.0167, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3640, grad_fn=<StdBackward0>)\n",
      "tensor(6.9841, grad_fn=<VarBackward0>)\n",
      "tensor(0.0154, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 449\tLoss: 12519.1934\n",
      "tensor(14.0120, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3703, grad_fn=<StdBackward0>)\n",
      "tensor(6.9278, grad_fn=<VarBackward0>)\n",
      "tensor(0.0234, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 450\tLoss: 12520.0537\n",
      "tensor(13.9740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3624, grad_fn=<StdBackward0>)\n",
      "tensor(6.9607, grad_fn=<VarBackward0>)\n",
      "tensor(0.0078, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 451\tLoss: 12518.3750\n",
      "tensor(13.9590, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3517, grad_fn=<StdBackward0>)\n",
      "tensor(6.9404, grad_fn=<VarBackward0>)\n",
      "tensor(0.0292, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 452\tLoss: 12520.3906\n",
      "tensor(13.9396, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3567, grad_fn=<StdBackward0>)\n",
      "tensor(6.9311, grad_fn=<VarBackward0>)\n",
      "tensor(0.0396, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 453\tLoss: 12521.4629\n",
      "tensor(13.9576, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3646, grad_fn=<StdBackward0>)\n",
      "tensor(6.9633, grad_fn=<VarBackward0>)\n",
      "tensor(0.0259, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 454\tLoss: 12520.1914\n",
      "tensor(13.9335, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3663, grad_fn=<StdBackward0>)\n",
      "tensor(6.9175, grad_fn=<VarBackward0>)\n",
      "tensor(0.0093, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 455\tLoss: 12518.5215\n",
      "tensor(13.9354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3533, grad_fn=<StdBackward0>)\n",
      "tensor(6.9676, grad_fn=<VarBackward0>)\n",
      "tensor(0.0179, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 456\tLoss: 12519.2559\n",
      "tensor(13.8719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3567, grad_fn=<StdBackward0>)\n",
      "tensor(6.9253, grad_fn=<VarBackward0>)\n",
      "tensor(0.0027, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 457\tLoss: 12517.7129\n",
      "tensor(13.9029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3548, grad_fn=<StdBackward0>)\n",
      "tensor(6.9646, grad_fn=<VarBackward0>)\n",
      "tensor(0.0245, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 458\tLoss: 12519.8955\n",
      "tensor(13.8196, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3683, grad_fn=<StdBackward0>)\n",
      "tensor(6.9355, grad_fn=<VarBackward0>)\n",
      "tensor(0.0252, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 459\tLoss: 12520.0234\n",
      "tensor(13.8643, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3468, grad_fn=<StdBackward0>)\n",
      "tensor(6.9525, grad_fn=<VarBackward0>)\n",
      "tensor(0.0039, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 460\tLoss: 12517.7207\n",
      "tensor(13.7905, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3451, grad_fn=<StdBackward0>)\n",
      "tensor(6.9284, grad_fn=<VarBackward0>)\n",
      "tensor(0.0313, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 461\tLoss: 12520.3682\n",
      "tensor(13.8385, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3388, grad_fn=<StdBackward0>)\n",
      "tensor(6.9367, grad_fn=<VarBackward0>)\n",
      "tensor(0.0387, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 462\tLoss: 12521.0986\n",
      "tensor(13.8018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3565, grad_fn=<StdBackward0>)\n",
      "tensor(6.9189, grad_fn=<VarBackward0>)\n",
      "tensor(0.0216, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 463\tLoss: 12519.5225\n",
      "tensor(13.8335, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3573, grad_fn=<StdBackward0>)\n",
      "tensor(6.9300, grad_fn=<VarBackward0>)\n",
      "tensor(0.0171, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 464\tLoss: 12519.1172\n",
      "tensor(13.7831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3561, grad_fn=<StdBackward0>)\n",
      "tensor(6.9109, grad_fn=<VarBackward0>)\n",
      "tensor(0.0287, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 465\tLoss: 12520.2158\n",
      "tensor(13.7894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3450, grad_fn=<StdBackward0>)\n",
      "tensor(6.9204, grad_fn=<VarBackward0>)\n",
      "tensor(0.0162, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 466\tLoss: 12518.8574\n",
      "tensor(13.7616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3510, grad_fn=<StdBackward0>)\n",
      "tensor(6.9091, grad_fn=<VarBackward0>)\n",
      "tensor(0.0184, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 467\tLoss: 12519.1104\n",
      "tensor(13.7618, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3584, grad_fn=<StdBackward0>)\n",
      "tensor(6.9180, grad_fn=<VarBackward0>)\n",
      "tensor(0.0262, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 468\tLoss: 12519.9629\n",
      "tensor(13.7253, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3576, grad_fn=<StdBackward0>)\n",
      "tensor(6.9157, grad_fn=<VarBackward0>)\n",
      "tensor(0.0095, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 469\tLoss: 12518.2539\n",
      "tensor(13.7132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3428, grad_fn=<StdBackward0>)\n",
      "tensor(6.9112, grad_fn=<VarBackward0>)\n",
      "tensor(0.0288, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 470\tLoss: 12520.0176\n",
      "tensor(13.6841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3370, grad_fn=<StdBackward0>)\n",
      "tensor(6.9166, grad_fn=<VarBackward0>)\n",
      "tensor(0.0399, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 471\tLoss: 12521.0469\n",
      "tensor(13.7117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3368, grad_fn=<StdBackward0>)\n",
      "tensor(6.9012, grad_fn=<VarBackward0>)\n",
      "tensor(0.0267, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 472\tLoss: 12519.7529\n",
      "tensor(13.6893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3483, grad_fn=<StdBackward0>)\n",
      "tensor(6.9113, grad_fn=<VarBackward0>)\n",
      "tensor(0.0087, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 473\tLoss: 12518.0371\n",
      "tensor(13.7100, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3440, grad_fn=<StdBackward0>)\n",
      "tensor(6.8939, grad_fn=<VarBackward0>)\n",
      "tensor(0.0170, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 474\tLoss: 12518.8467\n",
      "tensor(13.6605, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3404, grad_fn=<StdBackward0>)\n",
      "tensor(6.8967, grad_fn=<VarBackward0>)\n",
      "tensor(0.0021, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 475\tLoss: 12517.2793\n",
      "tensor(13.6693, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3316, grad_fn=<StdBackward0>)\n",
      "tensor(6.8939, grad_fn=<VarBackward0>)\n",
      "tensor(0.0019, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 476\tLoss: 12517.1709\n",
      "tensor(13.6034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3450, grad_fn=<StdBackward0>)\n",
      "tensor(6.8807, grad_fn=<VarBackward0>)\n",
      "tensor(0.0156, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 477\tLoss: 12518.6104\n",
      "tensor(13.6113, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3506, grad_fn=<StdBackward0>)\n",
      "tensor(6.9048, grad_fn=<VarBackward0>)\n",
      "tensor(0.0043, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 478\tLoss: 12517.5439\n",
      "tensor(13.5251, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3567, grad_fn=<StdBackward0>)\n",
      "tensor(6.8733, grad_fn=<VarBackward0>)\n",
      "tensor(0.0315, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 479\tLoss: 12520.2402\n",
      "tensor(13.5256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3392, grad_fn=<StdBackward0>)\n",
      "tensor(6.9068, grad_fn=<VarBackward0>)\n",
      "tensor(0.0383, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 480\tLoss: 12520.7490\n",
      "tensor(13.4640, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3436, grad_fn=<StdBackward0>)\n",
      "tensor(6.8696, grad_fn=<VarBackward0>)\n",
      "tensor(0.0182, grad_fn=<SumBackward0>)\n",
      "tensor(2799)\n",
      "Epoch: 481\tLoss: 14013.7256\n",
      "tensor(13.4976, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3421, grad_fn=<StdBackward0>)\n",
      "tensor(6.9028, grad_fn=<VarBackward0>)\n",
      "tensor(0.0274, grad_fn=<SumBackward0>)\n",
      "tensor(2914)\n",
      "Epoch: 482\tLoss: 14589.6631\n",
      "tensor(13.4441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3497, grad_fn=<StdBackward0>)\n",
      "tensor(6.8702, grad_fn=<VarBackward0>)\n",
      "tensor(0.0412, grad_fn=<SumBackward0>)\n",
      "tensor(2914)\n",
      "Epoch: 483\tLoss: 14591.0566\n",
      "tensor(13.4777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3313, grad_fn=<StdBackward0>)\n",
      "tensor(6.8959, grad_fn=<VarBackward0>)\n",
      "tensor(0.0250, grad_fn=<SumBackward0>)\n",
      "tensor(2914)\n",
      "Epoch: 484\tLoss: 14589.2939\n",
      "tensor(13.4331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3314, grad_fn=<StdBackward0>)\n",
      "tensor(6.8706, grad_fn=<VarBackward0>)\n",
      "tensor(0.0168, grad_fn=<SumBackward0>)\n",
      "tensor(2799)\n",
      "Epoch: 485\tLoss: 14013.4326\n",
      "tensor(13.4961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3276, grad_fn=<StdBackward0>)\n",
      "tensor(6.8963, grad_fn=<VarBackward0>)\n",
      "tensor(0.0272, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 486\tLoss: 12519.4951\n",
      "tensor(13.4424, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3487, grad_fn=<StdBackward0>)\n",
      "tensor(6.8714, grad_fn=<VarBackward0>)\n",
      "tensor(0.0101, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 487\tLoss: 12517.9395\n",
      "tensor(13.4920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3370, grad_fn=<StdBackward0>)\n",
      "tensor(6.9052, grad_fn=<VarBackward0>)\n",
      "tensor(0.0320, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 488\tLoss: 12520.0635\n",
      "tensor(13.4108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3414, grad_fn=<StdBackward0>)\n",
      "tensor(6.8714, grad_fn=<VarBackward0>)\n",
      "tensor(0.0428, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 489\tLoss: 12521.1074\n",
      "tensor(13.4672, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3314, grad_fn=<StdBackward0>)\n",
      "tensor(6.9162, grad_fn=<VarBackward0>)\n",
      "tensor(0.0253, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 490\tLoss: 12519.3154\n",
      "tensor(13.4175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3489, grad_fn=<StdBackward0>)\n",
      "tensor(6.8786, grad_fn=<VarBackward0>)\n",
      "tensor(0.0160, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 491\tLoss: 12518.5020\n",
      "tensor(13.4535, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3418, grad_fn=<StdBackward0>)\n",
      "tensor(6.9293, grad_fn=<VarBackward0>)\n",
      "tensor(0.0272, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 492\tLoss: 12519.5938\n",
      "tensor(13.4029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3382, grad_fn=<StdBackward0>)\n",
      "tensor(6.8844, grad_fn=<VarBackward0>)\n",
      "tensor(0.0126, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 493\tLoss: 12518.0420\n",
      "tensor(13.4014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3284, grad_fn=<StdBackward0>)\n",
      "tensor(6.9240, grad_fn=<VarBackward0>)\n",
      "tensor(0.0254, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 494\tLoss: 12519.2246\n",
      "tensor(13.3961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3301, grad_fn=<StdBackward0>)\n",
      "tensor(6.8917, grad_fn=<VarBackward0>)\n",
      "tensor(0.0345, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 495\tLoss: 12520.1465\n",
      "tensor(13.3953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3379, grad_fn=<StdBackward0>)\n",
      "tensor(6.9209, grad_fn=<VarBackward0>)\n",
      "tensor(0.0176, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 496\tLoss: 12518.5303\n",
      "tensor(13.4043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3370, grad_fn=<StdBackward0>)\n",
      "tensor(6.8995, grad_fn=<VarBackward0>)\n",
      "tensor(0.0218, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 497\tLoss: 12518.9551\n",
      "tensor(13.3752, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3364, grad_fn=<StdBackward0>)\n",
      "tensor(6.9086, grad_fn=<VarBackward0>)\n",
      "tensor(0.0328, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 498\tLoss: 12520.0225\n",
      "tensor(13.3835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3352, grad_fn=<StdBackward0>)\n",
      "tensor(6.9074, grad_fn=<VarBackward0>)\n",
      "tensor(0.0189, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n",
      "Epoch: 499\tLoss: 12518.6299\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'dataloader' is your PyTorch DataLoader\n",
    "Lambda1 = 0\n",
    "Lambda2 = 100\n",
    "Lambda3 = 5\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50  # You can adjust the number of epochs\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "# else torch.device(\"cpu\")\n",
    "\n",
    "# model = model.to(device)\n",
    "# m = m.to(device)\n",
    "# E = E.to(device)\n",
    "# L = L.to(device)\n",
    "# dgl_graph = dgl_graph.to(device)\n",
    "\n",
    "\n",
    "def train(x, edge_index, Lambda1, Lambda2):\n",
    "  optimizer.zero_grad()\n",
    "  features = dgl_graph.ndata[\"x\"]\n",
    "  ground_truth = dgl_graph.ndata[\"x\"]\n",
    "  h_a, h_f = model(dgl_graph, features)\n",
    "  loss = criterion(torch.squeeze(h_a)+torch.squeeze(h_f), ground_truth)  + \\\n",
    "  10*torch.std(torch.diff(torch.squeeze(h_a))) +\\\n",
    "  0 * torch.var(torch.squeeze(h_a)) +\\\n",
    "  100 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(torch.squeeze(h_f), 0, 1))[1])) +\\\n",
    "  5 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).long(), L.long()), torch.matmul(E, torch.squeeze(h_f).T).long()), 0))\n",
    "  print(criterion(torch.squeeze(h_a) + torch.squeeze(h_f), x))\n",
    "  print(torch.std(torch.diff(torch.squeeze(h_a))))\n",
    "  print(torch.var(torch.squeeze(h_a)))\n",
    "  print(torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(torch.squeeze(h_f), 0, 1))[1])))\n",
    "  print(torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).long(), L.long()), torch.matmul(E, torch.squeeze(h_f).T).long()), 0)))\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss, criterion(h_a+h_f, x), h_a, h_f\n",
    "\n",
    "epochs = range(1, 500)\n",
    "losses = []\n",
    "embeddings = []\n",
    "min_loss = 1000\n",
    "for epoch in epochs:\n",
    "  loss, loss_2, h_a, h_f= train(x, edge_index, Lambda1, Lambda2)\n",
    "  losses.append(loss)\n",
    "  print(f\"Epoch: {epoch}\\tLoss: {loss:.4f}\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     losses = []\n",
    "#     for input_nodes, output_nodes, blocks in dataloader:\n",
    "#         # Move blocks, input_features, and ground_truth to device\n",
    "#         blocks = [b.to(device) for b in blocks]\n",
    "#         input_features = blocks[0].srcdata['x'].to(device)\n",
    "#         ground_truth = blocks[-1].dstdata['x'].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         h_a, h_f = model(blocks, input_features)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(torch.squeeze(h_a) + torch.squeeze(h_f), ground_truth) + \\\n",
    "#                10 * torch.std(torch.diff(h_a)) + \\\n",
    "#                Lambda1 * torch.var(h_a) + \\\n",
    "#                Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)).float(),\n",
    "#                                                                    torch.transpose(m, 0, 1).float()),\n",
    "#                                                         torch.transpose(torch.squeeze(h_f), 0, 1).float())[1])) + \\\n",
    "#                Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).float(),\n",
    "#                                                                         L.float()),\n",
    "#                                                              torch.matmul(E, torch.squeeze(h_f).T).float()), 0))\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {np.mean(losses)}\")\n",
    "\n",
    "# print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.3835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3352, grad_fn=<StdBackward0>)\n",
      "tensor(6.9074, grad_fn=<VarBackward0>)\n",
      "tensor(0.0189, grad_fn=<SumBackward0>)\n",
      "tensor(2500)\n"
     ]
    }
   ],
   "source": [
    "  print(criterion(torch.squeeze(h_a) + torch.squeeze(h_f), x))\n",
    "  print(torch.std(torch.diff(torch.squeeze(h_a))))\n",
    "  print(torch.var(torch.squeeze(h_a)))\n",
    "  print(torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(torch.squeeze(h_f), 0, 1))[1])))\n",
    "  print(torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).long(), L.long()), torch.matmul(E, torch.squeeze(h_f).T).long()), 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 1, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))\n",
    "g = dgl.add_self_loop(g)\n",
    "feat = torch.ones(6, 10)\n",
    "gatconv1 = GATConv(10, 2, num_heads=1)\n",
    "gatconv2 = GATConv(2, 10, num_heads=1)\n",
    "res = gatconv1(g, feat)\n",
    "f = gatconv2(g, res)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "train_losses = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for input_nodes, output_nodes, blocks in dataloader:\n",
    "        # ... (rest of your code)\n",
    "\n",
    "    # Calculate the mean loss for the epoch and store it\n",
    "    epoch_loss = np.mean(losses)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Training time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jFvpkqHmDOit",
    "outputId": "2f03498e-35a1-4358-ff92-77f5d1521f70"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 100  # You can adjust the number of epochs\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "else torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "m = m.to(device)\n",
    "E = E.to(device)\n",
    "L = L.to(device)\n",
    "\n",
    "# Lists to store loss values for plotting\n",
    "train_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for input_nodes, output_nodes, blocks in dataloader:\n",
    "        # ... (rest of your code)\n",
    "                # Move blocks, input_features, and ground_truth to device\n",
    "        blocks = [b.to(device) for b in blocks]\n",
    "        input_features = blocks[0].srcdata['x'].to(device)\n",
    "        ground_truth = blocks[-1].dstdata['x'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        h_a, h_f = model(blocks, input_features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(torch.squeeze(h_a) * torch.squeeze(h_f), ground_truth) + \\\n",
    "               10 * torch.std(torch.diff(h_a)) + \\\n",
    "               Lambda1 * torch.var(h_a) + \\\n",
    "               Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)).float(),\n",
    "                                                                   torch.transpose(m, 0, 1).float()),\n",
    "                                                        torch.transpose(torch.squeeze(h_f), 0, 1).float())[1])) + \\\n",
    "               Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).float(),\n",
    "                                                                        L.float()),\n",
    "                                                             torch.matmul(E, torch.squeeze(h_f).T).float()), 0))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Append the current batch loss to the losses list\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Calculate the mean loss for the epoch and store it\n",
    "    epoch_loss = np.mean(losses)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Training time: {elapsed_time} seconds\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of available GPUs:\", num_gpus)\n",
    "else:\n",
    "    print(\"CUDA is not available. Make sure you have installed the correct GPU version of PyTorch.\")\n",
    "\n",
    "\n",
    "dgl_graph.create_formats_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rxf131/ondemand/ubuntu2204/python310/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -32.5790, -179.3676,  -90.9743,  ...,   -3.0696,   42.0174,\n",
      "             21.1875]]],\n",
      "\n",
      "\n",
      "        [[[ -30.8580, -177.6286,  -90.0213,  ...,   -3.1443,   43.7388,\n",
      "             20.7994]]],\n",
      "\n",
      "\n",
      "        [[[ -32.5045, -189.1380,  -91.3025,  ...,   -6.8868,   28.2495,\n",
      "             20.4220]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -32.5006, -189.1476,  -91.3071,  ...,   -6.8879,   28.2568,\n",
      "             20.4181]]],\n",
      "\n",
      "\n",
      "        [[[ -32.5012, -189.1460,  -91.3064,  ...,   -6.8877,   28.2556,\n",
      "             20.4187]]],\n",
      "\n",
      "\n",
      "        [[[ -30.8582, -177.6274,  -90.0215,  ...,   -3.1438,   43.7394,\n",
      "             20.7995]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[  -7.8046, -114.5195,   -8.4159,  ...,  -50.8800,    4.0630,\n",
      "             63.9066]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8046, -114.5197,   -8.4156,  ...,  -50.8803,    4.0623,\n",
      "             63.9067]]],\n",
      "\n",
      "\n",
      "        [[[ -18.8121, -139.5337,  -14.4212,  ...,  -67.9004,   -4.4609,\n",
      "             79.1707]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  -7.8167, -114.5293,   -8.4181,  ...,  -50.8842,    4.0464,\n",
      "             63.9124]]],\n",
      "\n",
      "\n",
      "        [[[ -18.8178, -139.5213,  -14.4161,  ...,  -67.8984,   -4.4724,\n",
      "             79.1754]]],\n",
      "\n",
      "\n",
      "        [[[ -18.8109, -139.5361,  -14.4222,  ...,  -67.9009,   -4.4586,\n",
      "             79.1698]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 12.1574, -88.1677,  15.1492,  ..., -30.2721, -80.4055, 140.6397]]],\n",
      "\n",
      "\n",
      "        [[[ 19.7078, -68.8477,  18.0717,  ..., -17.6104, -63.3312, 118.4436]]],\n",
      "\n",
      "\n",
      "        [[[ 19.7076, -68.8472,  18.0718,  ..., -17.6103, -63.3315, 118.4437]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 19.3393, -69.0514,  17.9764,  ..., -17.6323, -63.9217, 118.7260]]],\n",
      "\n",
      "\n",
      "        [[[  4.8838, -77.6630,  14.9273,  ..., -19.0687, -88.8868, 130.3767]]],\n",
      "\n",
      "\n",
      "        [[[  4.8850, -77.6658,  14.9264,  ..., -19.0691, -88.8845, 130.3758]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -13.7136,  -31.5072,   24.4131,  ...,   17.5615,  -99.9921,\n",
      "            150.5193]]],\n",
      "\n",
      "\n",
      "        [[[ -13.7138,  -31.5057,   24.4120,  ...,   17.5624,  -99.9890,\n",
      "            150.5181]]],\n",
      "\n",
      "\n",
      "        [[[ -13.9528,  -36.6269,   26.9432,  ...,   14.9611, -121.3674,\n",
      "            155.9745]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -13.7052,  -31.5044,   24.4141,  ...,   17.5610,  -99.9774,\n",
      "            150.5120]]],\n",
      "\n",
      "\n",
      "        [[[ -13.9524,  -36.6277,   26.9430,  ...,   14.9610, -121.3668,\n",
      "            155.9743]]],\n",
      "\n",
      "\n",
      "        [[[ -13.7134,  -31.5074,   24.4125,  ...,   17.5617,  -99.9907,\n",
      "            150.5188]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 1, Loss: 37426103.875\n",
      "tensor([[[[ -55.1704,    0.2674,   10.3146,  ...,   55.8029, -103.7324,\n",
      "            146.5716]]],\n",
      "\n",
      "\n",
      "        [[[ -76.2685,   -3.4186,    5.6646,  ...,   60.5939, -133.2972,\n",
      "            161.0448]]],\n",
      "\n",
      "\n",
      "        [[[ -55.1704,    0.2674,   10.3146,  ...,   55.8029, -103.7324,\n",
      "            146.5716]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -55.1701,    0.2668,   10.3145,  ...,   55.8028, -103.7319,\n",
      "            146.5715]]],\n",
      "\n",
      "\n",
      "        [[[ -55.1793,    0.2829,   10.3110,  ...,   55.8084, -103.7310,\n",
      "            146.5707]]],\n",
      "\n",
      "\n",
      "        [[[ -55.1703,    0.2674,   10.3147,  ...,   55.8029, -103.7323,\n",
      "            146.5716]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -95.3515,   20.8001,   -3.5328,  ...,   91.3439, -108.4756,\n",
      "            136.0499]]],\n",
      "\n",
      "\n",
      "        [[[ -95.3531,   20.8034,   -3.5320,  ...,   91.3444, -108.4786,\n",
      "            136.0507]]],\n",
      "\n",
      "\n",
      "        [[[ -78.5871,   24.5999,    1.9194,  ...,   91.6636, -107.6699,\n",
      "            117.5766]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -74.9845,   26.6785,    0.8835,  ...,   90.2422,  -86.2565,\n",
      "            113.6411]]],\n",
      "\n",
      "\n",
      "        [[[ -74.9812,   26.6787,    0.8843,  ...,   90.2410,  -86.2524,\n",
      "            113.6394]]],\n",
      "\n",
      "\n",
      "        [[[ -74.9802,   26.6780,    0.8843,  ...,   90.2407,  -86.2510,\n",
      "            113.6389]]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 38.09it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-70.5393,  45.6838,  14.9451,  ..., 118.4556, -82.4690,  66.4810]]],\n",
      "\n",
      "\n",
      "        [[[-67.1827,  46.6613,  13.4075,  ..., 115.4219, -62.3244,  65.0866]]],\n",
      "\n",
      "\n",
      "        [[[-67.1826,  46.6612,  13.4075,  ..., 115.4219, -62.3243,  65.0866]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-67.1826,  46.6613,  13.4075,  ..., 115.4219, -62.3243,  65.0866]]],\n",
      "\n",
      "\n",
      "        [[[-67.1827,  46.6613,  13.4075,  ..., 115.4219, -62.3244,  65.0866]]],\n",
      "\n",
      "\n",
      "        [[[-89.7910,  47.1971,   8.6471,  ..., 126.0947, -88.6425,  72.4640]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-56.8661,  53.9048,  40.2695,  ..., 135.1146, -63.5645,  21.0893]]],\n",
      "\n",
      "\n",
      "        [[[-40.7706,  55.9235,  39.4697,  ..., 128.6955, -46.4366,  11.8905]]],\n",
      "\n",
      "\n",
      "        [[[-40.7712,  55.9238,  39.4697,  ..., 128.6959, -46.4375,  11.8906]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-40.7712,  55.9238,  39.4697,  ..., 128.6959, -46.4375,  11.8906]]],\n",
      "\n",
      "\n",
      "        [[[-42.9424,  55.5081,  42.3046,  ..., 132.7780, -65.7513,  10.4847]]],\n",
      "\n",
      "\n",
      "        [[[-42.9415,  55.5066,  42.3043,  ..., 132.7776, -65.7501,  10.4842]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 2, Loss: 12964990.5\n",
      "tensor([[[[-13.8833,  53.4307,  75.1866,  ..., 132.6516, -64.4544, -43.1513]]],\n",
      "\n",
      "\n",
      "        [[[-25.5610,  51.6546,  75.7113,  ..., 134.8136, -62.2115, -36.4684]]],\n",
      "\n",
      "\n",
      "        [[[-12.9806,  53.8767,  70.7286,  ..., 128.2312, -45.1806, -39.0225]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-25.5616,  51.6556,  75.7115,  ..., 134.8139, -62.2124, -36.4681]]],\n",
      "\n",
      "\n",
      "        [[[-12.9808,  53.8767,  70.7287,  ..., 128.2313, -45.1808, -39.0225]]],\n",
      "\n",
      "\n",
      "        [[[-25.5622,  51.6564,  75.7116,  ..., 134.8141, -62.2132, -36.4679]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-14.8178,  39.4022, 107.8021,  ..., 121.4626, -75.2164, -83.2376]]],\n",
      "\n",
      "\n",
      "        [[[ -3.4337,  42.9564,  99.0302,  ..., 116.1871, -56.6488, -80.3545]]],\n",
      "\n",
      "\n",
      "        [[[-21.2373,  43.5522, 101.8323,  ..., 128.3378, -82.8053, -85.9007]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -3.4341,  42.9569,  99.0303,  ..., 116.1873, -56.6494, -80.3544]]],\n",
      "\n",
      "\n",
      "        [[[-21.2371,  43.5519, 101.8323,  ..., 128.3377, -82.8050, -85.9008]]],\n",
      "\n",
      "\n",
      "        [[[ -3.9930,  42.0125, 104.9857,  ..., 120.3100, -76.5338, -86.7702]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -30.1622,   25.3623,  119.2425,  ...,  109.7266,  -97.6631,\n",
      "           -113.7854]]],\n",
      "\n",
      "\n",
      "        [[[ -30.1633,   25.3637,  119.2427,  ...,  109.7273,  -97.6644,\n",
      "           -113.7851]]],\n",
      "\n",
      "\n",
      "        [[[ -12.5023,   26.0457,  114.8132,  ...,   99.2468,  -71.1236,\n",
      "           -106.0781]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -12.5024,   26.0458,  114.8132,  ...,   99.2468,  -71.1237,\n",
      "           -106.0781]]],\n",
      "\n",
      "\n",
      "        [[[ -12.5029,   26.0463,  114.8134,  ...,   99.2471,  -71.1246,\n",
      "           -106.0781]]],\n",
      "\n",
      "\n",
      "        [[[ -30.1633,   25.3637,  119.2427,  ...,  109.7273,  -97.6645,\n",
      "           -113.7851]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -44.4760,    1.3253,  123.5456,  ...,   83.1561,  -97.1604,\n",
      "           -122.2542]]],\n",
      "\n",
      "\n",
      "        [[[ -31.8350,    6.5006,  119.4327,  ...,   84.6137,  -96.8609,\n",
      "           -123.2992]]],\n",
      "\n",
      "\n",
      "        [[[ -29.9475,    9.0650,  112.7885,  ...,   81.8217,  -76.4156,\n",
      "           -114.6547]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -29.8555,    9.0727,  112.6291,  ...,   81.7678,  -76.0646,\n",
      "           -114.5791]]],\n",
      "\n",
      "\n",
      "        [[[ -29.8459,    9.0729,  112.6068,  ...,   81.7618,  -76.0190,\n",
      "           -114.5701]]],\n",
      "\n",
      "\n",
      "        [[[ -31.8350,    6.5006,  119.4327,  ...,   84.6137,  -96.8610,\n",
      "           -123.2992]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 3, Loss: 7883651.8125\n",
      "tensor([[[[ -38.5337,   -0.6576,   97.5451,  ...,   64.7110,  -67.2847,\n",
      "           -109.3237]]],\n",
      "\n",
      "\n",
      "        [[[ -38.5337,   -0.6576,   97.5451,  ...,   64.7110,  -67.2847,\n",
      "           -109.3237]]],\n",
      "\n",
      "\n",
      "        [[[ -54.2284,   -9.6165,  106.8313,  ...,   64.1897,  -87.3792,\n",
      "           -116.5975]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -40.9915,   -3.6950,  103.7570,  ...,   66.8872,  -87.6723,\n",
      "           -118.1335]]],\n",
      "\n",
      "\n",
      "        [[[ -38.5337,   -0.6576,   97.5450,  ...,   64.7110,  -67.2847,\n",
      "           -109.3237]]],\n",
      "\n",
      "\n",
      "        [[[ -40.9931,   -3.6927,  103.7574,  ...,   66.8881,  -87.6739,\n",
      "           -118.1332]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -29.6114,   -0.4985,   78.9386,  ...,   47.2790,  -46.3919,\n",
      "            -96.4207]]],\n",
      "\n",
      "\n",
      "        [[[ -29.6110,   -0.4989,   78.9381,  ...,   47.2788,  -46.3909,\n",
      "            -96.4206]]],\n",
      "\n",
      "\n",
      "        [[[ -29.6121,   -0.4974,   78.9385,  ...,   47.2795,  -46.3922,\n",
      "            -96.4206]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -29.6192,   -0.5031,   78.9852,  ...,   47.2816,  -46.4771,\n",
      "            -96.4365]]],\n",
      "\n",
      "\n",
      "        [[[ -44.2428,   -9.4523,   86.1187,  ...,   44.7824,  -63.9365,\n",
      "           -102.3182]]],\n",
      "\n",
      "\n",
      "        [[[ -29.6120,   -0.4986,   78.9414,  ...,   47.2792,  -46.3972,\n",
      "            -96.4217]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-18.1333,  -0.8864,  70.9990,  ...,  26.7517, -34.4800, -85.4884]]],\n",
      "\n",
      "\n",
      "        [[[-18.1358,  -0.8830,  71.0000,  ...,  26.7534, -34.4832, -85.4884]]],\n",
      "\n",
      "\n",
      "        [[[-18.1335,  -0.8861,  70.9991,  ...,  26.7518, -34.4803, -85.4884]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -7.4499,   4.4994,  70.2068,  ...,  31.8991, -38.2431, -89.3445]]],\n",
      "\n",
      "\n",
      "        [[[ -6.2528,   7.1450,  65.3630,  ...,  31.0945, -20.1134, -81.2635]]],\n",
      "\n",
      "\n",
      "        [[[ -6.2528,   7.1451,  65.3627,  ...,  31.0944, -20.1127, -81.2634]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 45.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 39.11it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 24.3332,  19.8282,  61.2233,  ...,  19.5864,   1.3738, -68.6482]]],\n",
      "\n",
      "\n",
      "        [[[ 16.0714,  13.3027,  66.3862,  ...,  13.9144, -10.4076, -71.4982]]],\n",
      "\n",
      "\n",
      "        [[[ 24.3337,  19.8276,  61.2231,  ...,  19.5860,   1.3744, -68.6482]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 16.0709,  13.3033,  66.3864,  ...,  13.9147, -10.4081, -71.4982]]],\n",
      "\n",
      "\n",
      "        [[[ 24.3303,  19.8269,  61.3618,  ...,  19.5847,   1.1310, -68.6875]]],\n",
      "\n",
      "\n",
      "        [[[ 24.3337,  19.8276,  61.2231,  ...,  19.5861,   1.3743, -68.6482]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4, Loss: 3997066.1875\n",
      "tensor([[[[ 52.9857,  35.2099,  64.1752,  ...,  16.5329,  11.3516, -60.2160]]],\n",
      "\n",
      "\n",
      "        [[[ 52.9888,  35.2125,  64.2274,  ...,  16.5323,  11.2623, -60.2298]]],\n",
      "\n",
      "\n",
      "        [[[ 52.9857,  35.2099,  64.1752,  ...,  16.5329,  11.3516, -60.2161]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 52.9862,  35.2091,  64.1732,  ...,  16.5326,  11.3552, -60.2155]]],\n",
      "\n",
      "\n",
      "        [[[ 52.9859,  35.2094,  64.1733,  ...,  16.5328,  11.3549, -60.2155]]],\n",
      "\n",
      "\n",
      "        [[[ 54.0291,  33.8207,  68.8759,  ...,  16.7481,  -5.3121, -67.7848]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 72.2761,  51.5232,  75.4891,  ...,  23.1639,  -1.3386, -56.9603]]],\n",
      "\n",
      "\n",
      "        [[[ 70.5902,  46.1863,  75.0482,  ...,  16.7629,  -0.6177, -56.7347]]],\n",
      "\n",
      "\n",
      "        [[[ 72.2761,  51.5232,  75.4891,  ...,  23.1639,  -1.3386, -56.9603]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 73.4024,  50.1507,  74.4148,  ...,  23.5352,  -8.0286, -62.9811]]],\n",
      "\n",
      "\n",
      "        [[[ 70.5851,  46.1919,  75.0499,  ...,  16.7662,  -0.6232, -56.7353]]],\n",
      "\n",
      "\n",
      "        [[[ 71.7079,  50.8563,  69.5647,  ...,  23.0244,   8.7382, -55.4447]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 79.5162,  66.9421,  78.7188,  ...,  35.1687, -10.4798, -56.7625]]],\n",
      "\n",
      "\n",
      "        [[[ 79.5162,  66.9421,  78.7188,  ...,  35.1687, -10.4798, -56.7625]]],\n",
      "\n",
      "\n",
      "        [[[ 79.0240,  65.8560,  72.8040,  ...,  34.6660,  -0.2145, -55.1731]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 81.0196,  65.6527,  77.5757,  ...,  35.6280, -17.1284, -62.8279]]],\n",
      "\n",
      "\n",
      "        [[[ 79.0241,  65.8559,  72.8038,  ...,  34.6659,  -0.2142, -55.1730]]],\n",
      "\n",
      "\n",
      "        [[[ 79.0270,  65.8651,  72.8495,  ...,  34.6703,  -0.2936, -55.1854]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 79.2491,  75.3532,  77.2039,  ...,  38.3035, -15.3529, -60.4368]]],\n",
      "\n",
      "\n",
      "        [[[ 79.5940,  76.8512,  71.8035,  ...,  42.0753,  -3.9399, -58.7309]]],\n",
      "\n",
      "\n",
      "        [[[ 79.5842,  76.8583,  71.7956,  ...,  42.0797,  -3.9290, -58.7295]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 79.5893,  76.8539,  71.7973,  ...,  42.0770,  -3.9303, -58.7295]]],\n",
      "\n",
      "\n",
      "        [[[ 79.2424,  75.3602,  77.2062,  ...,  38.3076, -15.3596, -60.4379]]],\n",
      "\n",
      "\n",
      "        [[[ 79.5939,  76.8514,  71.8036,  ...,  42.0754,  -3.9400, -58.7310]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 5, Loss: 1530263.0859375\n",
      "tensor([[[[ 79.6075,  83.8771,  69.2144,  ...,  38.2669,   4.4466, -66.4021]]],\n",
      "\n",
      "\n",
      "        [[[ 79.8969,  85.3462,  75.0799,  ...,  38.8795,  -5.9072, -68.4412]]],\n",
      "\n",
      "\n",
      "        [[[ 81.3180,  84.6083,  73.7652,  ...,  39.4861, -12.5854, -74.8764]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 79.4389,  83.1637,  74.2765,  ...,  34.0507,  -6.4854, -69.0748]]],\n",
      "\n",
      "\n",
      "        [[[ 79.4555,  83.1463,  74.2705,  ...,  34.0403,  -6.4685, -69.0717]]],\n",
      "\n",
      "\n",
      "        [[[ 79.8969,  85.3462,  75.0799,  ...,  38.8795,  -5.9072, -68.4412]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 82.4583,  87.4397,  67.9163,  ...,  23.1142,  25.4413, -76.4308]]],\n",
      "\n",
      "\n",
      "        [[[ 81.1687,  88.7957,  73.1659,  ...,  18.1041,  15.2747, -80.5984]]],\n",
      "\n",
      "\n",
      "        [[[ 81.1663,  88.7984,  73.1668,  ...,  18.1057,  15.2722, -80.5989]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 84.2401,  88.2909,  72.2259,  ...,  23.6353,   9.4755, -85.4397]]],\n",
      "\n",
      "\n",
      "        [[[ 82.4552,  87.4185,  67.8363,  ...,  23.1108,  25.5772, -76.3984]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2440,  88.2869,  72.2246,  ...,  23.6329,   9.4788, -85.4391]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 86.2316,  90.2673,  74.9372,  ...,   5.0450,  35.3844, -94.2999]]],\n",
      "\n",
      "\n",
      "        [[[ 84.3834,  89.3281,  70.5247,  ...,   5.3380,  50.3978, -84.7729]]],\n",
      "\n",
      "\n",
      "        [[[ 84.3834,  89.3281,  70.5247,  ...,   5.3380,  50.3978, -84.7729]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 84.3834,  89.3279,  70.5241,  ...,   5.3381,  50.3987, -84.7726]]],\n",
      "\n",
      "\n",
      "        [[[ 84.3827,  89.3283,  70.5235,  ...,   5.3386,  50.3993, -84.7724]]],\n",
      "\n",
      "\n",
      "        [[[ 83.5120,  90.9381,  76.0323,  ...,  -1.7973,  42.8037, -90.0171]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 80.1722,  91.3689,  76.7244,  ...,  -4.1939,  68.0338, -87.3845]]],\n",
      "\n",
      "\n",
      "        [[[ 81.8165,  92.3953,  81.3130,  ...,  -4.9315,  53.7910, -97.1269]]],\n",
      "\n",
      "\n",
      "        [[[ 81.8183,  92.3934,  81.3124,  ...,  -4.9327,  53.7926, -97.1265]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 80.1733,  91.3678,  76.7240,  ...,  -4.1947,  68.0350, -87.3842]]],\n",
      "\n",
      "\n",
      "        [[[ 78.9580,  93.2564,  82.8260,  ..., -12.4620,  62.2684, -92.9849]]],\n",
      "\n",
      "\n",
      "        [[[ 80.1733,  91.3678,  76.7240,  ...,  -4.1947,  68.0350, -87.3842]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 6, Loss: 1912498.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 39.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 44.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 39.10it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 68.9090,  94.8960,  88.7721,  ...,  -0.7281,  58.5521, -91.6342]]],\n",
      "\n",
      "\n",
      "        [[[ 68.9129,  94.8918,  88.7708,  ...,  -0.7308,  58.5558, -91.6334]]],\n",
      "\n",
      "\n",
      "        [[[ 65.2459,  95.9963,  90.7782,  ...,  -7.9061,  67.0678, -87.0497]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 67.8102,  93.7590,  83.9699,  ...,  -0.1314,  72.5904, -82.0303]]],\n",
      "\n",
      "\n",
      "        [[[ 67.8102,  93.7590,  83.9699,  ...,  -0.1314,  72.5904, -82.0303]]],\n",
      "\n",
      "\n",
      "        [[[ 68.9094,  94.8955,  88.7720,  ...,  -0.7283,  58.5525, -91.6341]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 50.3103,  96.5688,  89.5106,  ...,  13.9939,  66.4192, -68.6662]]],\n",
      "\n",
      "\n",
      "        [[[ 50.6372,  97.8381,  94.4468,  ...,  13.9929,  52.1055, -77.7561]]],\n",
      "\n",
      "\n",
      "        [[[ 45.7294,  99.2385,  96.8226,  ...,   7.9560,  59.8119, -72.1373]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 50.3116,  96.5673,  89.5101,  ...,  13.9929,  66.4207, -68.6657]]],\n",
      "\n",
      "\n",
      "        [[[ 50.3118,  96.5671,  89.5100,  ...,  13.9928,  66.4209, -68.6657]]],\n",
      "\n",
      "\n",
      "        [[[ 50.3113,  96.5677,  89.5103,  ...,  13.9932,  66.4202, -68.6659]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 32.7866,  99.1325,  97.8764,  ...,  29.2136,  49.5765, -51.5749]]],\n",
      "\n",
      "\n",
      "        [[[ 34.2043,  97.3629,  91.5621,  ...,  28.8881,  58.5572, -49.5155]]],\n",
      "\n",
      "\n",
      "        [[[ 34.2031,  97.3643,  91.5675,  ...,  28.8883,  58.5495, -49.5172]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 27.7577, 100.2358,  98.9532,  ...,  24.6679,  50.6785, -50.7549]]],\n",
      "\n",
      "\n",
      "        [[[ 27.7597, 100.2335,  98.9524,  ...,  24.6665,  50.6807, -50.7543]]],\n",
      "\n",
      "\n",
      "        [[[ 34.2045,  97.3627,  91.5621,  ...,  28.8880,  58.5573, -49.5154]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 25.6489,  94.7550,  90.9696,  ...,  35.7446,  57.5874, -29.4800]]],\n",
      "\n",
      "\n",
      "        [[[ 25.6501,  94.7536,  90.9668,  ...,  35.7440,  57.5917, -29.4791]]],\n",
      "\n",
      "\n",
      "        [[[ 25.6490,  94.7548,  90.9672,  ...,  35.7448,  57.5905, -29.4796]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 25.6503,  94.7534,  90.9667,  ...,  35.7439,  57.5919, -29.4790]]],\n",
      "\n",
      "\n",
      "        [[[ 24.8694,  95.9794,  95.7429,  ...,  36.6444,  42.8819, -36.9874]]],\n",
      "\n",
      "\n",
      "        [[[ 24.8672,  95.9820,  95.7437,  ...,  36.6459,  42.8798, -36.9883]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 7, Loss: 955547.3515625\n",
      "tensor([[[[ 26.9076,  89.9038,  90.1503,  ...,  31.7730,  66.3398, -14.0078]]],\n",
      "\n",
      "\n",
      "        [[[ 26.9064,  89.9052,  90.1508,  ...,  31.7739,  66.3385, -14.0084]]],\n",
      "\n",
      "\n",
      "        [[[ 25.2158,  91.5095,  96.4343,  ...,  32.1526,  57.6106, -15.3457]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 26.1648,  90.9313,  94.7747,  ...,  32.4565,  52.0313, -20.9117]]],\n",
      "\n",
      "\n",
      "        [[[ 26.9074,  89.9041,  90.1504,  ...,  31.7732,  66.3395, -14.0080]]],\n",
      "\n",
      "\n",
      "        [[[ 26.1640,  90.9323,  94.7749,  ...,  32.4571,  52.0305, -20.9121]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 32.2753,  86.5235,  98.0747,  ...,  22.7958,  70.8815,  -7.6102]]],\n",
      "\n",
      "\n",
      "        [[[ 27.6657,  86.6543,  98.6507,  ...,  17.5156,  73.2999,  -2.7862]]],\n",
      "\n",
      "\n",
      "        [[[ 33.3759,  85.8484,  96.3280,  ...,  22.8694,  65.5695, -13.0535]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 32.2753,  86.5235,  98.0747,  ...,  22.7958,  70.8815,  -7.6102]]],\n",
      "\n",
      "\n",
      "        [[[ 33.8438,  85.0197,  91.7518,  ...,  22.6326,  79.2829,  -6.4053]]],\n",
      "\n",
      "\n",
      "        [[[ 33.8428,  85.0209,  91.7522,  ...,  22.6334,  79.2818,  -6.4058]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 40.4606,  82.7558, 100.6385,  ...,  16.5759,  75.2262, -13.0642]]],\n",
      "\n",
      "\n",
      "        [[[ 35.4041,  83.3831, 103.2446,  ...,  10.7333,  83.5088,  -2.8588]]],\n",
      "\n",
      "\n",
      "        [[[ 40.6727,  82.0372,  95.9940,  ...,  16.6457,  88.5016,  -6.3397]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 40.6733,  82.0364,  95.9937,  ...,  16.6452,  88.5023,  -6.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 40.4619,  82.7543, 100.6381,  ...,  16.5751,  75.2275, -13.0637]]],\n",
      "\n",
      "\n",
      "        [[[ 35.4045,  83.3827, 103.2445,  ...,  10.7330,  83.5092,  -2.8586]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 44.2107,  83.1049, 102.1826,  ...,  18.6613,  88.9463, -11.7165]]],\n",
      "\n",
      "\n",
      "        [[[ 44.2094,  83.1065, 102.1832,  ...,  18.6623,  88.9446, -11.7172]]],\n",
      "\n",
      "\n",
      "        [[[ 44.2108,  83.1048, 102.1827,  ...,  18.6612,  88.9463, -11.7165]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 44.1000,  83.8972, 106.9883,  ...,  18.6463,  75.7222, -18.7589]]],\n",
      "\n",
      "\n",
      "        [[[ 44.2108,  83.1048, 102.1827,  ...,  18.6612,  88.9462, -11.7165]]],\n",
      "\n",
      "\n",
      "        [[[ 44.1021,  83.8946, 106.9876,  ...,  18.6448,  75.7244, -18.7580]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 8, Loss: 605526.2421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 44.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 36.52it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 44.7350,  85.7449, 106.9296,  ...,  26.6996,  83.1150, -16.6139]]],\n",
      "\n",
      "\n",
      "        [[[ 40.0284,  87.7351, 115.2609,  ...,  21.9388,  77.1854, -14.7159]]],\n",
      "\n",
      "\n",
      "        [[[ 44.7353,  85.7446, 106.9294,  ...,  26.6994,  83.1153, -16.6138]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 44.5942,  86.6780, 111.8378,  ...,  27.0039,  69.6743, -23.9463]]],\n",
      "\n",
      "\n",
      "        [[[ 44.5938,  86.6784, 111.8379,  ...,  27.0042,  69.6740, -23.9465]]],\n",
      "\n",
      "\n",
      "        [[[ 40.0283,  87.7352, 115.2609,  ...,  21.9389,  77.1853, -14.7160]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 45.2694,  86.5471, 107.4071,  ...,  33.8461,  77.6135, -16.7845]]],\n",
      "\n",
      "\n",
      "        [[[ 40.6462,  88.7293, 115.7079,  ...,  29.9160,  70.9065, -15.0683]]],\n",
      "\n",
      "\n",
      "        [[[ 45.2695,  86.5470, 107.4065,  ...,  33.8461,  77.6142, -16.7844]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 45.2697,  86.5468, 107.4065,  ...,  33.8460,  77.6143, -16.7844]]],\n",
      "\n",
      "\n",
      "        [[[ 40.6438,  88.7322, 115.7089,  ...,  29.9178,  70.9038, -15.0694]]],\n",
      "\n",
      "\n",
      "        [[[ 45.2697,  86.5468, 107.4065,  ...,  33.8460,  77.6143, -16.7844]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 46.3447,  85.3515, 109.5818,  ...,  35.8745,  69.1607, -12.5037]]],\n",
      "\n",
      "\n",
      "        [[[ 47.4774,  84.7211, 107.5820,  ...,  36.1041,  63.8035, -18.2754]]],\n",
      "\n",
      "\n",
      "        [[[ 46.3447,  85.3515, 109.5818,  ...,  35.8745,  69.1607, -12.5037]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 47.4744,  84.7247, 107.5831,  ...,  36.1062,  63.8005, -18.2767]]],\n",
      "\n",
      "\n",
      "        [[[ 46.3447,  85.3515, 109.5818,  ...,  35.8745,  69.1607, -12.5037]]],\n",
      "\n",
      "\n",
      "        [[[ 47.4768,  84.7219, 107.5822,  ...,  36.1046,  63.8029, -18.2757]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 49.4633,  79.3183, 101.8122,  ...,  32.0889,  74.4928,  -2.9263]]],\n",
      "\n",
      "\n",
      "        [[[ 50.6159,  77.8973,  95.4540,  ...,  31.7970,  82.5830,  -1.6739]]],\n",
      "\n",
      "\n",
      "        [[[ 50.6159,  77.8973,  95.4540,  ...,  31.7970,  82.5830,  -1.6739]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 50.6159,  77.8973,  95.4540,  ...,  31.7970,  82.5830,  -1.6739]]],\n",
      "\n",
      "\n",
      "        [[[ 50.6158,  77.8975,  95.4541,  ...,  31.7971,  82.5828,  -1.6740]]],\n",
      "\n",
      "\n",
      "        [[[ 50.6159,  77.8973,  95.4540,  ...,  31.7970,  82.5830,  -1.6739]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 9, Loss: 440390.1875\n",
      "tensor([[[[46.5527, 72.1638, 95.1800,  ..., 23.9459, 82.1931, 11.2341]]],\n",
      "\n",
      "\n",
      "        [[[46.5545, 72.1615, 95.1792,  ..., 23.9445, 82.1952, 11.2350]]],\n",
      "\n",
      "\n",
      "        [[[50.4685, 71.5659, 89.2462,  ..., 28.6655, 87.8089,  7.0312]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[50.4185, 71.9721, 93.0105,  ..., 28.8914, 74.8026,  0.5196]]],\n",
      "\n",
      "\n",
      "        [[[50.4200, 71.9703, 93.0099,  ..., 28.8903, 74.8040,  0.5204]]],\n",
      "\n",
      "\n",
      "        [[[49.3415, 72.8645, 95.4360,  ..., 28.8695, 79.8717,  5.9611]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.6323, 68.5275, 92.7191,  ..., 30.3343, 80.6545, 11.6398]]],\n",
      "\n",
      "\n",
      "        [[[44.6323, 68.5275, 92.7191,  ..., 30.3343, 80.6545, 11.6398]]],\n",
      "\n",
      "\n",
      "        [[[45.8288, 67.3021, 86.6198,  ..., 30.1142, 88.5417, 12.5961]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.3610, 67.4648, 92.2144,  ..., 25.5352, 82.9845, 17.3426]]],\n",
      "\n",
      "\n",
      "        [[[45.8294, 67.3014, 86.6195,  ..., 30.1137, 88.5424, 12.5964]]],\n",
      "\n",
      "\n",
      "        [[[44.6323, 68.5275, 92.7191,  ..., 30.3343, 80.6545, 11.6398]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[34.6327, 66.2069, 91.9647,  ..., 30.6233, 78.6025, 20.6000]]],\n",
      "\n",
      "\n",
      "        [[[39.8208, 66.1006, 86.4055,  ..., 34.6721, 84.6660, 15.6194]]],\n",
      "\n",
      "\n",
      "        [[[39.8209, 66.1004, 86.4055,  ..., 34.6720, 84.6662, 15.6195]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[34.6323, 66.2074, 91.9649,  ..., 30.6236, 78.6020, 20.5998]]],\n",
      "\n",
      "\n",
      "        [[[34.6323, 66.2075, 91.9650,  ..., 30.6236, 78.6020, 20.5997]]],\n",
      "\n",
      "\n",
      "        [[[39.8206, 66.1008, 86.4053,  ..., 34.6722, 84.6662, 15.6193]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 31.08it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 34.11it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[36.5082, 67.7522, 86.5721,  ..., 38.2721, 77.4147, 18.6727]]],\n",
      "\n",
      "\n",
      "        [[[30.9410, 68.1501, 92.1448,  ..., 34.6355, 70.4376, 23.8973]]],\n",
      "\n",
      "\n",
      "        [[[35.7913, 68.0826, 89.9889,  ..., 38.8013, 64.1817, 12.5492]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[36.5089, 67.7514, 86.5718,  ..., 38.2716, 77.4154, 18.6731]]],\n",
      "\n",
      "\n",
      "        [[[36.5078, 67.7527, 86.5723,  ..., 38.2724, 77.4142, 18.6724]]],\n",
      "\n",
      "\n",
      "        [[[35.2091, 69.0577, 92.6124,  ..., 38.6408, 69.3249, 17.8294]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 10, Loss: 286487.54296875\n",
      "tensor([[[[38.1349, 71.2366, 85.3490,  ..., 37.9424, 68.8922, 23.4464]]],\n",
      "\n",
      "\n",
      "        [[[37.4870, 71.7603, 88.6391,  ..., 38.4164, 55.3611, 17.4950]]],\n",
      "\n",
      "\n",
      "        [[[32.8282, 72.1215, 90.7535,  ..., 34.2182, 60.8780, 29.1454]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[32.8263, 72.1238, 90.7544,  ..., 34.2196, 60.8759, 29.1443]]],\n",
      "\n",
      "\n",
      "        [[[32.8278, 72.1220, 90.7537,  ..., 34.2185, 60.8776, 29.1452]]],\n",
      "\n",
      "\n",
      "        [[[37.4873, 71.7599, 88.6390,  ..., 38.4162, 55.3614, 17.4952]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.6366, 77.1960, 88.5751,  ..., 34.8139, 52.3403, 28.5772]]],\n",
      "\n",
      "\n",
      "        [[[43.6114, 75.6153, 82.6574,  ..., 34.5942, 60.7654, 29.2144]]],\n",
      "\n",
      "\n",
      "        [[[43.6115, 75.6151, 82.6568,  ..., 34.5941, 60.7663, 29.2145]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.2145, 76.3754, 85.7544,  ..., 34.8737, 46.9544, 23.4832]]],\n",
      "\n",
      "\n",
      "        [[[42.6366, 77.1960, 88.5751,  ..., 34.8139, 52.3403, 28.5772]]],\n",
      "\n",
      "\n",
      "        [[[43.6115, 75.6152, 82.6574,  ..., 34.5941, 60.7654, 29.2145]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[48.7707, 80.3566, 83.0774,  ..., 31.6894, 40.0944, 27.9130]]],\n",
      "\n",
      "\n",
      "        [[[48.9136, 79.3932, 80.1665,  ..., 31.5916, 54.1126, 33.4861]]],\n",
      "\n",
      "\n",
      "        [[[48.9148, 79.3917, 80.1660,  ..., 31.5907, 54.1138, 33.4868]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[45.0252, 81.4591, 84.9123,  ..., 27.0257, 44.2468, 40.1862]]],\n",
      "\n",
      "\n",
      "        [[[48.9152, 79.3913, 80.1658,  ..., 31.5904, 54.1142, 33.4870]]],\n",
      "\n",
      "\n",
      "        [[[48.7704, 80.3570, 83.0775,  ..., 31.6897, 40.0941, 27.9128]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[51.6638, 83.0925, 82.9822,  ..., 30.9146, 36.5623, 27.8797]]],\n",
      "\n",
      "\n",
      "        [[[51.6657, 81.9730, 80.1463,  ..., 30.8908, 50.6494, 33.4845]]],\n",
      "\n",
      "\n",
      "        [[[51.6660, 81.9726, 80.1461,  ..., 30.8906, 50.6497, 33.4847]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[51.0371, 83.7933, 86.0085,  ..., 30.9587, 42.0745, 32.9025]]],\n",
      "\n",
      "\n",
      "        [[[51.6633, 83.0931, 82.9824,  ..., 30.9149, 36.5618, 27.8794]]],\n",
      "\n",
      "\n",
      "        [[[48.1726, 84.4403, 84.8587,  ..., 26.2046, 40.3313, 40.0862]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 11, Loss: 124045.408203125\n",
      "tensor([[[[51.2420, 85.5758, 89.5754,  ..., 31.5961, 43.0553, 28.7214]]],\n",
      "\n",
      "\n",
      "        [[[48.3818, 86.4461, 88.7234,  ..., 26.8994, 41.3065, 35.4208]]],\n",
      "\n",
      "\n",
      "        [[[51.7994, 83.6746, 83.6207,  ..., 31.5383, 51.5526, 29.4158]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[48.3817, 86.4462, 88.7235,  ..., 26.8993, 41.3066, 35.4208]]],\n",
      "\n",
      "\n",
      "        [[[51.2420, 85.5758, 89.5754,  ..., 31.5961, 43.0553, 28.7214]]],\n",
      "\n",
      "\n",
      "        [[[51.7999, 83.6741, 83.6205,  ..., 31.5379, 51.5531, 29.4161]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[51.9878, 84.9316, 88.5017,  ..., 30.3046, 57.4021, 24.2684]]],\n",
      "\n",
      "\n",
      "        [[[51.4932, 86.9008, 94.5859,  ..., 30.3141, 49.1050, 23.4373]]],\n",
      "\n",
      "\n",
      "        [[[51.9883, 84.9311, 88.5015,  ..., 30.3042, 57.4025, 24.2687]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[51.4932, 86.9008, 94.5859,  ..., 30.3141, 49.1050, 23.4373]]],\n",
      "\n",
      "\n",
      "        [[[51.9882, 84.9312, 88.5015,  ..., 30.3043, 57.4025, 24.2686]]],\n",
      "\n",
      "\n",
      "        [[[51.4932, 86.9008, 94.5859,  ..., 30.3141, 49.1050, 23.4373]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[53.1902, 85.9927, 91.7745,  ..., 27.0416, 65.3971, 22.0925]]],\n",
      "\n",
      "\n",
      "        [[[53.1905, 85.9923, 91.7743,  ..., 27.0414, 65.3975, 22.0927]]],\n",
      "\n",
      "\n",
      "        [[[53.3637, 87.4092, 94.8807,  ..., 26.7567, 52.2368, 15.9015]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[53.1901, 85.9926, 91.7737,  ..., 27.0417, 65.3980, 22.0926]]],\n",
      "\n",
      "\n",
      "        [[[53.1902, 85.9927, 91.7745,  ..., 27.0416, 65.3972, 22.0925]]],\n",
      "\n",
      "\n",
      "        [[[53.1907, 85.9920, 91.7742,  ..., 27.0412, 65.3978, 22.0929]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[54.3045, 87.2384, 92.6705,  ..., 24.5918, 70.6977, 23.7470]]],\n",
      "\n",
      "\n",
      "        [[[54.3046, 87.2382, 92.6704,  ..., 24.5917, 70.6979, 23.7471]]],\n",
      "\n",
      "\n",
      "        [[[54.3045, 87.2384, 92.6704,  ..., 24.5918, 70.6977, 23.7470]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[54.5864, 88.7476, 95.7287,  ..., 24.1436, 57.8659, 17.6055]]],\n",
      "\n",
      "\n",
      "        [[[53.9544, 89.3354, 98.8869,  ..., 24.4463, 62.8398, 22.8625]]],\n",
      "\n",
      "\n",
      "        [[[54.5862, 88.7477, 95.7288,  ..., 24.1437, 57.8658, 17.6054]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 12, Loss: 158064.857421875\n",
      "tensor([[[[53.9526, 89.4749, 92.2144,  ..., 26.8648, 70.4469, 27.0095]]],\n",
      "\n",
      "\n",
      "        [[[53.9532, 89.4742, 92.2142,  ..., 26.8644, 70.4474, 27.0099]]],\n",
      "\n",
      "\n",
      "        [[[51.0863, 93.3366, 98.2039,  ..., 21.4872, 62.3985, 32.3749]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[53.6315, 91.6560, 98.4370,  ..., 26.7687, 62.6484, 26.1802]]],\n",
      "\n",
      "\n",
      "        [[[51.0854, 93.3377, 98.2043,  ..., 21.4879, 62.3975, 32.3744]]],\n",
      "\n",
      "\n",
      "        [[[53.9527, 89.4748, 92.2145,  ..., 26.8648, 70.4468, 27.0095]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[51.7038, 91.7961, 94.6406,  ..., 34.3300, 53.1988, 23.4420]]],\n",
      "\n",
      "\n",
      "        [[[51.7052, 91.7944, 94.6401,  ..., 34.3290, 53.2001, 23.4428]]],\n",
      "\n",
      "\n",
      "        [[[48.2983, 94.1154, 97.7005,  ..., 29.9727, 57.4501, 34.9575]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[51.4093, 90.0703, 91.7916,  ..., 34.4519, 66.0612, 29.3824]]],\n",
      "\n",
      "\n",
      "        [[[51.4094, 90.0704, 91.7922,  ..., 34.4518, 66.0606, 29.3824]]],\n",
      "\n",
      "\n",
      "        [[[48.2931, 94.1214, 97.7028,  ..., 29.9766, 57.4446, 34.9546]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[49.3770, 87.1173, 91.0122,  ..., 42.1255, 62.8796, 29.3331]]],\n",
      "\n",
      "\n",
      "        [[[46.1051, 90.8887, 96.7933,  ..., 38.5420, 53.8835, 34.8301]]],\n",
      "\n",
      "\n",
      "        [[[49.3771, 87.1172, 91.0122,  ..., 42.1254, 62.8797, 29.3331]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[49.3770, 87.1173, 91.0122,  ..., 42.1255, 62.8796, 29.3330]]],\n",
      "\n",
      "\n",
      "        [[[49.0203, 89.2980, 97.2307,  ..., 42.3687, 55.0444, 28.5222]]],\n",
      "\n",
      "\n",
      "        [[[46.1068, 90.8867, 96.7926,  ..., 38.5406, 53.8854, 34.8310]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[48.7714, 80.8654, 90.4145,  ..., 46.4246, 62.6100, 27.2289]]],\n",
      "\n",
      "\n",
      "        [[[49.1156, 82.2645, 93.0398,  ..., 46.7220, 49.7808, 21.1402]]],\n",
      "\n",
      "\n",
      "        [[[48.7714, 80.8654, 90.4145,  ..., 46.4246, 62.6100, 27.2289]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[49.1165, 82.2634, 93.0395,  ..., 46.7213, 49.7816, 21.1407]]],\n",
      "\n",
      "\n",
      "        [[[48.7719, 80.8648, 90.4143,  ..., 46.4243, 62.6106, 27.2292]]],\n",
      "\n",
      "\n",
      "        [[[48.7714, 80.8651, 90.4135,  ..., 46.4246, 62.6111, 27.2290]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 13, Loss: 49791.35546875\n",
      "tensor([[[[49.0551, 73.5396, 90.4133,  ..., 47.1612, 51.2757, 20.2070]]],\n",
      "\n",
      "\n",
      "        [[[48.6267, 72.4787, 87.9781,  ..., 46.9037, 63.9461, 26.3559]]],\n",
      "\n",
      "\n",
      "        [[[49.0551, 73.5396, 90.4132,  ..., 47.1612, 51.2757, 20.2070]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[48.6262, 72.4793, 87.9783,  ..., 46.9041, 63.9456, 26.3556]]],\n",
      "\n",
      "\n",
      "        [[[48.6267, 72.4787, 87.9781,  ..., 46.9037, 63.9462, 26.3559]]],\n",
      "\n",
      "\n",
      "        [[[48.3061, 74.3631, 94.1328,  ..., 47.2259, 56.2713, 25.4434]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[46.4352, 66.6698, 89.9506,  ..., 46.4828, 56.1561, 27.3386]]],\n",
      "\n",
      "\n",
      "        [[[43.4102, 66.3219, 88.7679,  ..., 43.0458, 54.8950, 33.3255]]],\n",
      "\n",
      "\n",
      "        [[[46.4352, 66.6698, 89.9506,  ..., 46.4828, 56.1561, 27.3386]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[46.4352, 66.6698, 89.9506,  ..., 46.4828, 56.1561, 27.3386]]],\n",
      "\n",
      "\n",
      "        [[[47.2109, 65.7009, 86.0658,  ..., 46.3752, 51.1856, 22.1375]]],\n",
      "\n",
      "\n",
      "        [[[46.7757, 64.9364, 83.8924,  ..., 46.2074, 63.7674, 28.2236]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 35.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 39.25it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[43.2604, 62.4556, 80.5197,  ..., 46.4184, 61.2023, 31.2107]]],\n",
      "\n",
      "\n",
      "        [[[43.6250, 63.1491, 82.4654,  ..., 46.5390, 48.5984, 25.2378]]],\n",
      "\n",
      "\n",
      "        [[[42.8637, 64.1601, 86.4987,  ..., 46.6535, 53.6016, 30.3808]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.6257, 63.1483, 82.4651,  ..., 46.5385, 48.5991, 25.2381]]],\n",
      "\n",
      "\n",
      "        [[[43.2604, 62.4557, 80.5197,  ..., 46.4184, 61.2022, 31.2107]]],\n",
      "\n",
      "\n",
      "        [[[43.2586, 62.4579, 80.5204,  ..., 46.4198, 61.2004, 31.2096]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.3949, 64.3211, 81.0305,  ..., 45.7445, 44.9475, 28.2517]]],\n",
      "\n",
      "\n",
      "        [[[39.3967, 64.3188, 81.0298,  ..., 45.7433, 44.9491, 28.2526]]],\n",
      "\n",
      "\n",
      "        [[[39.1318, 63.5389, 79.2209,  ..., 45.7081, 57.6199, 34.1152]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.1322, 63.5383, 79.2205,  ..., 45.7078, 57.6207, 34.1155]]],\n",
      "\n",
      "\n",
      "        [[[39.1322, 63.5383, 79.2205,  ..., 45.7078, 57.6206, 34.1155]]],\n",
      "\n",
      "\n",
      "        [[[39.3966, 64.3189, 81.0298,  ..., 45.7434, 44.9491, 28.2526]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 14, Loss: 69046.8671875\n",
      "tensor([[[[36.5443, 67.1865, 82.3793,  ..., 41.9759, 42.4146, 30.1972]]],\n",
      "\n",
      "\n",
      "        [[[35.8377, 68.1095, 86.5731,  ..., 42.1692, 47.5183, 35.2490]]],\n",
      "\n",
      "\n",
      "        [[[36.3229, 66.2477, 80.5907,  ..., 42.1518, 55.1036, 35.9948]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[36.3225, 66.2482, 80.5909,  ..., 42.1521, 55.1033, 35.9945]]],\n",
      "\n",
      "\n",
      "        [[[36.3227, 66.2477, 80.5894,  ..., 42.1521, 55.1052, 35.9947]]],\n",
      "\n",
      "\n",
      "        [[[36.5413, 67.1905, 82.3806,  ..., 41.9782, 42.4118, 30.1955]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[36.0767, 68.5037, 83.0203,  ..., 36.4144, 55.0018, 36.6902]]],\n",
      "\n",
      "\n",
      "        [[[31.6937, 70.7612, 87.8033,  ..., 31.9753, 45.1217, 42.5061]]],\n",
      "\n",
      "\n",
      "        [[[36.0772, 68.5032, 83.0201,  ..., 36.4140, 55.0023, 36.6905]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[36.3666, 69.5779, 84.8325,  ..., 35.9293, 42.4061, 30.9060]]],\n",
      "\n",
      "\n",
      "        [[[35.6409, 70.4552, 89.0581,  ..., 36.2322, 47.5051, 35.9460]]],\n",
      "\n",
      "\n",
      "        [[[36.0767, 68.5038, 83.0205,  ..., 36.4144, 55.0015, 36.6901]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[36.7881, 69.3896, 85.3477,  ..., 32.1712, 55.4850, 36.8871]]],\n",
      "\n",
      "\n",
      "        [[[36.7881, 69.3896, 85.3478,  ..., 32.1712, 55.4849, 36.8871]]],\n",
      "\n",
      "\n",
      "        [[[36.7872, 69.3906, 85.3479,  ..., 32.1719, 55.4843, 36.8866]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.1840, 70.5363, 87.1775,  ..., 31.4462, 43.0085, 31.0961]]],\n",
      "\n",
      "\n",
      "        [[[37.1830, 70.5375, 87.1779,  ..., 31.4470, 43.0077, 31.0955]]],\n",
      "\n",
      "\n",
      "        [[[32.5736, 71.8496, 90.4186,  ..., 27.1900, 45.7002, 42.6531]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[37.3058, 69.4802, 86.5827,  ..., 31.5941, 55.4631, 37.2887]]],\n",
      "\n",
      "\n",
      "        [[[37.7954, 70.6634, 88.3831,  ..., 30.7949, 43.0821, 31.5009]]],\n",
      "\n",
      "\n",
      "        [[[37.7949, 70.6640, 88.3833,  ..., 30.7952, 43.0816, 31.5007]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.7944, 70.6646, 88.3835,  ..., 30.7956, 43.0812, 31.5004]]],\n",
      "\n",
      "\n",
      "        [[[37.0327, 71.5282, 92.6916,  ..., 31.1902, 48.1731, 36.5248]]],\n",
      "\n",
      "\n",
      "        [[[37.3045, 69.4818, 86.5830,  ..., 31.5952, 55.4620, 37.2879]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 15, Loss: 41035.32666015625\n",
      "tensor([[[[38.1131, 69.4853, 86.4631,  ..., 33.2944, 55.9296, 37.7416]]],\n",
      "\n",
      "\n",
      "        [[[38.7055, 70.7032, 88.1746,  ..., 32.5274, 43.6645, 31.9607]]],\n",
      "\n",
      "\n",
      "        [[[38.7071, 70.7010, 88.1740,  ..., 32.5261, 43.6660, 31.9617]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[34.2191, 72.1394, 91.6801,  ..., 28.4041, 46.2745, 43.4767]]],\n",
      "\n",
      "\n",
      "        [[[34.2203, 72.1379, 91.6796,  ..., 28.4032, 46.2758, 43.4774]]],\n",
      "\n",
      "\n",
      "        [[[38.1133, 69.4842, 86.4604,  ..., 33.2944, 55.9329, 37.7421]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.7528, 70.6256, 86.3152,  ..., 34.9971, 57.9463, 37.3514]]],\n",
      "\n",
      "\n",
      "        [[[39.6879, 72.7653, 92.3866,  ..., 34.6086, 50.9318, 36.5516]]],\n",
      "\n",
      "\n",
      "        [[[39.7532, 70.6251, 86.3150,  ..., 34.9968, 57.9468, 37.3517]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7533, 70.6250, 86.3150,  ..., 34.9967, 57.9469, 37.3517]]],\n",
      "\n",
      "\n",
      "        [[[36.1373, 73.5103, 91.5185,  ..., 30.2900, 48.5844, 42.9745]]],\n",
      "\n",
      "\n",
      "        [[[39.7528, 70.6256, 86.3153,  ..., 34.9971, 57.9463, 37.3514]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.2388, 72.3501, 86.8240,  ..., 36.0657, 60.8485, 36.0162]]],\n",
      "\n",
      "\n",
      "        [[[41.2382, 72.3512, 86.8254,  ..., 36.0661, 60.8466, 36.0157]]],\n",
      "\n",
      "\n",
      "        [[[42.1067, 73.7521, 88.3702,  ..., 35.3353, 49.0175, 30.1411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.8827, 75.5339, 92.0934,  ..., 31.4578, 51.8937, 41.4145]]],\n",
      "\n",
      "\n",
      "        [[[41.2389, 72.3520, 86.8293,  ..., 36.0653, 60.8425, 36.0155]]],\n",
      "\n",
      "\n",
      "        [[[41.2847, 74.5576, 92.8820,  ..., 35.6824, 54.0174, 35.1688]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.6403, 74.2236, 87.9440,  ..., 37.3961, 63.0703, 34.2260]]],\n",
      "\n",
      "\n",
      "        [[[41.6399, 74.2246, 87.9458,  ..., 37.3962, 63.0681, 34.2255]]],\n",
      "\n",
      "\n",
      "        [[[41.7733, 76.4884, 93.9744,  ..., 37.0275, 56.4352, 33.3268]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.3942, 77.7304, 93.3344,  ..., 32.9280, 54.4572, 39.3415]]],\n",
      "\n",
      "\n",
      "        [[[41.6407, 74.2230, 87.9438,  ..., 37.3957, 63.0708, 34.2262]]],\n",
      "\n",
      "\n",
      "        [[[41.6403, 74.2235, 87.9440,  ..., 37.3960, 63.0704, 34.2260]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 38.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 43.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.20it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 23594.7470703125\n",
      "tensor([[[[40.9999, 75.4742, 88.8049,  ..., 39.4699, 63.6661, 33.0360]]],\n",
      "\n",
      "\n",
      "        [[[41.0002, 75.4737, 88.8046,  ..., 39.4697, 63.6668, 33.0363]]],\n",
      "\n",
      "\n",
      "        [[[37.7514, 79.1998, 94.2633,  ..., 35.2150, 55.2355, 37.9551]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.7645, 79.1840, 94.2578,  ..., 35.2049, 55.2496, 37.9626]]],\n",
      "\n",
      "\n",
      "        [[[37.7647, 79.1837, 94.2577,  ..., 35.2047, 55.2498, 37.9628]]],\n",
      "\n",
      "\n",
      "        [[[41.9937, 77.0383, 90.1982,  ..., 38.8002, 52.2545, 27.0251]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.4264, 75.4106, 88.6583,  ..., 41.1929, 62.9440, 33.1223]]],\n",
      "\n",
      "\n",
      "        [[[40.4265, 75.4098, 88.6569,  ..., 41.1928, 62.9456, 33.1226]]],\n",
      "\n",
      "\n",
      "        [[[40.6903, 77.6920, 94.5323,  ..., 40.8782, 56.6575, 32.1930]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.4648, 76.9613, 89.8913,  ..., 40.5515, 51.6907, 27.1222]]],\n",
      "\n",
      "\n",
      "        [[[37.1980, 79.1627, 94.0273,  ..., 37.0967, 54.5884, 38.0163]]],\n",
      "\n",
      "\n",
      "        [[[41.4631, 76.9634, 89.8920,  ..., 40.5528, 51.6891, 27.1212]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.2912, 73.5262, 87.2256,  ..., 41.8054, 61.7246, 34.8165]]],\n",
      "\n",
      "\n",
      "        [[[40.2918, 73.5261, 87.2267,  ..., 41.8049, 61.7237, 34.8167]]],\n",
      "\n",
      "\n",
      "        [[[40.2903, 73.5221, 87.2143,  ..., 41.8062, 61.7364, 34.8181]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2918, 73.5260, 87.2266,  ..., 41.8049, 61.7239, 34.8167]]],\n",
      "\n",
      "\n",
      "        [[[41.3949, 74.9638, 88.2075,  ..., 41.1427, 50.6498, 28.9065]]],\n",
      "\n",
      "\n",
      "        [[[37.1238, 77.0789, 92.3189,  ..., 37.7429, 53.4181, 39.8812]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.2996, 75.1771, 90.8376,  ..., 37.9192, 53.4594, 41.0433]]],\n",
      "\n",
      "\n",
      "        [[[41.2701, 71.8370, 86.0587,  ..., 41.9987, 61.5066, 35.8716]]],\n",
      "\n",
      "\n",
      "        [[[41.7091, 73.9288, 91.5242,  ..., 41.6558, 55.7237, 35.0401]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.2674, 71.8328, 86.0437,  ..., 42.0008, 61.5216, 35.8731]]],\n",
      "\n",
      "\n",
      "        [[[41.2703, 71.8367, 86.0584,  ..., 41.9986, 61.5071, 35.8718]]],\n",
      "\n",
      "\n",
      "        [[[41.2705, 71.8364, 86.0582,  ..., 41.9985, 61.5074, 35.8719]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 17, Loss: 30895.59619140625\n",
      "tensor([[[[41.9546, 70.1633, 85.5733,  ..., 41.9772, 61.5604, 36.2852]]],\n",
      "\n",
      "\n",
      "        [[[42.4877, 72.0558, 90.6529,  ..., 41.5968, 56.2681, 35.5285]]],\n",
      "\n",
      "\n",
      "        [[[41.9550, 70.1625, 85.5724,  ..., 41.9769, 61.5616, 36.2856]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.9541, 70.1629, 85.5714,  ..., 41.9775, 61.5622, 36.2853]]],\n",
      "\n",
      "\n",
      "        [[[43.2741, 71.2292, 85.8056,  ..., 41.2196, 51.2901, 30.5395]]],\n",
      "\n",
      "\n",
      "        [[[41.9495, 70.1604, 85.5566,  ..., 41.9809, 61.5759, 36.2859]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.2746, 68.8695, 85.6070,  ..., 41.3463, 61.8504, 36.1059]]],\n",
      "\n",
      "\n",
      "        [[[39.5882, 71.4646, 89.2954,  ..., 37.0997, 55.1912, 41.4284]]],\n",
      "\n",
      "\n",
      "        [[[39.5876, 71.4653, 89.2957,  ..., 37.1002, 55.1906, 41.4281]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.8918, 70.4127, 90.0098,  ..., 40.9084, 57.3825, 35.4842]]],\n",
      "\n",
      "\n",
      "        [[[42.2746, 68.8690, 85.6060,  ..., 41.3463, 61.8514, 36.1060]]],\n",
      "\n",
      "\n",
      "        [[[42.8918, 70.4126, 90.0098,  ..., 40.9084, 57.3825, 35.4842]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.8650, 68.5422, 84.2438,  ..., 38.6054, 53.7495, 30.3956]]],\n",
      "\n",
      "\n",
      "        [[[39.7834, 70.4276, 88.4570,  ..., 35.0740, 56.5658, 41.2711]]],\n",
      "\n",
      "\n",
      "        [[[42.3859, 68.3124, 85.6749,  ..., 39.5630, 62.1003, 35.8196]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7835, 70.4275, 88.4570,  ..., 35.0739, 56.5658, 41.2712]]],\n",
      "\n",
      "\n",
      "        [[[42.3857, 68.3125, 85.6746,  ..., 39.5631, 62.1005, 35.8196]]],\n",
      "\n",
      "\n",
      "        [[[39.7853, 70.4252, 88.4562,  ..., 35.0725, 56.5678, 41.2722]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.4701, 70.8402, 87.7152,  ..., 33.5874, 57.1996, 41.4193]]],\n",
      "\n",
      "\n",
      "        [[[42.7762, 69.7199, 88.4580,  ..., 37.6994, 59.2718, 35.5680]]],\n",
      "\n",
      "\n",
      "        [[[43.5724, 68.8743, 83.4628,  ..., 37.2091, 54.3745, 30.5788]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.4703, 70.8400, 87.7151,  ..., 33.5872, 57.1999, 41.4194]]],\n",
      "\n",
      "\n",
      "        [[[39.4724, 70.8374, 87.7142,  ..., 33.5856, 57.2021, 41.4206]]],\n",
      "\n",
      "\n",
      "        [[[42.7762, 69.7199, 88.4580,  ..., 37.6994, 59.2718, 35.5680]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 18, Loss: 16103.80810546875\n",
      "tensor([[[[42.4771, 69.5539, 82.8957,  ..., 37.1342, 53.4911, 31.4722]]],\n",
      "\n",
      "\n",
      "        [[[38.2884, 71.6344, 87.2100,  ..., 33.5258, 56.2073, 42.3268]]],\n",
      "\n",
      "\n",
      "        [[[40.9595, 70.0084, 85.8702,  ..., 38.1933, 60.1969, 36.5387]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.9595, 70.0084, 85.8702,  ..., 38.1933, 60.1969, 36.5387]]],\n",
      "\n",
      "\n",
      "        [[[42.4773, 69.5537, 82.8956,  ..., 37.1341, 53.4912, 31.4723]]],\n",
      "\n",
      "\n",
      "        [[[40.9602, 70.0086, 85.8718,  ..., 38.1928, 60.1956, 36.5387]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 43.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.98it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[41.3173, 69.3793, 82.1775,  ..., 38.1487, 52.5522, 33.0339]]],\n",
      "\n",
      "\n",
      "        [[[41.3164, 69.3803, 82.1777,  ..., 38.1493, 52.5514, 33.0335]]],\n",
      "\n",
      "\n",
      "        [[[39.8172, 70.0042, 85.5558,  ..., 39.1871, 58.8717, 37.9806]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.8065, 69.9538, 85.4431,  ..., 39.1950, 58.9878, 38.0011]]],\n",
      "\n",
      "\n",
      "        [[[37.0398, 71.5055, 86.5392,  ..., 34.6309, 55.1597, 43.9548]]],\n",
      "\n",
      "\n",
      "        [[[39.8172, 70.0042, 85.5558,  ..., 39.1871, 58.8717, 37.9806]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.0183, 68.5124, 82.3612,  ..., 39.9644, 53.1535, 33.8743]]],\n",
      "\n",
      "\n",
      "        [[[36.7155, 70.6329, 86.8378,  ..., 36.5967, 55.7658, 44.8076]]],\n",
      "\n",
      "\n",
      "        [[[39.4958, 69.1821, 85.7982,  ..., 40.9745, 59.3720, 38.7853]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0158, 68.5157, 82.3622,  ..., 39.9662, 53.1512, 33.8729]]],\n",
      "\n",
      "\n",
      "        [[[39.4954, 69.1815, 85.7965,  ..., 40.9748, 59.3736, 38.7855]]],\n",
      "\n",
      "\n",
      "        [[[39.4960, 69.1877, 85.8086,  ..., 40.9745, 59.3609, 38.7830]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.2664, 67.2634, 86.8520,  ..., 41.8555, 60.5650, 38.6028]]],\n",
      "\n",
      "\n",
      "        [[[37.5712, 68.5856, 88.0562,  ..., 37.5820, 57.1086, 44.5483]]],\n",
      "\n",
      "\n",
      "        [[[40.2512, 67.2143, 86.7354,  ..., 41.8641, 60.6813, 38.6229]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.5711, 68.5857, 88.0563,  ..., 37.5821, 57.1085, 44.5482]]],\n",
      "\n",
      "\n",
      "        [[[40.2507, 67.2138, 86.7336,  ..., 41.8644, 60.6829, 38.6230]]],\n",
      "\n",
      "\n",
      "        [[[37.5713, 68.5855, 88.0562,  ..., 37.5819, 57.1087, 44.5483]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 19, Loss: 16413.574462890625\n",
      "tensor([[[[41.2616, 65.9504, 87.9467,  ..., 42.6028, 61.2021, 38.1853]]],\n",
      "\n",
      "\n",
      "        [[[38.7134, 67.3241, 89.6004,  ..., 38.4022, 57.5689, 43.9803]]],\n",
      "\n",
      "\n",
      "        [[[41.2610, 65.9494, 87.9439,  ..., 42.6031, 61.2047, 38.1856]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.7117, 67.3262, 89.6011,  ..., 38.4035, 57.5671, 43.9793]]],\n",
      "\n",
      "\n",
      "        [[[41.2614, 65.9504, 87.9465,  ..., 42.6029, 61.2022, 38.1852]]],\n",
      "\n",
      "\n",
      "        [[[41.2624, 65.9514, 87.9499,  ..., 42.6023, 61.1991, 38.1849]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.1085, 64.8846, 85.2210,  ..., 42.0453, 53.2618, 33.3826]]],\n",
      "\n",
      "\n",
      "        [[[38.9791, 66.9146, 90.2269,  ..., 38.8705, 55.7845, 44.1253]]],\n",
      "\n",
      "\n",
      "        [[[42.2884, 65.7540, 90.2913,  ..., 42.5126, 58.0515, 38.3250]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.9823, 66.9107, 90.2257,  ..., 38.8681, 55.7880, 44.1271]]],\n",
      "\n",
      "\n",
      "        [[[41.4789, 65.3604, 88.1608,  ..., 43.0114, 59.8684, 38.4338]]],\n",
      "\n",
      "\n",
      "        [[[43.1099, 64.8830, 85.2205,  ..., 42.0443, 53.2630, 33.3834]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.1665, 66.3571, 87.2833,  ..., 43.3462, 58.4713, 38.6196]]],\n",
      "\n",
      "\n",
      "        [[[41.9898, 66.9507, 89.7226,  ..., 42.8606, 56.2955, 38.4298]]],\n",
      "\n",
      "\n",
      "        [[[42.8078, 66.1149, 84.6422,  ..., 42.3965, 51.4983, 33.4886]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.6613, 68.2930, 89.7137,  ..., 39.2639, 53.8571, 44.1879]]],\n",
      "\n",
      "\n",
      "        [[[41.9897, 66.9507, 89.7226,  ..., 42.8606, 56.2955, 38.4298]]],\n",
      "\n",
      "\n",
      "        [[[42.8082, 66.1145, 84.6420,  ..., 42.3963, 51.4986, 33.4888]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.8446, 67.9415, 85.9696,  ..., 43.1539, 58.3031, 38.4107]]],\n",
      "\n",
      "\n",
      "        [[[40.8445, 67.9414, 85.9694,  ..., 43.1539, 58.3032, 38.4107]]],\n",
      "\n",
      "\n",
      "        [[[40.8443, 67.9400, 85.9665,  ..., 43.1540, 58.3063, 38.4113]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.6821, 68.7290, 88.6748,  ..., 42.6725, 55.8176, 38.1420]]],\n",
      "\n",
      "\n",
      "        [[[40.8441, 67.9400, 85.9662,  ..., 43.1541, 58.3065, 38.4113]]],\n",
      "\n",
      "\n",
      "        [[[41.6821, 68.7290, 88.6748,  ..., 42.6725, 55.8176, 38.1420]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 20, Loss: 17735.58154296875\n",
      "tensor([[[[38.4344, 71.4577, 87.3740,  ..., 38.1395, 54.5270, 43.0146]]],\n",
      "\n",
      "\n",
      "        [[[40.9125, 68.8055, 84.5073,  ..., 42.2840, 59.5757, 37.7827]]],\n",
      "\n",
      "\n",
      "        [[[42.5854, 68.9650, 82.2640,  ..., 41.3170, 52.1770, 32.4888]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.9126, 68.8050, 84.5067,  ..., 42.2839, 59.5764, 37.7829]]],\n",
      "\n",
      "\n",
      "        [[[42.5860, 68.9642, 82.2638,  ..., 41.3165, 52.1776, 32.4892]]],\n",
      "\n",
      "\n",
      "        [[[38.4192, 71.4761, 87.3803,  ..., 38.1508, 54.5108, 43.0060]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.0150, 68.7810, 83.5511,  ..., 41.2987, 60.7829, 36.7103]]],\n",
      "\n",
      "\n",
      "        [[[38.5743, 71.6373, 86.6444,  ..., 37.0769, 55.6306, 41.7065]]],\n",
      "\n",
      "\n",
      "        [[[38.5738, 71.6379, 86.6446,  ..., 37.0773, 55.6301, 41.7063]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0156, 68.7814, 83.5529,  ..., 41.2983, 60.7813, 36.7101]]],\n",
      "\n",
      "\n",
      "        [[[41.8905, 69.8321, 86.5975,  ..., 40.7950, 57.9199, 36.3058]]],\n",
      "\n",
      "\n",
      "        [[[41.0005, 68.8185, 83.5891,  ..., 41.3105, 60.7328, 36.6923]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 36.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 45.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 43.79it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[40.4530, 67.7818, 82.4814,  ..., 41.1016, 60.3123, 36.7995]]],\n",
      "\n",
      "\n",
      "        [[[42.1510, 68.1730, 80.5483,  ..., 40.1136, 52.5892, 31.3565]]],\n",
      "\n",
      "\n",
      "        [[[37.9794, 70.7091, 85.7448,  ..., 36.8854, 54.9147, 41.6999]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.3380, 68.9335, 85.6752,  ..., 40.5955, 57.2641, 36.3434]]],\n",
      "\n",
      "\n",
      "        [[[42.1491, 68.1753, 80.5490,  ..., 40.1149, 52.5875, 31.3555]]],\n",
      "\n",
      "\n",
      "        [[[42.1504, 68.1737, 80.5486,  ..., 40.1140, 52.5886, 31.3562]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.7383, 68.4883, 85.2327,  ..., 41.3285, 55.9767, 37.1443]]],\n",
      "\n",
      "\n",
      "        [[[41.5395, 67.7276, 80.0996,  ..., 40.8714, 51.3004, 32.1735]]],\n",
      "\n",
      "\n",
      "        [[[37.3275, 70.2803, 85.3655,  ..., 37.7168, 53.5037, 42.5291]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.8304, 67.3008, 81.9779,  ..., 41.8371, 59.1066, 37.6083]]],\n",
      "\n",
      "\n",
      "        [[[39.8316, 67.2945, 81.9695,  ..., 41.8358, 59.1165, 37.6113]]],\n",
      "\n",
      "\n",
      "        [[[41.5377, 67.7298, 80.1002,  ..., 40.8726, 51.2988, 32.1725]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 21, Loss: 12580.125732421875\n",
      "tensor([[[[40.2687, 67.7298, 82.5163,  ..., 42.0165, 59.4120, 38.1185]]],\n",
      "\n",
      "\n",
      "        [[[40.2686, 67.7309, 82.5178,  ..., 42.0167, 59.4102, 38.1181]]],\n",
      "\n",
      "\n",
      "        [[[41.1862, 69.0256, 85.8956,  ..., 41.5208, 56.1538, 37.6140]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.8238, 70.9307, 86.1944,  ..., 37.9555, 53.7015, 42.9941]]],\n",
      "\n",
      "\n",
      "        [[[40.2685, 67.7297, 82.5158,  ..., 42.0166, 59.4124, 38.1186]]],\n",
      "\n",
      "\n",
      "        [[[41.1862, 69.0256, 85.8956,  ..., 41.5208, 56.1538, 37.6140]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.3977, 69.4781, 82.2021,  ..., 39.9347, 53.1868, 32.4394]]],\n",
      "\n",
      "\n",
      "        [[[39.3442, 72.2517, 87.8239,  ..., 36.7370, 55.4907, 42.7223]]],\n",
      "\n",
      "\n",
      "        [[[39.3407, 72.2559, 87.8253,  ..., 36.7397, 55.4869, 42.7203]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.6041, 68.8001, 83.8107,  ..., 40.9030, 61.0933, 37.9477]]],\n",
      "\n",
      "\n",
      "        [[[41.5906, 68.8259, 83.8305,  ..., 40.9135, 61.0638, 37.9350]]],\n",
      "\n",
      "\n",
      "        [[[41.5933, 68.7913, 83.7816,  ..., 40.9093, 61.1191, 37.9507]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.3324, 71.2228, 88.3823,  ..., 39.6280, 58.7570, 36.9123]]],\n",
      "\n",
      "\n",
      "        [[[42.3523, 69.7976, 84.8606,  ..., 40.1560, 62.0809, 37.4739]]],\n",
      "\n",
      "\n",
      "        [[[40.1910, 73.4726, 89.1159,  ..., 35.9371, 56.5673, 42.1201]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.1920, 70.5537, 83.3074,  ..., 39.1680, 54.2223, 31.9306]]],\n",
      "\n",
      "\n",
      "        [[[42.3520, 69.7974, 84.8598,  ..., 40.1562, 62.0816, 37.4739]]],\n",
      "\n",
      "\n",
      "        [[[40.2026, 73.4588, 89.1111,  ..., 35.9285, 56.5794, 42.1266]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.8765, 71.6781, 83.2745,  ..., 39.9968, 53.3134, 31.5380]]],\n",
      "\n",
      "\n",
      "        [[[43.0160, 72.3142, 88.3593,  ..., 40.4330, 57.8375, 36.5297]]],\n",
      "\n",
      "\n",
      "        [[[43.8766, 71.6780, 83.2745,  ..., 39.9967, 53.3135, 31.5381]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.0312, 70.8994, 84.9028,  ..., 40.9526, 61.0970, 37.0874]]],\n",
      "\n",
      "\n",
      "        [[[42.0313, 70.8994, 84.9029,  ..., 40.9525, 61.0970, 37.0874]]],\n",
      "\n",
      "\n",
      "        [[[43.8757, 71.6790, 83.2748,  ..., 39.9974, 53.3127, 31.5376]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 22, Loss: 11914.4462890625\n",
      "tensor([[[[41.1937, 70.1368, 82.3265,  ..., 41.6587, 59.3739, 37.6942]]],\n",
      "\n",
      "\n",
      "        [[[41.1936, 70.1367, 82.3263,  ..., 41.6588, 59.3741, 37.6942]]],\n",
      "\n",
      "\n",
      "        [[[41.1935, 70.1368, 82.3263,  ..., 41.6589, 59.3740, 37.6942]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.0341, 71.0089, 80.7443,  ..., 40.7381, 51.3964, 32.1159]]],\n",
      "\n",
      "\n",
      "        [[[41.1936, 70.1358, 82.3251,  ..., 41.6587, 59.3755, 37.6946]]],\n",
      "\n",
      "\n",
      "        [[[42.1829, 71.6459, 85.8892,  ..., 41.1529, 55.9329, 37.0967]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.0665, 72.9429, 84.5181,  ..., 38.0678, 51.8254, 42.7552]]],\n",
      "\n",
      "\n",
      "        [[[41.3470, 70.5786, 83.9595,  ..., 41.5178, 54.4027, 37.6153]]],\n",
      "\n",
      "\n",
      "        [[[38.0654, 72.9442, 84.5186,  ..., 38.0686, 51.8242, 42.7546]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.3592, 69.0611, 80.4060,  ..., 42.0200, 57.8822, 38.2194]]],\n",
      "\n",
      "\n",
      "        [[[38.0642, 72.9456, 84.5191,  ..., 38.0695, 51.8230, 42.7540]]],\n",
      "\n",
      "\n",
      "        [[[40.3586, 69.0598, 80.4032,  ..., 42.0203, 57.8850, 38.2200]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[37.6608, 72.0697, 85.2911,  ..., 37.5626, 51.7408, 42.3136]]],\n",
      "\n",
      "\n",
      "        [[[37.6618, 72.0686, 85.2908,  ..., 37.5619, 51.7418, 42.3141]]],\n",
      "\n",
      "\n",
      "        [[[39.9699, 68.3959, 81.3078,  ..., 41.5637, 57.4712, 37.7876]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.8061, 69.0708, 79.3875,  ..., 40.6372, 49.7997, 32.2777]]],\n",
      "\n",
      "\n",
      "        [[[39.9699, 68.3959, 81.3078,  ..., 41.5637, 57.4712, 37.7876]]],\n",
      "\n",
      "\n",
      "        [[[41.8039, 69.0736, 79.3884,  ..., 40.6387, 49.7977, 32.2765]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.0703, 68.4146, 81.4157,  ..., 39.7714, 51.0266, 31.8351]]],\n",
      "\n",
      "\n",
      "        [[[41.2158, 69.0580, 86.5989,  ..., 40.1727, 55.5022, 36.8215]]],\n",
      "\n",
      "\n",
      "        [[[40.2478, 68.0222, 83.7576,  ..., 40.7183, 58.1139, 37.2411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2492, 68.0291, 83.7695,  ..., 40.7179, 58.1015, 37.2380]]],\n",
      "\n",
      "\n",
      "        [[[40.2481, 68.0222, 83.7580,  ..., 40.7181, 58.1137, 37.2411]]],\n",
      "\n",
      "\n",
      "        [[[40.2484, 68.0239, 83.7608,  ..., 40.7180, 58.1107, 37.2404]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 23, Loss: 11090.3408203125\n",
      "tensor([[[[40.8267, 67.5226, 84.9884,  ..., 39.7458, 58.5229, 37.6883]]],\n",
      "\n",
      "\n",
      "        [[[38.6018, 70.7456, 88.7083,  ..., 35.5776, 53.8300, 42.3242]]],\n",
      "\n",
      "\n",
      "        [[[40.8266, 67.5229, 84.9888,  ..., 39.7459, 58.5224, 37.6881]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.7942, 68.3894, 87.5749,  ..., 39.1833, 56.2193, 37.3442]]],\n",
      "\n",
      "\n",
      "        [[[41.7942, 68.3894, 87.5748,  ..., 39.1833, 56.2193, 37.3442]]],\n",
      "\n",
      "\n",
      "        [[[40.8258, 67.5212, 84.9852,  ..., 39.7463, 58.5260, 37.6889]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.1068, 67.4309, 84.4850,  ..., 39.4898, 57.6066, 38.5794]]],\n",
      "\n",
      "\n",
      "        [[[41.1067, 67.4308, 84.4847,  ..., 39.4898, 57.6069, 38.5794]]],\n",
      "\n",
      "\n",
      "        [[[41.1063, 67.4303, 84.4833,  ..., 39.4901, 57.6082, 38.5797]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.1063, 67.4309, 84.4843,  ..., 39.4901, 57.6072, 38.5794]]],\n",
      "\n",
      "\n",
      "        [[[38.9397, 70.7475, 88.2725,  ..., 35.3104, 52.7739, 43.2529]]],\n",
      "\n",
      "\n",
      "        [[[41.0973, 67.4824, 84.5463,  ..., 39.4998, 57.5339, 38.5551]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.5707, 68.2713, 84.1453,  ..., 40.2582, 55.4601, 39.1312]]],\n",
      "\n",
      "\n",
      "        [[[40.5779, 68.2160, 84.0762,  ..., 40.2491, 55.5405, 39.1569]]],\n",
      "\n",
      "\n",
      "        [[[40.5775, 68.2149, 84.0741,  ..., 40.2493, 55.5427, 39.1574]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5778, 68.2163, 84.0765,  ..., 40.2492, 55.5401, 39.1568]]],\n",
      "\n",
      "\n",
      "        [[[42.4371, 68.4811, 81.3712,  ..., 39.3034, 48.8172, 33.8906]]],\n",
      "\n",
      "\n",
      "        [[[38.3701, 71.6210, 87.7892,  ..., 36.1655, 50.6068, 43.8782]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.1581, 68.7878, 81.5930,  ..., 39.2421, 48.6984, 33.3423]]],\n",
      "\n",
      "\n",
      "        [[[38.0728, 72.0003, 88.1178,  ..., 36.1096, 50.4526, 43.2455]]],\n",
      "\n",
      "\n",
      "        [[[38.0743, 71.9984, 88.1171,  ..., 36.1085, 50.4542, 43.2464]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2960, 68.5234, 84.3482,  ..., 40.1880, 55.3430, 38.6189]]],\n",
      "\n",
      "\n",
      "        [[[41.2778, 69.3575, 86.8261,  ..., 39.6286, 53.1376, 38.2952]]],\n",
      "\n",
      "\n",
      "        [[[38.0755, 71.9971, 88.1166,  ..., 36.1076, 50.4554, 43.2471]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 24, Loss: 9503.4951171875\n",
      "tensor([[[[38.6587, 71.7095, 89.0517,  ..., 35.0998, 52.7290, 41.5777]]],\n",
      "\n",
      "\n",
      "        [[[41.7985, 69.0352, 87.6012,  ..., 38.7026, 55.2127, 36.8144]]],\n",
      "\n",
      "\n",
      "        [[[40.7919, 68.0969, 84.9565,  ..., 39.2686, 57.5524, 37.2091]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.6599, 71.7081, 89.0512,  ..., 35.0989, 52.7302, 41.5784]]],\n",
      "\n",
      "\n",
      "        [[[41.7985, 69.0352, 87.6012,  ..., 38.7026, 55.2127, 36.8144]]],\n",
      "\n",
      "\n",
      "        [[[40.7843, 68.1503, 85.0215,  ..., 39.2778, 57.4770, 37.1837]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.8857, 67.3362, 84.6197,  ..., 39.9414, 59.1166, 36.7817]]],\n",
      "\n",
      "\n",
      "        [[[42.8388, 67.8932, 82.3237,  ..., 38.9924, 52.1703, 31.2973]]],\n",
      "\n",
      "\n",
      "        [[[40.8968, 67.2998, 84.5834,  ..., 39.9311, 59.1625, 36.7994]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.8244, 71.1258, 89.0835,  ..., 35.8501, 54.1437, 40.9654]]],\n",
      "\n",
      "\n",
      "        [[[41.9371, 68.4462, 87.5512,  ..., 39.3831, 56.5040, 36.2939]]],\n",
      "\n",
      "\n",
      "        [[[40.8960, 67.2916, 84.5708,  ..., 39.9309, 59.1757, 36.8032]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.4312, 66.6791, 83.1311,  ..., 42.0680, 58.9070, 38.1539]]],\n",
      "\n",
      "\n",
      "        [[[42.3921, 67.4920, 81.1515,  ..., 41.2180, 51.5791, 32.5917]]],\n",
      "\n",
      "\n",
      "        [[[40.4319, 66.6805, 83.1341,  ..., 42.0677, 58.9041, 38.1532]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.4327, 66.6858, 83.1424,  ..., 42.0678, 58.8956, 38.1508]]],\n",
      "\n",
      "\n",
      "        [[[40.4318, 66.6807, 83.1342,  ..., 42.0678, 58.9040, 38.1531]]],\n",
      "\n",
      "\n",
      "        [[[38.3550, 70.7333, 87.9100,  ..., 38.2438, 53.4907, 42.3176]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.5077, 67.0659, 80.5217,  ..., 41.8226, 50.8152, 34.0891]]],\n",
      "\n",
      "\n",
      "        [[[38.4918, 70.3145, 87.3193,  ..., 38.8941, 52.6527, 43.8885]]],\n",
      "\n",
      "\n",
      "        [[[42.5103, 67.0626, 80.5207,  ..., 41.8207, 50.8175, 34.0905]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5179, 66.1077, 82.3205,  ..., 42.6367, 58.3627, 39.6687]]],\n",
      "\n",
      "\n",
      "        [[[38.4911, 70.3154, 87.3195,  ..., 38.8946, 52.6519, 43.8881]]],\n",
      "\n",
      "\n",
      "        [[[42.5104, 67.0626, 80.5207,  ..., 41.8207, 50.8175, 34.0905]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 37.10it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 38.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 45.77it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 8918.544311523438\n",
      "tensor([[[[42.1259, 67.3476, 86.7253,  ..., 40.4904, 55.2165, 39.1548]]],\n",
      "\n",
      "\n",
      "        [[[39.0735, 70.0913, 88.4406,  ..., 37.0750, 52.7323, 43.9887]]],\n",
      "\n",
      "\n",
      "        [[[39.0716, 70.0937, 88.4414,  ..., 37.0765, 52.7303, 43.9876]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.0397, 66.8188, 81.4829,  ..., 40.1277, 50.9084, 34.2201]]],\n",
      "\n",
      "\n",
      "        [[[41.0168, 65.8596, 83.2703,  ..., 40.9964, 58.4019, 39.7971]]],\n",
      "\n",
      "\n",
      "        [[[39.0754, 70.0890, 88.4399,  ..., 37.0735, 52.7343, 43.9897]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.9662, 66.7900, 85.5343,  ..., 40.1899, 58.0330, 38.2128]]],\n",
      "\n",
      "\n",
      "        [[[39.0261, 71.0294, 90.7967,  ..., 36.1809, 52.6022, 42.2481]]],\n",
      "\n",
      "\n",
      "        [[[42.9828, 67.6551, 83.5915,  ..., 39.2944, 50.8033, 32.6392]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.0237, 71.0323, 90.7977,  ..., 36.1827, 52.5997, 42.2468]]],\n",
      "\n",
      "\n",
      "        [[[39.0137, 71.0445, 90.8018,  ..., 36.1902, 52.5894, 42.2412]]],\n",
      "\n",
      "\n",
      "        [[[40.9662, 66.7904, 85.5347,  ..., 40.1899, 58.0325, 38.2127]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.0699, 68.4306, 84.1353,  ..., 39.9431, 49.5335, 32.0008]]],\n",
      "\n",
      "\n",
      "        [[[40.0813, 67.6307, 86.2328,  ..., 40.8123, 56.5939, 37.5514]]],\n",
      "\n",
      "\n",
      "        [[[41.1635, 68.9001, 89.3174,  ..., 40.2961, 53.8393, 36.9822]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.0695, 68.4311, 84.1354,  ..., 39.9434, 49.5331, 32.0006]]],\n",
      "\n",
      "\n",
      "        [[[38.0556, 71.9049, 91.4701,  ..., 36.8827, 51.2182, 41.5199]]],\n",
      "\n",
      "\n",
      "        [[[40.0715, 67.6740, 86.2791,  ..., 40.8229, 56.5382, 37.5305]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.3404, 68.8284, 86.9223,  ..., 40.8758, 54.2021, 37.6463]]],\n",
      "\n",
      "\n",
      "        [[[40.2193, 67.4529, 83.7150,  ..., 41.3861, 57.1048, 38.2590]]],\n",
      "\n",
      "\n",
      "        [[[38.2658, 71.8791, 88.9457,  ..., 37.5282, 51.6103, 42.2082]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.2566, 68.3719, 81.7030,  ..., 40.5414, 49.9185, 32.6791]]],\n",
      "\n",
      "\n",
      "        [[[40.2193, 67.4529, 83.7150,  ..., 41.3861, 57.1048, 38.2590]]],\n",
      "\n",
      "\n",
      "        [[[40.2182, 67.4446, 83.7024,  ..., 41.3857, 57.1179, 38.2630]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 26, Loss: 7995.77392578125\n",
      "tensor([[[[41.4931, 66.8116, 80.6561,  ..., 41.1232, 59.6173, 38.6865]]],\n",
      "\n",
      "\n",
      "        [[[43.6078, 67.9424, 78.8345,  ..., 40.2933, 52.2300, 33.0157]]],\n",
      "\n",
      "\n",
      "        [[[41.4927, 66.8099, 80.6534,  ..., 41.1233, 59.6200, 38.6873]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.6068, 67.9436, 78.8348,  ..., 40.2940, 52.2291, 33.0152]]],\n",
      "\n",
      "\n",
      "        [[[39.7236, 71.4584, 85.9508,  ..., 37.2655, 54.0689, 42.5292]]],\n",
      "\n",
      "\n",
      "        [[[39.7237, 71.4583, 85.9508,  ..., 37.2654, 54.0690, 42.5293]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.4235, 66.3893, 79.5354,  ..., 40.7196, 60.6671, 38.2916]]],\n",
      "\n",
      "\n",
      "        [[[42.5983, 67.9952, 82.9741,  ..., 40.2224, 57.5149, 37.5538]]],\n",
      "\n",
      "\n",
      "        [[[41.4139, 66.4210, 79.5651,  ..., 40.7290, 60.6294, 38.2761]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.6772, 71.0604, 84.8016,  ..., 36.8248, 55.2315, 42.0386]]],\n",
      "\n",
      "\n",
      "        [[[43.5551, 67.5410, 77.6901,  ..., 39.8830, 53.3313, 32.5929]]],\n",
      "\n",
      "\n",
      "        [[[43.5550, 67.5411, 77.6901,  ..., 39.8830, 53.3313, 32.5929]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.1788, 67.5487, 81.2032,  ..., 41.3640, 59.7919, 36.9229]]],\n",
      "\n",
      "\n",
      "        [[[38.2895, 72.2419, 86.4921,  ..., 37.5449, 54.5230, 40.5476]]],\n",
      "\n",
      "\n",
      "        [[[42.2551, 68.6099, 79.1848,  ..., 40.5529, 52.6943, 31.2409]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.1790, 67.5492, 81.2041,  ..., 41.3639, 59.7910, 36.9227]]],\n",
      "\n",
      "\n",
      "        [[[40.1754, 67.5948, 81.2585,  ..., 41.3715, 59.7297, 36.9005]]],\n",
      "\n",
      "\n",
      "        [[[42.2551, 68.6098, 79.1848,  ..., 40.5529, 52.6943, 31.2409]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 31.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.02it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[40.7003, 69.4911, 86.0302,  ..., 40.4057, 56.0097, 35.8154]]],\n",
      "\n",
      "\n",
      "        [[[39.5701, 68.1072, 82.9393,  ..., 40.9091, 58.7430, 36.4700]]],\n",
      "\n",
      "\n",
      "        [[[39.5606, 68.1514, 82.9852,  ..., 40.9199, 58.6881, 36.4483]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5701, 68.1081, 82.9404,  ..., 40.9092, 58.7418, 36.4696]]],\n",
      "\n",
      "\n",
      "        [[[39.5606, 68.1514, 82.9852,  ..., 40.9199, 58.6881, 36.4483]]],\n",
      "\n",
      "\n",
      "        [[[39.5712, 68.1135, 82.9486,  ..., 40.9092, 58.7335, 36.4670]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 27, Loss: 7670.18310546875\n",
      "tensor([[[[38.0335, 72.1599, 89.2213,  ..., 36.4185, 53.0953, 41.4782]]],\n",
      "\n",
      "\n",
      "        [[[41.9972, 68.4692, 81.5724,  ..., 39.5055, 51.4055, 32.1738]]],\n",
      "\n",
      "\n",
      "        [[[39.9320, 67.5456, 83.7834,  ..., 40.3439, 58.2203, 37.7543]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9281, 67.5968, 83.8442,  ..., 40.3523, 58.1522, 37.7295]]],\n",
      "\n",
      "\n",
      "        [[[39.9316, 67.5454, 83.7825,  ..., 40.3441, 58.2210, 37.7545]]],\n",
      "\n",
      "\n",
      "        [[[38.0345, 72.1587, 89.2209,  ..., 36.4178, 53.0963, 41.4788]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.3290, 67.7506, 84.7284,  ..., 40.5128, 58.4146, 38.2763]]],\n",
      "\n",
      "\n",
      "        [[[43.4491, 68.6862, 82.5143,  ..., 39.6898, 51.6636, 32.7144]]],\n",
      "\n",
      "\n",
      "        [[[41.3211, 67.7993, 84.7811,  ..., 40.5236, 58.3535, 38.2527]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.6105, 72.4263, 90.3157,  ..., 36.6150, 53.3542, 42.0238]]],\n",
      "\n",
      "\n",
      "        [[[41.3290, 67.7518, 84.7299,  ..., 40.5130, 58.4130, 38.2758]]],\n",
      "\n",
      "\n",
      "        [[[41.3290, 67.7518, 84.7299,  ..., 40.5130, 58.4130, 38.2758]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.1328, 67.9127, 84.9136,  ..., 41.3997, 58.2047, 38.4480]]],\n",
      "\n",
      "\n",
      "        [[[42.1447, 67.8774, 84.8829,  ..., 41.3883, 58.2444, 38.4652]]],\n",
      "\n",
      "\n",
      "        [[[44.3076, 68.8682, 82.7068,  ..., 40.6070, 51.4759, 32.8883]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.3340, 69.2282, 87.9115,  ..., 40.8975, 55.6291, 37.8428]]],\n",
      "\n",
      "\n",
      "        [[[42.1434, 67.8752, 84.8787,  ..., 41.3887, 58.2481, 38.4663]]],\n",
      "\n",
      "\n",
      "        [[[40.5469, 72.6560, 90.6036,  ..., 37.5999, 53.1320, 42.1753]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.0767, 68.3931, 81.5247,  ..., 40.6199, 51.0880, 33.3413]]],\n",
      "\n",
      "\n",
      "        [[[40.3127, 72.1778, 89.4111,  ..., 37.6123, 52.6967, 42.6288]]],\n",
      "\n",
      "\n",
      "        [[[41.8598, 67.1977, 83.4437,  ..., 41.3904, 58.1433, 39.0054]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.8743, 67.2064, 83.4706,  ..., 41.3834, 58.1222, 39.0016]]],\n",
      "\n",
      "\n",
      "        [[[41.8643, 67.2393, 83.5005,  ..., 41.3936, 58.0845, 38.9854]]],\n",
      "\n",
      "\n",
      "        [[[43.0998, 68.7508, 86.7533,  ..., 40.9051, 55.2317, 38.2863]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 28, Loss: 6982.200927734375\n",
      "tensor([[[[38.9460, 71.0581, 87.9991,  ..., 36.8483, 52.6390, 42.8836]]],\n",
      "\n",
      "\n",
      "        [[[40.5926, 65.9732, 81.8922,  ..., 40.6793, 58.3119, 39.3514]]],\n",
      "\n",
      "\n",
      "        [[[42.7956, 67.3181, 80.1399,  ..., 39.9084, 51.0532, 33.6095]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5915, 66.0154, 81.9434,  ..., 40.6857, 58.2557, 39.3309]]],\n",
      "\n",
      "\n",
      "        [[[40.5925, 65.9716, 81.8901,  ..., 40.6792, 58.3141, 39.3521]]],\n",
      "\n",
      "\n",
      "        [[[38.9501, 71.0531, 87.9975,  ..., 36.8452, 52.6433, 42.8858]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.3101, 66.0850, 81.8128,  ..., 40.4141, 58.9680, 38.5587]]],\n",
      "\n",
      "\n",
      "        [[[40.5449, 67.8533, 85.3724,  ..., 39.9385, 55.7981, 37.7125]]],\n",
      "\n",
      "\n",
      "        [[[37.5572, 71.2931, 88.0541,  ..., 36.5648, 53.3124, 41.9337]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5449, 67.8533, 85.3724,  ..., 39.9385, 55.7981, 37.7125]]],\n",
      "\n",
      "\n",
      "        [[[37.5575, 71.2928, 88.0539,  ..., 36.5646, 53.3128, 41.9338]]],\n",
      "\n",
      "\n",
      "        [[[40.5449, 67.8533, 85.3724,  ..., 39.9385, 55.7981, 37.7125]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[36.9406, 72.2717, 88.7382,  ..., 37.3037, 53.9153, 41.1003]]],\n",
      "\n",
      "\n",
      "        [[[36.9383, 72.2745, 88.7392,  ..., 37.3055, 53.9130, 41.0991]]],\n",
      "\n",
      "\n",
      "        [[[39.9642, 68.7070, 85.9299,  ..., 40.6092, 56.3500, 36.9776]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.9058, 68.3902, 80.6884,  ..., 40.3357, 52.2750, 32.0089]]],\n",
      "\n",
      "\n",
      "        [[[38.7588, 67.1151, 82.6489,  ..., 41.0829, 59.2035, 37.7483]]],\n",
      "\n",
      "\n",
      "        [[[38.7588, 67.1159, 82.6499,  ..., 41.0831, 59.2023, 37.7479]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[37.6204, 72.2117, 88.8317,  ..., 37.2194, 53.3582, 41.5749]]],\n",
      "\n",
      "\n",
      "        [[[39.3619, 67.1175, 82.8241,  ..., 41.0059, 58.5366, 38.1573]]],\n",
      "\n",
      "\n",
      "        [[[41.5269, 68.3052, 80.7077,  ..., 40.2587, 51.7744, 32.4797]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.3619, 67.1174, 82.8241,  ..., 41.0060, 58.5367, 38.1573]]],\n",
      "\n",
      "\n",
      "        [[[39.3619, 67.1169, 82.8234,  ..., 41.0059, 58.5374, 38.1576]]],\n",
      "\n",
      "\n",
      "        [[[39.3632, 67.1228, 82.8319,  ..., 41.0059, 58.5290, 38.1547]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 29, Loss: 6284.6253662109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 39.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 47.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 45.34it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[40.6305, 66.5379, 82.8190,  ..., 40.7103, 57.2441, 38.9947]]],\n",
      "\n",
      "\n",
      "        [[[39.0598, 71.5266, 88.7779,  ..., 36.8823, 52.0581, 42.5151]]],\n",
      "\n",
      "\n",
      "        [[[40.6377, 66.4919, 82.7712,  ..., 40.6996, 57.2993, 39.0174]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.8533, 67.6415, 80.5926,  ..., 39.9478, 50.5797, 33.3824]]],\n",
      "\n",
      "\n",
      "        [[[40.6376, 66.4904, 82.7694,  ..., 40.6995, 57.3012, 39.0181]]],\n",
      "\n",
      "\n",
      "        [[[39.0638, 71.5217, 88.7762,  ..., 36.8791, 52.0623, 42.5174]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.9614, 66.3323, 83.2195,  ..., 40.7058, 57.1397, 38.6203]]],\n",
      "\n",
      "\n",
      "        [[[41.9680, 66.2861, 83.1714,  ..., 40.6953, 57.1950, 38.6431]]],\n",
      "\n",
      "\n",
      "        [[[40.5677, 71.3557, 89.2954,  ..., 36.8839, 51.9504, 42.0592]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.9672, 66.2842, 83.1682,  ..., 40.6954, 57.1980, 38.6440]]],\n",
      "\n",
      "\n",
      "        [[[44.2437, 67.4559, 81.0089,  ..., 39.9515, 50.4947, 32.9857]]],\n",
      "\n",
      "\n",
      "        [[[43.2324, 67.7516, 86.2464,  ..., 40.2159, 54.5598, 37.9283]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.3850, 67.7688, 81.6438,  ..., 40.3383, 51.2017, 32.1495]]],\n",
      "\n",
      "\n",
      "        [[[42.0872, 66.6034, 83.8277,  ..., 41.0679, 57.8032, 37.8232]]],\n",
      "\n",
      "\n",
      "        [[[40.7338, 71.7169, 90.0461,  ..., 37.2959, 52.6927, 41.1321]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.7240, 71.7289, 90.0502,  ..., 37.3033, 52.6827, 41.1267]]],\n",
      "\n",
      "\n",
      "        [[[40.7324, 71.7186, 90.0467,  ..., 37.2970, 52.6913, 41.1314]]],\n",
      "\n",
      "\n",
      "        [[[42.0880, 66.5512, 83.7667,  ..., 41.0596, 57.8691, 37.8492]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.1082, 67.2343, 84.0495,  ..., 41.4021, 58.6486, 37.9873]]],\n",
      "\n",
      "\n",
      "        [[[41.1082, 67.2340, 84.0491,  ..., 41.4020, 58.6490, 37.9875]]],\n",
      "\n",
      "\n",
      "        [[[42.3826, 68.7516, 87.1505,  ..., 40.9383, 56.0417, 37.2371]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.1088, 67.2350, 84.0509,  ..., 41.4018, 58.6474, 37.9870]]],\n",
      "\n",
      "\n",
      "        [[[43.3870, 68.5008, 81.9363,  ..., 40.7010, 52.0374, 32.2848]]],\n",
      "\n",
      "\n",
      "        [[[42.3826, 68.7516, 87.1505,  ..., 40.9383, 56.0417, 37.2371]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 30, Loss: 5573.065185546875\n",
      "tensor([[[[40.2860, 67.0394, 82.9779,  ..., 40.6533, 59.5712, 38.7927]]],\n",
      "\n",
      "\n",
      "        [[[40.2858, 67.0394, 82.9777,  ..., 40.6534, 59.5714, 38.7927]]],\n",
      "\n",
      "\n",
      "        [[[38.8228, 72.5472, 89.5523,  ..., 36.8633, 54.3648, 42.0184]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2859, 67.0396, 82.9779,  ..., 40.6534, 59.5711, 38.7926]]],\n",
      "\n",
      "\n",
      "        [[[41.5855, 68.7315, 86.2849,  ..., 40.1920, 56.7621, 37.9632]]],\n",
      "\n",
      "\n",
      "        [[[41.5855, 68.7315, 86.2849,  ..., 40.1920, 56.7621, 37.9632]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.0546, 72.0340, 88.3615,  ..., 36.4930, 54.0837, 42.3706]]],\n",
      "\n",
      "\n",
      "        [[[39.5427, 66.3823, 81.6491,  ..., 40.3082, 59.5075, 39.2144]]],\n",
      "\n",
      "\n",
      "        [[[39.5427, 66.3825, 81.6493,  ..., 40.3083, 59.5073, 39.2143]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5441, 66.3903, 81.6598,  ..., 40.3086, 59.4969, 39.2104]]],\n",
      "\n",
      "\n",
      "        [[[39.5351, 66.4148, 81.6792,  ..., 40.3173, 59.4714, 39.1982]]],\n",
      "\n",
      "\n",
      "        [[[39.5428, 66.3820, 81.6488,  ..., 40.3081, 59.5079, 39.2145]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.9096, 67.7110, 85.0518,  ..., 40.8881, 55.0170, 38.3610]]],\n",
      "\n",
      "\n",
      "        [[[37.0241, 71.5065, 88.3369,  ..., 37.6241, 52.4581, 42.4026]]],\n",
      "\n",
      "\n",
      "        [[[37.0241, 71.5065, 88.3369,  ..., 37.6241, 52.4581, 42.4026]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.6107, 66.0522, 81.8139,  ..., 41.3412, 57.7822, 39.1821]]],\n",
      "\n",
      "\n",
      "        [[[40.8918, 67.4777, 79.8089,  ..., 40.6605, 51.0489, 33.4377]]],\n",
      "\n",
      "\n",
      "        [[[37.0250, 71.5054, 88.3366,  ..., 37.6235, 52.4590, 42.4030]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.1384, 71.9674, 88.6478,  ..., 38.1794, 52.5411, 41.9949]]],\n",
      "\n",
      "\n",
      "        [[[39.6374, 66.6020, 82.2960,  ..., 41.8421, 57.5928, 38.7570]]],\n",
      "\n",
      "\n",
      "        [[[41.9159, 67.8904, 80.0429,  ..., 41.1843, 51.1404, 33.0808]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.1401, 71.9652, 88.6470,  ..., 38.1781, 52.5429, 41.9958]]],\n",
      "\n",
      "\n",
      "        [[[39.6375, 66.6021, 82.2962,  ..., 41.8420, 57.5926, 38.7569]]],\n",
      "\n",
      "\n",
      "        [[[40.9158, 68.0992, 85.2778,  ..., 41.3940, 55.0922, 38.0094]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 31, Loss: 5100.092956542969\n",
      "tensor([[[[43.7834, 68.5518, 80.3998,  ..., 40.6386, 51.7170, 32.9689]]],\n",
      "\n",
      "\n",
      "        [[[41.4487, 67.3254, 82.7840,  ..., 41.3143, 57.9813, 38.6008]]],\n",
      "\n",
      "\n",
      "        [[[41.4477, 67.3235, 82.7810,  ..., 41.3146, 57.9840, 38.6017]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.1595, 72.7021, 89.0939,  ..., 37.5859, 53.1448, 41.8493]]],\n",
      "\n",
      "\n",
      "        [[[41.4486, 67.3257, 82.7843,  ..., 41.3144, 57.9810, 38.6007]]],\n",
      "\n",
      "\n",
      "        [[[41.4493, 67.3261, 82.7855,  ..., 41.3141, 57.9802, 38.6004]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 41.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 41.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 44.61it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[42.9506, 68.9532, 85.6361,  ..., 41.2214, 55.1782, 37.9734]]],\n",
      "\n",
      "\n",
      "        [[[41.6392, 67.5177, 82.7802,  ..., 41.6778, 57.5281, 38.6935]]],\n",
      "\n",
      "\n",
      "        [[[40.3988, 72.9767, 89.1642,  ..., 37.9959, 52.6361, 41.9149]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.6375, 67.5153, 82.7759,  ..., 41.6783, 57.5318, 38.6947]]],\n",
      "\n",
      "\n",
      "        [[[40.3986, 72.9770, 89.1643,  ..., 37.9962, 52.6358, 41.9147]]],\n",
      "\n",
      "\n",
      "        [[[41.6383, 67.5156, 82.7771,  ..., 41.6780, 57.5310, 38.6946]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.9890, 66.9763, 82.5876,  ..., 41.9884, 57.6307, 37.9661]]],\n",
      "\n",
      "\n",
      "        [[[40.9884, 66.9747, 82.5852,  ..., 41.9884, 57.6330, 37.9669]]],\n",
      "\n",
      "\n",
      "        [[[39.7453, 72.5931, 89.2388,  ..., 38.3540, 52.5358, 40.9894]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.3711, 68.4101, 80.4213,  ..., 41.3676, 51.1790, 32.2142]]],\n",
      "\n",
      "\n",
      "        [[[43.3720, 68.4091, 80.4210,  ..., 41.3670, 51.1798, 32.2147]]],\n",
      "\n",
      "\n",
      "        [[[39.7437, 72.5952, 89.2394,  ..., 38.3553, 52.5340, 40.9885]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.5132, 66.0367, 82.3448,  ..., 41.5948, 58.8152, 37.4057]]],\n",
      "\n",
      "\n",
      "        [[[40.5144, 66.0859, 82.4005,  ..., 41.6024, 58.7566, 37.3807]]],\n",
      "\n",
      "\n",
      "        [[[39.2822, 71.8004, 89.2914,  ..., 37.9362, 53.6002, 40.2301]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5131, 66.0366, 82.3445,  ..., 41.5948, 58.8155, 37.4058]]],\n",
      "\n",
      "\n",
      "        [[[39.2836, 71.7988, 89.2908,  ..., 37.9352, 53.6015, 40.2308]]],\n",
      "\n",
      "\n",
      "        [[[40.5123, 66.0347, 82.3416,  ..., 41.5950, 58.8181, 37.4068]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 32, Loss: 4660.500183105469\n",
      "tensor([[[[39.1955, 71.7744, 89.7821,  ..., 36.6187, 54.0325, 40.6317]]],\n",
      "\n",
      "\n",
      "        [[[42.8405, 67.5998, 80.8138,  ..., 39.7612, 52.6080, 31.9300]]],\n",
      "\n",
      "\n",
      "        [[[42.8401, 67.6004, 80.8140,  ..., 39.7615, 52.6077, 31.9297]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.1980, 71.7714, 89.7811,  ..., 36.6168, 54.0349, 40.6330]]],\n",
      "\n",
      "\n",
      "        [[[40.3790, 65.8452, 82.5717,  ..., 40.4002, 59.3510, 37.8578]]],\n",
      "\n",
      "\n",
      "        [[[42.8406, 67.5997, 80.8138,  ..., 39.7611, 52.6081, 31.9300]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.3904, 66.6744, 82.6136,  ..., 40.2656, 58.9817, 38.6937]]],\n",
      "\n",
      "\n",
      "        [[[40.3842, 66.7074, 82.6440,  ..., 40.2744, 58.9463, 38.6771]]],\n",
      "\n",
      "\n",
      "        [[[40.3903, 66.6735, 82.6125,  ..., 40.2655, 58.9829, 38.6942]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.2489, 72.7849, 89.9348,  ..., 36.4758, 53.5836, 41.5117]]],\n",
      "\n",
      "\n",
      "        [[[39.2489, 72.7849, 89.9347,  ..., 36.4757, 53.5836, 41.5117]]],\n",
      "\n",
      "\n",
      "        [[[40.3921, 66.7194, 82.6645,  ..., 40.2721, 58.9288, 38.6710]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.4611, 67.3500, 82.6786,  ..., 40.7536, 58.1255, 38.9428]]],\n",
      "\n",
      "\n",
      "        [[[39.3383, 73.5191, 89.9661,  ..., 37.0240, 52.7326, 41.7929]]],\n",
      "\n",
      "\n",
      "        [[[40.4609, 67.3500, 82.6783,  ..., 40.7537, 58.1257, 38.9428]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.4617, 67.3519, 82.6811,  ..., 40.7536, 58.1231, 38.9419]]],\n",
      "\n",
      "\n",
      "        [[[40.4623, 67.3524, 82.6822,  ..., 40.7534, 58.1223, 38.9416]]],\n",
      "\n",
      "\n",
      "        [[[40.4614, 67.3502, 82.6790,  ..., 40.7535, 58.1252, 38.9427]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.2777, 68.9267, 80.7766,  ..., 40.1702, 51.1261, 32.8356]]],\n",
      "\n",
      "\n",
      "        [[[43.2799, 68.9239, 80.7757,  ..., 40.1687, 51.1281, 32.8367]]],\n",
      "\n",
      "\n",
      "        [[[40.7792, 67.1922, 82.7352,  ..., 40.7702, 57.6569, 38.6747]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7063, 73.2543, 89.9143,  ..., 37.0320, 52.3943, 41.5332]]],\n",
      "\n",
      "\n",
      "        [[[40.7797, 67.1927, 82.7362,  ..., 40.7700, 57.6562, 38.6744]]],\n",
      "\n",
      "\n",
      "        [[[39.7062, 73.2544, 89.9143,  ..., 37.0320, 52.3943, 41.5332]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 33, Loss: 4640.225158691406\n",
      "tensor([[[[39.7005, 72.3917, 89.5025,  ..., 36.4988, 52.2370, 41.6752]]],\n",
      "\n",
      "\n",
      "        [[[40.7626, 66.4650, 82.4326,  ..., 40.2963, 57.3913, 38.7832]]],\n",
      "\n",
      "\n",
      "        [[[43.2618, 68.1037, 80.3408,  ..., 39.6812, 50.9951, 32.9904]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.7625, 66.4641, 82.4315,  ..., 40.2962, 57.3923, 38.7836]]],\n",
      "\n",
      "\n",
      "        [[[39.7019, 72.3900, 89.5019,  ..., 36.4978, 52.2384, 41.6759]]],\n",
      "\n",
      "\n",
      "        [[[40.7626, 66.4654, 82.4329,  ..., 40.2964, 57.3908, 38.7830]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.3417, 67.8174, 79.9898,  ..., 39.4431, 51.3614, 32.7064]]],\n",
      "\n",
      "\n",
      "        [[[39.7970, 72.1053, 89.1825,  ..., 36.2359, 52.6127, 41.3460]]],\n",
      "\n",
      "\n",
      "        [[[40.8246, 66.1797, 82.1154,  ..., 40.0601, 57.7028, 38.5054]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7995, 72.1023, 89.1814,  ..., 36.2340, 52.6152, 41.3474]]],\n",
      "\n",
      "\n",
      "        [[[40.8245, 66.1806, 82.1163,  ..., 40.0603, 57.7018, 38.5050]]],\n",
      "\n",
      "\n",
      "        [[[40.8242, 66.1790, 82.1143,  ..., 40.0602, 57.7038, 38.5058]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.9576, 65.9780, 82.1764,  ..., 40.7438, 57.5004, 38.7769]]],\n",
      "\n",
      "\n",
      "        [[[39.9546, 66.0228, 82.2213,  ..., 40.7531, 57.4515, 38.7542]]],\n",
      "\n",
      "\n",
      "        [[[42.4830, 67.7213, 80.1609,  ..., 40.1648, 51.0689, 32.9388]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9594, 65.9805, 82.1807,  ..., 40.7433, 57.4968, 38.7756]]],\n",
      "\n",
      "\n",
      "        [[[39.9593, 65.9804, 82.1805,  ..., 40.7433, 57.4970, 38.7757]]],\n",
      "\n",
      "\n",
      "        [[[41.4025, 67.8033, 85.4060,  ..., 40.2934, 54.8623, 37.8154]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.3556, 66.1411, 82.7534,  ..., 41.2081, 57.6152, 39.3116]]],\n",
      "\n",
      "\n",
      "        [[[41.8828, 67.9331, 80.7779,  ..., 40.6549, 51.1929, 33.4789]]],\n",
      "\n",
      "\n",
      "        [[[41.8813, 67.9350, 80.7785,  ..., 40.6560, 51.1915, 33.4781]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.2536, 72.2660, 90.1399,  ..., 37.5189, 52.3988, 42.1290]]],\n",
      "\n",
      "\n",
      "        [[[39.3559, 66.1418, 82.7544,  ..., 41.2080, 57.6143, 39.3113]]],\n",
      "\n",
      "\n",
      "        [[[38.2465, 72.2747, 90.1430,  ..., 37.5243, 52.3916, 42.1251]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 34, Loss: 4234.66845703125\n",
      "tensor([[[[40.4124, 66.8178, 83.1529,  ..., 40.6129, 58.9482, 39.2095]]],\n",
      "\n",
      "\n",
      "        [[[40.4083, 66.8595, 83.1930,  ..., 40.6224, 58.9041, 39.1884]]],\n",
      "\n",
      "\n",
      "        [[[40.4124, 66.8180, 83.1531,  ..., 40.6129, 58.9480, 39.2094]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.4125, 66.8173, 83.1525,  ..., 40.6128, 58.9487, 39.2097]]],\n",
      "\n",
      "\n",
      "        [[[40.4124, 66.8178, 83.1529,  ..., 40.6129, 58.9482, 39.2094]]],\n",
      "\n",
      "\n",
      "        [[[40.4124, 66.8176, 83.1527,  ..., 40.6129, 58.9484, 39.2095]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.7906, 69.1332, 80.7504,  ..., 39.5915, 52.3515, 32.8988]]],\n",
      "\n",
      "\n",
      "        [[[43.7902, 69.1336, 80.7506,  ..., 39.5918, 52.3512, 32.8986]]],\n",
      "\n",
      "\n",
      "        [[[41.1514, 67.2123, 82.6805,  ..., 40.1680, 58.7286, 38.7804]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.6615, 69.1519, 85.9824,  ..., 39.7212, 56.0741, 37.7556]]],\n",
      "\n",
      "\n",
      "        [[[41.1513, 67.2122, 82.6804,  ..., 40.1681, 58.7287, 38.7804]]],\n",
      "\n",
      "\n",
      "        [[[40.3208, 73.5993, 90.2175,  ..., 36.3623, 53.6032, 41.4580]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.9817, 68.9235, 85.1683,  ..., 40.1727, 53.9226, 37.1996]]],\n",
      "\n",
      "\n",
      "        [[[40.4658, 66.9598, 81.8618,  ..., 40.6090, 56.6168, 38.2528]]],\n",
      "\n",
      "\n",
      "        [[[40.4620, 67.0001, 81.9000,  ..., 40.6182, 56.5746, 38.2324]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.4665, 66.9605, 81.8630,  ..., 40.6088, 56.6158, 38.2525]]],\n",
      "\n",
      "\n",
      "        [[[40.4659, 66.9600, 81.8621,  ..., 40.6090, 56.6165, 38.2527]]],\n",
      "\n",
      "\n",
      "        [[[40.4659, 66.9598, 81.8619,  ..., 40.6089, 56.6167, 38.2528]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.3249, 67.7602, 79.5458,  ..., 40.4548, 49.6133, 31.8847]]],\n",
      "\n",
      "\n",
      "        [[[42.3237, 67.7618, 79.5462,  ..., 40.4558, 49.6122, 31.8840]]],\n",
      "\n",
      "\n",
      "        [[[39.7109, 65.8966, 81.5889,  ..., 40.9948, 55.9712, 37.7836]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.3252, 67.7599, 79.5456,  ..., 40.4546, 49.6136, 31.8849]]],\n",
      "\n",
      "\n",
      "        [[[41.2104, 67.7852, 84.8012,  ..., 40.5626, 53.3644, 36.7487]]],\n",
      "\n",
      "\n",
      "        [[[42.3245, 67.7608, 79.5459,  ..., 40.4552, 49.6129, 31.8845]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 35, Loss: 3817.0723266601562\n",
      "tensor([[[[40.2685, 64.5196, 81.7137,  ..., 40.4256, 58.3923, 37.5405]]],\n",
      "\n",
      "\n",
      "        [[[40.2685, 64.5207, 81.7149,  ..., 40.4258, 58.3912, 37.5399]]],\n",
      "\n",
      "\n",
      "        [[[41.7906, 66.4088, 84.9362,  ..., 39.9890, 55.8247, 36.4868]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2684, 64.5205, 81.7146,  ..., 40.4258, 58.3914, 37.5400]]],\n",
      "\n",
      "\n",
      "        [[[40.2686, 64.5205, 81.7147,  ..., 40.4257, 58.3914, 37.5400]]],\n",
      "\n",
      "\n",
      "        [[[40.2685, 64.5207, 81.7149,  ..., 40.4258, 58.3911, 37.5399]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.2171, 64.4650, 82.2275,  ..., 40.5673, 59.4077, 38.4349]]],\n",
      "\n",
      "\n",
      "        [[[41.7514, 66.3697, 85.4591,  ..., 40.1359, 56.8643, 37.3825]]],\n",
      "\n",
      "\n",
      "        [[[41.7514, 66.3697, 85.4591,  ..., 40.1359, 56.8643, 37.3825]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2181, 64.4660, 82.2294,  ..., 40.5669, 59.4063, 38.4343]]],\n",
      "\n",
      "\n",
      "        [[[39.4040, 70.6426, 89.8354,  ..., 36.7855, 54.4681, 40.9962]]],\n",
      "\n",
      "\n",
      "        [[[42.8942, 66.3358, 80.2185,  ..., 40.0230, 53.2212, 32.5445]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.9355, 65.8812, 82.4318,  ..., 41.5557, 58.6782, 39.6299]]],\n",
      "\n",
      "\n",
      "        [[[39.1276, 72.2978, 90.1550,  ..., 37.8892, 53.6180, 42.2799]]],\n",
      "\n",
      "\n",
      "        [[[39.9289, 65.9131, 82.4584,  ..., 41.5650, 58.6469, 39.6138]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9349, 65.8803, 82.4304,  ..., 41.5558, 58.6793, 39.6303]]],\n",
      "\n",
      "\n",
      "        [[[39.9349, 65.8803, 82.4304,  ..., 41.5558, 58.6793, 39.6304]]],\n",
      "\n",
      "\n",
      "        [[[39.9353, 65.8809, 82.4313,  ..., 41.5557, 58.6785, 39.6300]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.9956, 67.1608, 82.3675,  ..., 41.5030, 57.8158, 40.2146]]],\n",
      "\n",
      "\n",
      "        [[[39.2355, 73.7955, 90.1712,  ..., 37.8293, 52.6261, 42.8875]]],\n",
      "\n",
      "\n",
      "        [[[39.2355, 73.7956, 90.1713,  ..., 37.8293, 52.6261, 42.8875]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9959, 67.1615, 82.3685,  ..., 41.5029, 57.8149, 40.2143]]],\n",
      "\n",
      "\n",
      "        [[[41.5692, 69.2062, 85.6799,  ..., 41.0979, 55.1916, 39.1322]]],\n",
      "\n",
      "\n",
      "        [[[39.2355, 73.7956, 90.1713,  ..., 37.8293, 52.6261, 42.8875]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 48.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 36.34it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 3603.4415893554688\n",
      "tensor([[[[39.5871, 74.2691, 90.6597,  ..., 37.3210, 52.4771, 42.0275]]],\n",
      "\n",
      "\n",
      "        [[[40.3000, 67.6790, 82.9033,  ..., 41.0678, 57.5379, 39.4028]]],\n",
      "\n",
      "\n",
      "        [[[43.0401, 69.6843, 80.8597,  ..., 40.5485, 51.4242, 33.5642]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.3027, 67.6355, 82.8624,  ..., 41.0581, 57.5819, 39.4250]]],\n",
      "\n",
      "\n",
      "        [[[41.8743, 69.6225, 86.0837,  ..., 40.6428, 55.0563, 38.3609]]],\n",
      "\n",
      "\n",
      "        [[[40.3016, 67.6331, 82.8591,  ..., 41.0583, 57.5847, 39.4262]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.4043, 66.6518, 82.3740,  ..., 41.0326, 58.0071, 38.3772]]],\n",
      "\n",
      "\n",
      "        [[[42.0017, 68.6732, 85.6281,  ..., 40.6211, 55.4597, 37.2675]]],\n",
      "\n",
      "\n",
      "        [[[40.3988, 66.6855, 82.4027,  ..., 41.0418, 57.9742, 38.3600]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.1748, 68.7225, 80.3954,  ..., 40.5299, 51.8522, 32.4562]]],\n",
      "\n",
      "\n",
      "        [[[40.4045, 66.6527, 82.3750,  ..., 41.0327, 58.0062, 38.3767]]],\n",
      "\n",
      "\n",
      "        [[[43.1735, 68.7241, 80.3959,  ..., 40.5309, 51.8511, 32.4555]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.7631, 72.4816, 89.7340,  ..., 38.2738, 53.0403, 40.4158]]],\n",
      "\n",
      "\n",
      "        [[[40.3904, 65.8587, 81.8320,  ..., 41.9069, 58.1896, 38.0773]]],\n",
      "\n",
      "\n",
      "        [[[40.3888, 65.8565, 81.8287,  ..., 41.9072, 58.1923, 38.0785]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.1955, 67.9877, 79.9083,  ..., 41.4485, 51.9964, 32.1082]]],\n",
      "\n",
      "\n",
      "        [[[40.3762, 65.8530, 81.8153,  ..., 41.9127, 58.1994, 38.0809]]],\n",
      "\n",
      "\n",
      "        [[[39.7713, 72.4716, 89.7305,  ..., 38.2677, 53.0485, 40.4202]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.6318, 65.4434, 81.6813,  ..., 41.8526, 58.4619, 38.7130]]],\n",
      "\n",
      "\n",
      "        [[[42.3188, 67.6842, 85.1714,  ..., 41.4710, 55.7107, 37.4756]]],\n",
      "\n",
      "\n",
      "        [[[43.5082, 67.7314, 79.9286,  ..., 41.4110, 52.1388, 32.6849]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.5088, 67.7307, 79.9284,  ..., 41.4106, 52.1393, 32.6852]]],\n",
      "\n",
      "\n",
      "        [[[40.6314, 65.4431, 81.6807,  ..., 41.8527, 58.4623, 38.7132]]],\n",
      "\n",
      "\n",
      "        [[[40.1153, 72.2113, 89.7999,  ..., 38.2180, 53.1790, 41.0179]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 37, Loss: 3308.9151611328125\n",
      "tensor([[[[40.8706, 66.4743, 82.8375,  ..., 41.3929, 58.4443, 38.9801]]],\n",
      "\n",
      "\n",
      "        [[[40.3967, 73.3017, 91.0228,  ..., 37.6943, 53.2732, 41.3416]]],\n",
      "\n",
      "\n",
      "        [[[43.7605, 68.7356, 81.0246,  ..., 40.9332, 52.2440, 33.0044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.8654, 66.4265, 82.7873,  ..., 41.3860, 58.4938, 39.0050]]],\n",
      "\n",
      "\n",
      "        [[[42.5593, 68.6586, 86.2466,  ..., 40.9927, 55.7993, 37.7812]]],\n",
      "\n",
      "\n",
      "        [[[43.7614, 68.7346, 81.0243,  ..., 40.9325, 52.2448, 33.0048]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.2050, 68.9745, 81.6424,  ..., 40.1771, 51.6837, 33.0240]]],\n",
      "\n",
      "\n",
      "        [[[39.8102, 73.5696, 91.7303,  ..., 36.8716, 52.6547, 41.3437]]],\n",
      "\n",
      "\n",
      "        [[[40.3056, 66.6314, 83.3836,  ..., 40.6482, 57.9267, 39.0291]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.0037, 68.8820, 86.8513,  ..., 40.2419, 55.2337, 37.7942]]],\n",
      "\n",
      "\n",
      "        [[[43.2060, 68.9731, 81.6420,  ..., 40.1764, 51.6844, 33.0245]]],\n",
      "\n",
      "\n",
      "        [[[40.3056, 66.6314, 83.3836,  ..., 40.6482, 57.9267, 39.0291]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.8421, 69.2894, 81.3868,  ..., 40.1587, 51.1389, 32.5783]]],\n",
      "\n",
      "\n",
      "        [[[41.6358, 69.1861, 86.6002,  ..., 40.2134, 54.6822, 37.3482]]],\n",
      "\n",
      "\n",
      "        [[[42.8417, 69.2900, 81.3870,  ..., 40.1590, 51.1386, 32.5781]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9564, 67.0120, 83.2522,  ..., 40.6287, 57.2679, 38.5515]]],\n",
      "\n",
      "\n",
      "        [[[42.8416, 69.2901, 81.3870,  ..., 40.1591, 51.1385, 32.5780]]],\n",
      "\n",
      "\n",
      "        [[[39.9567, 67.0123, 83.2527,  ..., 40.6285, 57.2675, 38.5513]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.5970, 72.8088, 89.3691,  ..., 36.8108, 52.4796, 40.8162]]],\n",
      "\n",
      "\n",
      "        [[[39.5958, 72.8103, 89.3696,  ..., 36.8117, 52.4784, 40.8155]]],\n",
      "\n",
      "\n",
      "        [[[41.7757, 68.1515, 84.6126,  ..., 40.1825, 55.0772, 37.3313]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.0776, 65.9697, 81.2893,  ..., 40.5981, 57.6550, 38.5514]]],\n",
      "\n",
      "\n",
      "        [[[42.9931, 68.2409, 79.3610,  ..., 40.1367, 51.5581, 32.5699]]],\n",
      "\n",
      "\n",
      "        [[[40.0734, 65.9999, 81.3147,  ..., 40.6062, 57.6264, 38.5359]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 38, Loss: 3115.532470703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 40.63it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 49.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 36.82it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[39.9136, 71.9467, 88.1835,  ..., 36.8630, 53.0436, 40.8770]]],\n",
      "\n",
      "\n",
      "        [[[43.2866, 67.4190, 78.2124,  ..., 40.1893, 52.1073, 32.6477]]],\n",
      "\n",
      "\n",
      "        [[[43.2855, 67.4204, 78.2129,  ..., 40.1901, 52.1062, 32.6471]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.2867, 67.4189, 78.2124,  ..., 40.1892, 52.1073, 32.6477]]],\n",
      "\n",
      "\n",
      "        [[[42.0561, 67.3392, 83.4876,  ..., 40.2256, 55.6009, 37.3994]]],\n",
      "\n",
      "\n",
      "        [[[40.3529, 65.2325, 80.2668,  ..., 40.6500, 58.0796, 38.5894]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.1152, 65.0121, 80.6320,  ..., 40.8312, 58.1672, 38.7894]]],\n",
      "\n",
      "\n",
      "        [[[43.0513, 67.2039, 78.5703,  ..., 40.3866, 52.2374, 32.8560]]],\n",
      "\n",
      "\n",
      "        [[[41.8176, 67.1220, 83.8404,  ..., 40.4137, 55.7138, 37.5967]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.1149, 65.0120, 80.6316,  ..., 40.8314, 58.1674, 38.7895]]],\n",
      "\n",
      "\n",
      "        [[[41.8176, 67.1220, 83.8404,  ..., 40.4137, 55.7138, 37.5967]]],\n",
      "\n",
      "\n",
      "        [[[40.1149, 65.0120, 80.6316,  ..., 40.8314, 58.1674, 38.7895]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.3393, 66.3971, 82.5554,  ..., 40.7337, 58.1511, 38.8763]]],\n",
      "\n",
      "\n",
      "        [[[42.0044, 68.3574, 85.5792,  ..., 40.3038, 55.9028, 37.7731]]],\n",
      "\n",
      "\n",
      "        [[[39.8893, 73.0884, 90.5493,  ..., 36.9370, 53.3746, 41.2664]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.8893, 73.0884, 90.5493,  ..., 36.9370, 53.3745, 41.2664]]],\n",
      "\n",
      "\n",
      "        [[[40.3386, 66.3960, 82.5539,  ..., 40.7338, 58.1523, 38.8770]]],\n",
      "\n",
      "\n",
      "        [[[40.3381, 66.3945, 82.5522,  ..., 40.7338, 58.1538, 38.8777]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.2489, 66.8535, 83.0372,  ..., 39.6339, 58.7025, 39.3334]]],\n",
      "\n",
      "\n",
      "        [[[42.9853, 68.9749, 86.2297,  ..., 39.1978, 56.3246, 38.1517]]],\n",
      "\n",
      "\n",
      "        [[[41.2502, 66.8557, 83.0401,  ..., 39.6337, 58.7001, 39.3321]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.9810, 73.7828, 91.2981,  ..., 35.7136, 53.8394, 41.6716]]],\n",
      "\n",
      "\n",
      "        [[[40.9826, 73.7809, 91.2974,  ..., 35.7124, 53.8410, 41.6725]]],\n",
      "\n",
      "\n",
      "        [[[40.9830, 73.7803, 91.2972,  ..., 35.7120, 53.8414, 41.6727]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 39, Loss: 2876.5023193359375\n",
      "tensor([[[[40.6914, 66.4769, 82.2718,  ..., 40.0943, 57.5915, 39.7751]]],\n",
      "\n",
      "\n",
      "        [[[40.6920, 66.4773, 82.2726,  ..., 40.0940, 57.5910, 39.7749]]],\n",
      "\n",
      "\n",
      "        [[[40.6933, 66.4801, 82.2761,  ..., 40.0940, 57.5880, 39.7733]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.6916, 66.4770, 82.2721,  ..., 40.0942, 57.5913, 39.7750]]],\n",
      "\n",
      "\n",
      "        [[[40.6913, 66.4770, 82.2717,  ..., 40.0944, 57.5915, 39.7751]]],\n",
      "\n",
      "\n",
      "        [[[40.4827, 73.6819, 90.8069,  ..., 36.2371, 52.3177, 41.9914]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.2038, 65.6039, 81.8012,  ..., 41.6694, 56.4267, 39.4648]]],\n",
      "\n",
      "\n",
      "        [[[39.2037, 65.6035, 81.8009,  ..., 41.6693, 56.4270, 39.4650]]],\n",
      "\n",
      "\n",
      "        [[[39.2038, 65.6034, 81.8008,  ..., 41.6693, 56.4271, 39.4650]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.2037, 65.6038, 81.8011,  ..., 41.6694, 56.4267, 39.4648]]],\n",
      "\n",
      "\n",
      "        [[[42.2976, 68.2380, 80.1401,  ..., 41.2968, 50.1978, 33.3650]]],\n",
      "\n",
      "\n",
      "        [[[38.8924, 72.8647, 90.4519,  ..., 37.9993, 50.9066, 41.5601]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.2361, 67.7592, 85.7493,  ..., 41.3071, 55.1564, 37.3313]]],\n",
      "\n",
      "\n",
      "        [[[42.5007, 67.8972, 80.5055,  ..., 41.3065, 51.7498, 32.6224]]],\n",
      "\n",
      "\n",
      "        [[[39.1180, 72.5054, 90.8826,  ..., 38.0016, 52.5560, 40.7457]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.3863, 65.2857, 82.1953,  ..., 41.6774, 57.8588, 38.7348]]],\n",
      "\n",
      "\n",
      "        [[[39.3868, 65.2867, 82.1965,  ..., 41.6774, 57.8578, 38.7343]]],\n",
      "\n",
      "\n",
      "        [[[39.3855, 65.2841, 82.1932,  ..., 41.6776, 57.8605, 38.7357]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.7246, 65.5512, 82.3342,  ..., 40.5023, 59.7283, 38.2668]]],\n",
      "\n",
      "\n",
      "        [[[40.7245, 65.5513, 82.3343,  ..., 40.5024, 59.7282, 38.2667]]],\n",
      "\n",
      "\n",
      "        [[[40.7315, 65.5942, 82.3777,  ..., 40.5081, 59.6871, 38.2438]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.6551, 72.8647, 91.1062,  ..., 36.6900, 54.6234, 40.1898]]],\n",
      "\n",
      "\n",
      "        [[[40.7273, 65.5592, 82.3433,  ..., 40.5027, 59.7204, 38.2624]]],\n",
      "\n",
      "\n",
      "        [[[43.9253, 68.2230, 80.6774,  ..., 40.0923, 53.6925, 32.1206]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 40, Loss: 2758.91357421875\n",
      "tensor([[[[43.4686, 69.3049, 80.5845,  ..., 41.3731, 52.3662, 32.5912]]],\n",
      "\n",
      "\n",
      "        [[[40.2913, 66.6725, 82.3602,  ..., 41.7408, 58.3188, 38.6683]]],\n",
      "\n",
      "\n",
      "        [[[42.1658, 69.1241, 85.8238,  ..., 41.3659, 55.7302, 37.2799]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.4700, 69.3031, 80.5838,  ..., 41.3720, 52.3675, 32.5919]]],\n",
      "\n",
      "\n",
      "        [[[43.4694, 69.3039, 80.5841,  ..., 41.3725, 52.3669, 32.5916]]],\n",
      "\n",
      "\n",
      "        [[[40.1711, 74.0346, 91.0441,  ..., 38.0576, 53.1805, 40.6787]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 47.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 36.64it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[42.0827, 69.2022, 85.3468,  ..., 41.5602, 54.6802, 37.9146]]],\n",
      "\n",
      "\n",
      "        [[[40.0955, 74.1349, 90.5611,  ..., 38.2643, 52.0274, 41.3662]]],\n",
      "\n",
      "\n",
      "        [[[40.1980, 66.7520, 81.9139,  ..., 41.9342, 57.2553, 39.2945]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.3928, 69.3899, 80.0984,  ..., 41.5735, 51.3108, 33.2480]]],\n",
      "\n",
      "\n",
      "        [[[40.1987, 66.7527, 81.9150,  ..., 41.9340, 57.2545, 39.2941]]],\n",
      "\n",
      "\n",
      "        [[[40.1980, 66.7511, 81.9131,  ..., 41.9339, 57.2561, 39.2950]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.9662, 68.2952, 79.5663,  ..., 39.8787, 51.9993, 33.3232]]],\n",
      "\n",
      "\n",
      "        [[[42.6335, 68.1228, 84.8235,  ..., 39.8968, 55.3362, 37.9796]]],\n",
      "\n",
      "\n",
      "        [[[42.6335, 68.1228, 84.8235,  ..., 39.8968, 55.3362, 37.9796]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.6808, 65.5423, 81.2567,  ..., 40.2890, 58.0211, 39.4415]]],\n",
      "\n",
      "\n",
      "        [[[40.6671, 65.5315, 81.2381,  ..., 40.2933, 58.0331, 39.4479]]],\n",
      "\n",
      "\n",
      "        [[[42.6335, 68.1228, 84.8235,  ..., 39.8968, 55.3362, 37.9796]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.7530, 68.2336, 79.6467,  ..., 40.2107, 52.4424, 32.6645]]],\n",
      "\n",
      "\n",
      "        [[[43.7529, 68.2338, 79.6468,  ..., 40.2108, 52.4423, 32.6643]]],\n",
      "\n",
      "\n",
      "        [[[40.4647, 65.5079, 81.3891,  ..., 40.6085, 58.3737, 38.7847]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.4656, 65.5090, 81.3906,  ..., 40.6083, 58.3725, 38.7840]]],\n",
      "\n",
      "\n",
      "        [[[42.4156, 68.0561, 84.9024,  ..., 40.2175, 55.7557, 37.3247]]],\n",
      "\n",
      "\n",
      "        [[[40.4648, 65.5408, 81.4180,  ..., 40.6156, 58.3439, 38.7674]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 41, Loss: 2631.0125122070312\n",
      "tensor([[[[41.6484, 68.4326, 85.2882,  ..., 40.8772, 54.9606, 37.2233]]],\n",
      "\n",
      "\n",
      "        [[[39.7409, 66.0386, 81.9557,  ..., 41.2758, 57.4093, 38.6025]]],\n",
      "\n",
      "\n",
      "        [[[39.7406, 66.0018, 81.9232,  ..., 41.2680, 57.4425, 38.6219]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7395, 65.9994, 81.9204,  ..., 41.2680, 57.4448, 38.6232]]],\n",
      "\n",
      "\n",
      "        [[[42.9799, 68.6251, 80.0374,  ..., 40.8899, 51.6454, 32.5710]]],\n",
      "\n",
      "\n",
      "        [[[39.6696, 73.3364, 90.6070,  ..., 37.5083, 52.3311, 40.5923]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.2279, 68.3078, 85.1461,  ..., 39.4642, 55.3620, 37.4658]]],\n",
      "\n",
      "\n",
      "        [[[40.3229, 73.2123, 90.4822,  ..., 35.9538, 52.7722, 40.8553]]],\n",
      "\n",
      "\n",
      "        [[[40.3237, 73.2113, 90.4818,  ..., 35.9532, 52.7730, 40.8557]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2985, 65.8707, 81.7933,  ..., 39.8780, 57.8187, 38.8687]]],\n",
      "\n",
      "\n",
      "        [[[40.2841, 65.8630, 81.7774,  ..., 39.8833, 57.8274, 38.8736]]],\n",
      "\n",
      "\n",
      "        [[[42.2279, 68.3078, 85.1462,  ..., 39.4642, 55.3620, 37.4658]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.6550, 66.3896, 82.5283,  ..., 39.9268, 58.0337, 38.6654]]],\n",
      "\n",
      "\n",
      "        [[[43.9174, 68.8874, 80.4548,  ..., 39.4899, 52.4879, 32.7087]]],\n",
      "\n",
      "\n",
      "        [[[42.5482, 68.6790, 85.7066,  ..., 39.5044, 55.7551, 37.3408]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.9184, 68.8862, 80.4544,  ..., 39.4892, 52.4887, 32.7092]]],\n",
      "\n",
      "\n",
      "        [[[40.6552, 66.3910, 82.5297,  ..., 39.9271, 58.0324, 38.6646]]],\n",
      "\n",
      "\n",
      "        [[[40.6546, 66.3890, 82.5276,  ..., 39.9269, 58.0343, 38.6657]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.2654, 66.1361, 82.3910,  ..., 41.0981, 57.7539, 38.6832]]],\n",
      "\n",
      "\n",
      "        [[[40.3043, 73.4414, 91.0711,  ..., 37.2948, 52.8440, 40.6782]]],\n",
      "\n",
      "\n",
      "        [[[40.2654, 66.1351, 82.3901,  ..., 41.0979, 57.7548, 38.6838]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.1874, 68.4890, 85.6238,  ..., 40.6992, 55.4311, 37.3193]]],\n",
      "\n",
      "\n",
      "        [[[42.1874, 68.4890, 85.6238,  ..., 40.6992, 55.4311, 37.3193]]],\n",
      "\n",
      "\n",
      "        [[[43.5562, 68.7016, 80.3673,  ..., 40.7084, 52.1711, 32.6935]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 42, Loss: 2516.2863159179688\n",
      "tensor([[[[43.0414, 68.1390, 79.6459,  ..., 40.9999, 51.6593, 33.5564]]],\n",
      "\n",
      "\n",
      "        [[[39.6452, 65.2950, 81.3995,  ..., 41.3593, 57.4955, 39.6686]]],\n",
      "\n",
      "\n",
      "        [[[39.7546, 72.8471, 90.3332,  ..., 37.6023, 52.2766, 41.5890]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7547, 72.8470, 90.3332,  ..., 37.6023, 52.2766, 41.5890]]],\n",
      "\n",
      "\n",
      "        [[[43.0414, 68.1390, 79.6459,  ..., 40.9999, 51.6593, 33.5564]]],\n",
      "\n",
      "\n",
      "        [[[39.7561, 72.8452, 90.3325,  ..., 37.6012, 52.2781, 41.5898]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.6225, 73.2255, 90.9137,  ..., 37.9034, 52.6130, 41.9352]]],\n",
      "\n",
      "\n",
      "        [[[39.4859, 65.5759, 81.8580,  ..., 41.6308, 57.8127, 40.0106]]],\n",
      "\n",
      "\n",
      "        [[[41.5356, 68.2653, 85.4156,  ..., 41.2655, 55.2167, 38.4725]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.9117, 68.4859, 80.1522,  ..., 41.2875, 51.9860, 33.8918]]],\n",
      "\n",
      "\n",
      "        [[[39.4857, 65.5745, 81.8567,  ..., 41.6306, 57.8140, 40.0113]]],\n",
      "\n",
      "\n",
      "        [[[39.4865, 65.5766, 81.8589,  ..., 41.6307, 57.8121, 40.0102]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 37.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 48.34it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[41.5625, 68.6540, 86.2156,  ..., 40.9736, 55.6910, 38.0922]]],\n",
      "\n",
      "\n",
      "        [[[41.5625, 68.6540, 86.2156,  ..., 40.9736, 55.6910, 38.0922]]],\n",
      "\n",
      "\n",
      "        [[[39.5303, 66.0438, 82.7568,  ..., 41.3513, 58.1712, 39.5938]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5292, 66.0415, 82.7542,  ..., 41.3513, 58.1733, 39.5951]]],\n",
      "\n",
      "\n",
      "        [[[39.6633, 73.6636, 91.8273,  ..., 37.5814, 53.1309, 41.5147]]],\n",
      "\n",
      "\n",
      "        [[[39.5292, 66.0415, 82.7541,  ..., 41.3513, 58.1734, 39.5951]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.2326, 65.5238, 82.1709,  ..., 40.2668, 58.4757, 38.5160]]],\n",
      "\n",
      "\n",
      "        [[[43.7282, 68.4493, 80.4432,  ..., 39.8758, 52.7566, 32.3502]]],\n",
      "\n",
      "\n",
      "        [[[40.5050, 73.2029, 91.2990,  ..., 36.3782, 53.4037, 40.2501]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5070, 73.2005, 91.2982,  ..., 36.3767, 53.4057, 40.2511]]],\n",
      "\n",
      "\n",
      "        [[[40.2324, 65.5236, 82.1706,  ..., 40.2669, 58.4759, 38.5161]]],\n",
      "\n",
      "\n",
      "        [[[40.5095, 73.1974, 91.2971,  ..., 36.3747, 53.4082, 40.2524]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 43, Loss: 2385.2806396484375\n",
      "tensor([[[[40.0640, 65.1579, 81.1532,  ..., 40.7783, 57.2513, 38.3663]]],\n",
      "\n",
      "\n",
      "        [[[42.1873, 67.9127, 84.7089,  ..., 40.3998, 54.6643, 36.7483]]],\n",
      "\n",
      "\n",
      "        [[[42.1873, 67.9126, 84.7089,  ..., 40.3998, 54.6643, 36.7483]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.0637, 65.1578, 81.1529,  ..., 40.7784, 57.2515, 38.3664]]],\n",
      "\n",
      "\n",
      "        [[[40.3688, 72.8802, 90.2536,  ..., 36.9511, 52.0019, 40.0378]]],\n",
      "\n",
      "\n",
      "        [[[40.3733, 72.8746, 90.2516,  ..., 36.9476, 52.0064, 40.0402]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.8907, 68.5206, 78.7912,  ..., 40.3395, 51.1168, 32.5160]]],\n",
      "\n",
      "\n",
      "        [[[40.2970, 65.4363, 80.4670,  ..., 40.6977, 56.9444, 38.7425]]],\n",
      "\n",
      "\n",
      "        [[[43.8896, 68.5220, 78.7917,  ..., 40.3402, 51.1159, 32.5155]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2833, 65.4244, 80.4492,  ..., 40.7015, 56.9557, 38.7497]]],\n",
      "\n",
      "\n",
      "        [[[43.8916, 68.5195, 78.7909,  ..., 40.3388, 51.1176, 32.5165]]],\n",
      "\n",
      "\n",
      "        [[[40.2967, 65.4628, 80.4887,  ..., 40.7039, 56.9218, 38.7285]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.5661, 66.3164, 81.5556,  ..., 40.9439, 57.7191, 38.9819]]],\n",
      "\n",
      "\n",
      "        [[[44.1393, 69.3032, 79.7590,  ..., 40.5881, 52.0882, 32.8465]]],\n",
      "\n",
      "\n",
      "        [[[42.7045, 69.0380, 85.0212,  ..., 40.5618, 55.2465, 37.3967]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.5661, 66.3168, 81.5559,  ..., 40.9440, 57.7188, 38.9817]]],\n",
      "\n",
      "\n",
      "        [[[44.1391, 69.3034, 79.7591,  ..., 40.5882, 52.0880, 32.8463]]],\n",
      "\n",
      "\n",
      "        [[[40.5660, 66.3168, 81.5559,  ..., 40.9441, 57.7188, 38.9817]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.4340, 65.9399, 82.1869,  ..., 40.2195, 58.4020, 39.1256]]],\n",
      "\n",
      "\n",
      "        [[[40.4336, 65.9119, 82.1639,  ..., 40.2134, 58.4253, 39.1403]]],\n",
      "\n",
      "\n",
      "        [[[40.4317, 65.9094, 82.1607,  ..., 40.2137, 58.4275, 39.1418]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.4329, 65.9098, 82.1618,  ..., 40.2132, 58.4271, 39.1415]]],\n",
      "\n",
      "\n",
      "        [[[40.4336, 65.9113, 82.1633,  ..., 40.2132, 58.4259, 39.1407]]],\n",
      "\n",
      "\n",
      "        [[[40.8802, 73.7922, 91.4563,  ..., 36.3097, 53.3664, 40.8633]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 44, Loss: 2334.493408203125\n",
      "tensor([[[[40.0161, 73.3210, 91.6405,  ..., 37.0995, 52.9539, 41.0244]]],\n",
      "\n",
      "\n",
      "        [[[43.2573, 68.5258, 80.6025,  ..., 40.5712, 52.4256, 33.1333]]],\n",
      "\n",
      "\n",
      "        [[[39.6265, 65.4304, 82.2782,  ..., 40.9176, 58.0688, 39.3140]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.0208, 73.3151, 91.6385,  ..., 37.0959, 52.9586, 41.0269]]],\n",
      "\n",
      "\n",
      "        [[[39.6271, 65.4324, 82.2802,  ..., 40.9178, 58.0671, 39.3129]]],\n",
      "\n",
      "\n",
      "        [[[39.6295, 65.4375, 82.2857,  ..., 40.9178, 58.0628, 39.3100]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.0808, 65.0207, 81.6001,  ..., 41.3783, 57.6162, 39.8777]]],\n",
      "\n",
      "\n",
      "        [[[39.0809, 65.0204, 81.5999,  ..., 41.3782, 57.6165, 39.8779]]],\n",
      "\n",
      "\n",
      "        [[[39.0806, 65.0449, 81.6195,  ..., 41.3840, 57.5962, 39.8650]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.4773, 72.9678, 90.9900,  ..., 37.6101, 52.3998, 41.5977]]],\n",
      "\n",
      "\n",
      "        [[[39.4795, 72.9650, 90.9891,  ..., 37.6084, 52.4020, 41.5989]]],\n",
      "\n",
      "\n",
      "        [[[41.3078, 67.9368, 85.2289,  ..., 41.0172, 55.0337, 38.1796]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.0306, 73.5905, 90.6968,  ..., 37.5964, 52.8510, 41.4212]]],\n",
      "\n",
      "\n",
      "        [[[43.2561, 68.7736, 79.6691,  ..., 41.0508, 52.3562, 33.5227]]],\n",
      "\n",
      "\n",
      "        [[[43.2540, 68.7763, 79.6699,  ..., 41.0523, 52.3545, 33.5217]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5778, 65.6410, 81.4033,  ..., 41.3776, 57.9292, 39.6833]]],\n",
      "\n",
      "\n",
      "        [[[43.2570, 68.7724, 79.6687,  ..., 41.0501, 52.3570, 33.5232]]],\n",
      "\n",
      "\n",
      "        [[[39.5773, 65.6405, 81.4026,  ..., 41.3777, 57.9297, 39.6836]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.9928, 65.8443, 81.2782,  ..., 41.0278, 57.8391, 39.0117]]],\n",
      "\n",
      "\n",
      "        [[[40.0073, 65.8553, 81.2949,  ..., 41.0236, 57.8295, 39.0048]]],\n",
      "\n",
      "\n",
      "        [[[40.0075, 65.8569, 81.2963,  ..., 41.0238, 57.8282, 39.0040]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.0178, 65.8997, 81.3360,  ..., 41.0290, 57.7926, 38.9806]]],\n",
      "\n",
      "\n",
      "        [[[42.2389, 68.6833, 84.7733,  ..., 40.6462, 55.3952, 37.3412]]],\n",
      "\n",
      "\n",
      "        [[[40.0059, 65.8543, 81.2934,  ..., 41.0240, 57.8304, 39.0054]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 45, Loss: 2104.832733154297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 47.36it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 32.55it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[39.9030, 65.8259, 81.4105,  ..., 41.1737, 57.0088, 38.4528]]],\n",
      "\n",
      "\n",
      "        [[[40.4078, 73.7325, 90.6303,  ..., 37.3559, 51.9572, 40.0909]]],\n",
      "\n",
      "\n",
      "        [[[42.1284, 68.6028, 84.8210,  ..., 40.7923, 54.6320, 36.8044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.6010, 68.8947, 79.5505,  ..., 40.8360, 51.5567, 32.3070]]],\n",
      "\n",
      "\n",
      "        [[[39.9068, 65.8607, 81.4402,  ..., 41.1801, 56.9803, 38.4340]]],\n",
      "\n",
      "\n",
      "        [[[40.4032, 73.7381, 90.6323,  ..., 37.3594, 51.9527, 40.0885]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.0028, 65.5096, 81.7942,  ..., 40.8818, 56.9674, 38.7985]]],\n",
      "\n",
      "\n",
      "        [[[43.7070, 68.5318, 79.8861,  ..., 40.5329, 51.5893, 32.6962]]],\n",
      "\n",
      "\n",
      "        [[[40.0022, 65.5091, 81.7934,  ..., 40.8819, 56.9678, 38.7988]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.7061, 68.5329, 79.8865,  ..., 40.5335, 51.5885, 32.6958]]],\n",
      "\n",
      "\n",
      "        [[[40.0016, 65.5071, 81.7916,  ..., 40.8817, 56.9694, 38.7999]]],\n",
      "\n",
      "\n",
      "        [[[42.2272, 68.2428, 85.1532,  ..., 40.4906, 54.6500, 37.1745]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.9867, 67.7945, 85.5982,  ..., 39.7032, 55.8858, 37.5705]]],\n",
      "\n",
      "\n",
      "        [[[44.4892, 68.0798, 80.3377,  ..., 39.7340, 52.8672, 33.1122]]],\n",
      "\n",
      "\n",
      "        [[[40.7125, 65.0056, 82.1869,  ..., 40.1057, 58.2104, 39.2256]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.9867, 67.7945, 85.5982,  ..., 39.7032, 55.8859, 37.5705]]],\n",
      "\n",
      "\n",
      "        [[[41.3713, 72.8651, 91.5360,  ..., 36.1592, 53.3332, 40.9319]]],\n",
      "\n",
      "\n",
      "        [[[41.3727, 72.8633, 91.5354,  ..., 36.1581, 53.3346, 40.9326]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.0396, 68.1984, 85.5522,  ..., 40.1256, 56.1007, 37.4675]]],\n",
      "\n",
      "\n",
      "        [[[40.7222, 65.3496, 82.1031,  ..., 40.5231, 58.4405, 39.1574]]],\n",
      "\n",
      "\n",
      "        [[[41.4484, 73.3123, 91.5135,  ..., 36.6156, 53.5743, 40.8225]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.7201, 65.3197, 82.0787,  ..., 40.5170, 58.4643, 39.1733]]],\n",
      "\n",
      "\n",
      "        [[[40.7209, 65.3211, 82.0802,  ..., 40.5170, 58.4631, 39.1725]]],\n",
      "\n",
      "\n",
      "        [[[44.5525, 68.4982, 80.2907,  ..., 40.1691, 53.1018, 33.0192]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 46, Loss: 2021.8815002441406\n",
      "tensor([[[[40.4675, 74.0282, 90.8470,  ..., 37.6417, 52.3796, 40.8366]]],\n",
      "\n",
      "\n",
      "        [[[42.1360, 68.8389, 84.9158,  ..., 41.0636, 55.0147, 37.4817]]],\n",
      "\n",
      "\n",
      "        [[[43.6415, 69.1558, 79.6417,  ..., 41.1272, 52.0087, 33.0454]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.6414, 69.1559, 79.6417,  ..., 41.1273, 52.0086, 33.0453]]],\n",
      "\n",
      "\n",
      "        [[[43.6418, 69.1554, 79.6416,  ..., 41.1271, 52.0089, 33.0456]]],\n",
      "\n",
      "\n",
      "        [[[43.6413, 69.1561, 79.6418,  ..., 41.1274, 52.0085, 33.0453]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.9915, 73.5212, 90.1971,  ..., 36.6240, 51.9042, 40.7444]]],\n",
      "\n",
      "\n",
      "        [[[39.9916, 73.5211, 90.1971,  ..., 36.6240, 51.9043, 40.7444]]],\n",
      "\n",
      "\n",
      "        [[[43.1958, 68.6811, 79.0106,  ..., 40.1866, 51.5799, 32.9700]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.6873, 68.3701, 84.2976,  ..., 40.1427, 54.5784, 37.3968]]],\n",
      "\n",
      "\n",
      "        [[[43.1967, 68.6799, 79.0103,  ..., 40.1860, 51.5806, 32.9705]]],\n",
      "\n",
      "\n",
      "        [[[41.6873, 68.3701, 84.2976,  ..., 40.1427, 54.5784, 37.3968]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.0888, 65.0740, 81.3186,  ..., 40.1302, 57.1926, 38.6027]]],\n",
      "\n",
      "\n",
      "        [[[39.0888, 65.0734, 81.3181,  ..., 40.1300, 57.1931, 38.6030]]],\n",
      "\n",
      "\n",
      "        [[[39.8160, 73.3400, 90.9523,  ..., 36.1845, 52.0246, 40.0246]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.5126, 68.1973, 84.9596,  ..., 39.7461, 54.6863, 36.7391]]],\n",
      "\n",
      "\n",
      "        [[[39.0886, 65.0725, 81.3173,  ..., 40.1299, 57.1938, 38.6035]]],\n",
      "\n",
      "\n",
      "        [[[39.0888, 65.0729, 81.3177,  ..., 40.1299, 57.1935, 38.6033]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.0698, 68.7126, 80.3724,  ..., 40.8817, 52.1137, 32.1575]]],\n",
      "\n",
      "\n",
      "        [[[43.0713, 68.7107, 80.3717,  ..., 40.8805, 52.1150, 32.1583]]],\n",
      "\n",
      "\n",
      "        [[[39.8674, 73.5602, 91.7190,  ..., 37.3572, 52.4532, 39.8508]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.1188, 65.2868, 82.0237,  ..., 41.1877, 57.5270, 38.4345]]],\n",
      "\n",
      "\n",
      "        [[[39.8682, 73.5592, 91.7186,  ..., 37.3565, 52.4541, 39.8513]]],\n",
      "\n",
      "\n",
      "        [[[41.5472, 68.3907, 85.6340,  ..., 40.8203, 55.0740, 36.5771]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 47, Loss: 1863.1806640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 34.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 46.57it/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 27.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[41.0135, 74.0300, 91.6132,  ..., 37.7646, 53.3115, 41.0549]]],\n",
      "\n",
      "\n",
      "        [[[40.1750, 65.8433, 82.0581,  ..., 41.5717, 58.1565, 39.4366]]],\n",
      "\n",
      "\n",
      "        [[[41.0151, 74.0280, 91.6124,  ..., 37.7634, 53.3131, 41.0557]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.5876, 68.8129, 85.5133,  ..., 41.1960, 55.8575, 37.6738]]],\n",
      "\n",
      "\n",
      "        [[[40.1753, 65.8204, 82.0409,  ..., 41.5660, 58.1741, 39.4487]]],\n",
      "\n",
      "\n",
      "        [[[40.1708, 65.8126, 82.0328,  ..., 41.5662, 58.1801, 39.4531]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.3187, 66.2366, 82.1314,  ..., 40.9314, 58.3131, 39.7459]]],\n",
      "\n",
      "\n",
      "        [[[41.3174, 66.2342, 82.1290,  ..., 40.9314, 58.3149, 39.7473]]],\n",
      "\n",
      "\n",
      "        [[[41.3184, 66.2352, 82.1302,  ..., 40.9312, 58.3142, 39.7467]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.3194, 66.2374, 82.1324,  ..., 40.9313, 58.3125, 39.7455]]],\n",
      "\n",
      "\n",
      "        [[[41.3188, 66.2369, 82.1317,  ..., 40.9314, 58.3129, 39.7458]]],\n",
      "\n",
      "\n",
      "        [[[43.7096, 69.1031, 85.4575,  ..., 40.5398, 56.1301, 38.0476]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.6834, 65.7996, 81.5489,  ..., 40.9090, 57.2471, 39.1926]]],\n",
      "\n",
      "\n",
      "        [[[40.6834, 65.7990, 81.5485,  ..., 40.9089, 57.2475, 39.1929]]],\n",
      "\n",
      "\n",
      "        [[[41.5985, 73.9519, 91.0099,  ..., 37.0080, 52.3879, 40.7993]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.6833, 65.7993, 81.5487,  ..., 40.9090, 57.2473, 39.1928]]],\n",
      "\n",
      "\n",
      "        [[[40.6822, 65.7970, 81.5464,  ..., 40.9090, 57.2491, 39.1941]]],\n",
      "\n",
      "\n",
      "        [[[40.6826, 65.7973, 81.5468,  ..., 40.9088, 57.2488, 39.1939]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.8303, 73.2917, 90.7236,  ..., 37.5177, 51.1344, 40.6218]]],\n",
      "\n",
      "\n",
      "        [[[41.4834, 68.1226, 84.6297,  ..., 40.9904, 53.8681, 37.2787]]],\n",
      "\n",
      "\n",
      "        [[[38.9994, 65.0213, 81.1167,  ..., 41.3619, 56.2357, 39.1369]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.8304, 73.2916, 90.7236,  ..., 37.5176, 51.1345, 40.6219]]],\n",
      "\n",
      "\n",
      "        [[[38.9973, 65.0189, 81.1139,  ..., 41.3623, 56.2375, 39.1383]]],\n",
      "\n",
      "\n",
      "        [[[43.0234, 68.4515, 79.3378,  ..., 41.0556, 50.9375, 32.9203]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 48, Loss: 1806.591552734375\n",
      "tensor([[[[39.2292, 64.8424, 81.2062,  ..., 40.4909, 57.8484, 39.6211]]],\n",
      "\n",
      "\n",
      "        [[[41.7789, 68.0381, 84.7964,  ..., 40.1041, 55.4600, 37.7127]]],\n",
      "\n",
      "\n",
      "        [[[39.2285, 64.8214, 81.1903,  ..., 40.4862, 57.8643, 39.6322]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.1750, 73.2003, 90.9323,  ..., 36.5358, 52.8908, 41.1035]]],\n",
      "\n",
      "\n",
      "        [[[39.2307, 64.8259, 81.1947,  ..., 40.4862, 57.8609, 39.6297]]],\n",
      "\n",
      "\n",
      "        [[[43.3350, 68.3694, 79.5074,  ..., 40.1530, 52.5784, 33.3768]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.6042, 68.3346, 85.0790,  ..., 39.7517, 56.6721, 37.5779]]],\n",
      "\n",
      "\n",
      "        [[[40.0017, 65.0575, 81.4371,  ..., 40.1387, 59.0683, 39.5337]]],\n",
      "\n",
      "\n",
      "        [[[41.0900, 73.5347, 91.2712,  ..., 36.1448, 54.2183, 40.9570]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0908, 73.5338, 91.2708,  ..., 36.1442, 54.2190, 40.9574]]],\n",
      "\n",
      "\n",
      "        [[[42.6042, 68.3346, 85.0790,  ..., 39.7517, 56.6721, 37.5779]]],\n",
      "\n",
      "\n",
      "        [[[40.0000, 65.0552, 81.4346,  ..., 40.1389, 59.0699, 39.5351]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.0212, 65.6018, 81.6866,  ..., 41.2353, 57.7750, 38.4530]]],\n",
      "\n",
      "\n",
      "        [[[40.0212, 65.6018, 81.6867,  ..., 41.2353, 57.7749, 38.4529]]],\n",
      "\n",
      "\n",
      "        [[[40.0212, 65.6023, 81.6870,  ..., 41.2354, 57.7745, 38.4527]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.1922, 69.1699, 79.9598,  ..., 40.9268, 52.5859, 32.1815]]],\n",
      "\n",
      "\n",
      "        [[[40.0199, 65.5999, 81.6846,  ..., 41.2354, 57.7763, 38.4541]]],\n",
      "\n",
      "\n",
      "        [[[40.0231, 65.6254, 81.7048,  ..., 41.2402, 57.7574, 38.4403]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.5653, 68.3281, 84.8700,  ..., 40.5970, 54.1365, 36.8014]]],\n",
      "\n",
      "\n",
      "        [[[44.1560, 68.6763, 79.5782,  ..., 40.6542, 51.2692, 32.4856]]],\n",
      "\n",
      "\n",
      "        [[[39.9492, 65.0832, 81.3049,  ..., 40.9737, 56.4956, 38.7587]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9492, 65.0832, 81.3049,  ..., 40.9737, 56.4955, 38.7586]]],\n",
      "\n",
      "\n",
      "        [[[44.1561, 68.6762, 79.5781,  ..., 40.6541, 51.2693, 32.4856]]],\n",
      "\n",
      "\n",
      "        [[[41.0619, 73.5428, 91.0934,  ..., 37.0573, 51.4399, 40.1108]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 49, Loss: 1794.1755676269531\n",
      "tensor([[[[44.5360, 68.4024, 79.8029,  ..., 39.8730, 52.0652, 33.3364]]],\n",
      "\n",
      "\n",
      "        [[[40.2765, 64.7752, 81.5063,  ..., 40.2209, 57.2487, 39.5896]]],\n",
      "\n",
      "\n",
      "        [[[41.4811, 73.2408, 91.3597,  ..., 36.2039, 52.2896, 41.0168]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.9297, 68.0552, 85.0913,  ..., 39.8320, 54.9000, 37.6223]]],\n",
      "\n",
      "\n",
      "        [[[41.4761, 73.2470, 91.3620,  ..., 36.2078, 52.2847, 41.0142]]],\n",
      "\n",
      "\n",
      "        [[[44.5373, 68.4008, 79.8024,  ..., 39.8720, 52.0663, 33.3371]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.7673, 65.0814, 82.1026,  ..., 41.4199, 58.1317, 39.9393]]],\n",
      "\n",
      "\n",
      "        [[[44.0558, 68.7812, 80.4524,  ..., 41.1234, 52.9868, 33.6809]]],\n",
      "\n",
      "\n",
      "        [[[39.7655, 65.0942, 82.1110,  ..., 41.4238, 58.1223, 39.9327]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.9655, 73.6504, 92.0864,  ..., 37.5406, 53.2645, 41.3736]]],\n",
      "\n",
      "\n",
      "        [[[42.4479, 68.4234, 85.7278,  ..., 41.0561, 55.7863, 37.9459]]],\n",
      "\n",
      "\n",
      "        [[[39.7626, 65.0722, 82.0938,  ..., 41.4197, 58.1382, 39.9445]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.7717, 74.9644, 92.5083,  ..., 38.4060, 53.3977, 41.3366]]],\n",
      "\n",
      "\n",
      "        [[[40.7717, 74.9644, 92.5083,  ..., 38.4060, 53.3977, 41.3366]]],\n",
      "\n",
      "\n",
      "        [[[40.7717, 74.9644, 92.5083,  ..., 38.4060, 53.3977, 41.3366]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5839, 66.2840, 82.5055,  ..., 42.2036, 58.1983, 39.8877]]],\n",
      "\n",
      "\n",
      "        [[[43.8718, 70.0051, 80.8209,  ..., 41.9381, 53.1228, 33.6563]]],\n",
      "\n",
      "\n",
      "        [[[43.8719, 70.0049, 80.8208,  ..., 41.9379, 53.1230, 33.6563]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.3772, 65.9843, 81.7071,  ..., 40.0608, 57.9686, 39.7512]]],\n",
      "\n",
      "\n",
      "        [[[40.3777, 65.9847, 81.7076,  ..., 40.0607, 57.9683, 39.7509]]],\n",
      "\n",
      "\n",
      "        [[[43.1149, 69.3951, 85.3253,  ..., 39.6697, 55.6408, 37.7197]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.3772, 65.9843, 81.7071,  ..., 40.0608, 57.9686, 39.7512]]],\n",
      "\n",
      "\n",
      "        [[[40.3787, 66.0035, 81.7214,  ..., 40.0648, 57.9548, 39.7410]]],\n",
      "\n",
      "\n",
      "        [[[44.7557, 69.7785, 80.0443,  ..., 39.7154, 52.8671, 33.4805]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 50, Loss: 1739.5587463378906\n",
      "Training time: 5.21673846244812 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 28.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 44.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from st_DynGNN import run\n",
    "devices = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "else torch.device(\"cpu\")\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_gpus = 1\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.spawn(run, args=(list(range(num_gpus)),dgl_graph), nprocs=num_gpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rxf131/ondemand/ubuntu2204/python310/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/rxf131/ondemand/ubuntu2204/python310/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -44.8861, -210.5856, -106.1942,  ...,  -12.3924,   39.9872,\n",
      "             30.7661]]],\n",
      "\n",
      "\n",
      "        [[[ -44.8903, -210.5752, -106.1891,  ...,  -12.3911,   39.9778,\n",
      "             30.7704]]],\n",
      "\n",
      "\n",
      "        [[[ -44.8870, -210.5836, -106.1932,  ...,  -12.3922,   39.9854,\n",
      "             30.7669]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -49.1797, -194.3997, -100.4074,  ...,   -1.7012,   26.8106,\n",
      "             24.9881]]],\n",
      "\n",
      "\n",
      "        [[[ -44.8855, -210.5870, -106.1949,  ...,  -12.3926,   39.9886,\n",
      "             30.7654]]],\n",
      "\n",
      "\n",
      "        [[[ -30.8586, -177.6261,  -90.0209,  ...,   -3.1436,   43.7385,\n",
      "             20.8000]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -30.8580, -177.6286,  -90.0213,  ...,   -3.1443,   43.7388,\n",
      "             20.7994]]],\n",
      "\n",
      "\n",
      "        [[[ -31.6060, -178.4310,  -90.4268,  ...,   -3.1302,   42.9495,\n",
      "             20.9672]]],\n",
      "\n",
      "\n",
      "        [[[ -32.5050, -189.1369,  -91.3021,  ...,   -6.8867,   28.2487,\n",
      "             20.4225]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -32.5042, -189.1388,  -91.3030,  ...,   -6.8869,   28.2502,\n",
      "             20.4217]]],\n",
      "\n",
      "\n",
      "        [[[ -49.4618, -194.6734, -100.5634,  ...,   -1.6855,   26.5345,\n",
      "             25.0527]]],\n",
      "\n",
      "\n",
      "        [[[ -32.5029, -189.1420,  -91.3045,  ...,   -6.8872,   28.2525,\n",
      "             20.4204]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[  -7.8114, -114.5184,   -8.4140,  ...,  -47.1896,    4.0512,\n",
      "             63.9108]]],\n",
      "\n",
      "\n",
      "        [[[ -18.8118, -139.5343,  -14.4215,  ...,  -64.7056,   -4.4603,\n",
      "             79.1705]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8104, -114.5216,   -8.4151,  ...,  -47.1903,    4.0531,\n",
      "             63.9100]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -24.9642, -126.8154,  -12.9480,  ...,  -52.0261,  -16.2084,\n",
      "             71.4845]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8109, -114.5199,   -8.4145,  ...,  -47.1899,    4.0520,\n",
      "             63.9104]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8105, -114.5216,   -8.4151,  ...,  -47.1903,    4.0530,\n",
      "             63.9100]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[  -7.8107, -114.5216,   -8.4140,  ...,  -47.1910,    4.0508,\n",
      "             63.9105]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8211, -114.5299,   -8.4161,  ...,  -47.1947,    4.0371,\n",
      "             63.9153]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8108, -114.5215,   -8.4133,  ...,  -47.1913,    4.0497,\n",
      "             63.9108]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -23.6683, -125.8777,  -12.6092,  ...,  -51.6545,  -14.6683,\n",
      "             70.9110]]],\n",
      "\n",
      "\n",
      "        [[[  -7.8107, -114.5216,   -8.4139,  ...,  -47.1911,    4.0507,\n",
      "             63.9105]]],\n",
      "\n",
      "\n",
      "        [[[ -24.9649, -126.8138,  -12.9474,  ...,  -52.0258,  -16.2098,\n",
      "             71.4851]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 1, Loss: 62638516.0\n",
      "Epoch 1, Loss: 59603443.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.61it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.61it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.38it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.34it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 17.9419, -70.3500,  17.1129,  ..., -13.7609, -61.4273, 118.2391]]],\n",
      "\n",
      "\n",
      "        [[[ 17.9408, -70.3467,  17.1140,  ..., -13.7602, -61.4290, 118.2397]]],\n",
      "\n",
      "\n",
      "        [[[ 10.0961, -89.8651,  14.1683,  ..., -26.9861, -78.2340, 140.3002]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 17.9409, -70.3471,  17.1138,  ..., -13.7603, -61.4288, 118.2396]]],\n",
      "\n",
      "\n",
      "        [[[ 17.9418, -70.3496,  17.1130,  ..., -13.7608, -61.4275, 118.2392]]],\n",
      "\n",
      "\n",
      "        [[[ 10.0953, -89.8636,  14.1689,  ..., -26.9859, -78.2353, 140.3007]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[  2.9681, -79.2708,  13.9826,  ..., -15.6563, -86.7673, 130.0384]]],\n",
      "\n",
      "\n",
      "        [[[ 17.9415, -70.3495,  17.1148,  ..., -13.7617, -61.4313, 118.2404]]],\n",
      "\n",
      "\n",
      "        [[[ 19.1330, -77.3438,  19.7162,  ..., -18.4334, -81.0505, 122.0236]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 19.1328, -77.3434,  19.7164,  ..., -18.4333, -81.0508, 122.0238]]],\n",
      "\n",
      "\n",
      "        [[[ 17.9549, -70.3413,  17.1166,  ..., -13.7594, -61.4066, 118.2292]]],\n",
      "\n",
      "\n",
      "        [[[ 19.1285, -77.3350,  19.7193,  ..., -18.4321, -81.0569, 122.0265]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -27.9886,  -48.0771,   21.2288,  ...,   11.6965, -121.0714,\n",
      "            176.3542]]],\n",
      "\n",
      "\n",
      "        [[[ -15.8463,  -33.2505,   23.3505,  ...,   20.6536,  -99.4146,\n",
      "            150.2745]]],\n",
      "\n",
      "\n",
      "        [[[ -27.9916,  -48.0706,   21.2309,  ...,   11.6975, -121.0771,\n",
      "            176.3560]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -27.9911,  -48.0718,   21.2305,  ...,   11.6973, -121.0760,\n",
      "            176.3556]]],\n",
      "\n",
      "\n",
      "        [[[ -33.4823,  -39.1383,   20.3014,  ...,   21.6762, -127.7217,\n",
      "            164.5315]]],\n",
      "\n",
      "\n",
      "        [[[ -15.8453,  -33.2533,   23.3498,  ...,   20.6531,  -99.4132,\n",
      "            150.2742]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -16.2186,  -38.4414,   25.8492,  ...,   17.8994, -120.7188,\n",
      "            155.6264]]],\n",
      "\n",
      "\n",
      "        [[[ -15.8367,  -33.2490,   23.3523,  ...,   20.6525,  -99.4007,\n",
      "            150.2675]]],\n",
      "\n",
      "\n",
      "        [[[ -16.2164,  -38.4458,   25.8479,  ...,   17.8988, -120.7157,\n",
      "            155.6252]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -18.2313,  -34.0128,   22.9157,  ...,   20.8102, -103.1756,\n",
      "            152.1769]]],\n",
      "\n",
      "\n",
      "        [[[ -33.4846,  -39.1347,   20.3022,  ...,   21.6769, -127.7254,\n",
      "            164.5330]]],\n",
      "\n",
      "\n",
      "        [[[ -16.2161,  -38.4465,   25.8476,  ...,   17.8987, -120.7152,\n",
      "            155.6250]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 2, Loss: 12995937.0\n",
      "Epoch 2, Loss: 12079086.5\n",
      "tensor([[[[ -72.6946,  -11.9992,    5.3312,  ...,   54.0222, -127.2074,\n",
      "            171.2342]]],\n",
      "\n",
      "\n",
      "        [[[ -55.6337,   -0.6105,    9.1569,  ...,   58.4236, -104.5013,\n",
      "            145.5398]]],\n",
      "\n",
      "\n",
      "        [[[ -55.6336,   -0.6107,    9.1568,  ...,   58.4236, -104.5012,\n",
      "            145.5398]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -55.6329,   -0.6124,    9.1563,  ...,   58.4233, -104.4999,\n",
      "            145.5394]]],\n",
      "\n",
      "\n",
      "        [[[ -55.6449,   -0.5485,    9.1519,  ...,   58.4385, -104.4735,\n",
      "            145.5245]]],\n",
      "\n",
      "\n",
      "        [[[ -76.9079,   -4.3346,    4.4799,  ...,   62.7305, -134.1030,\n",
      "            159.6969]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -55.6331,   -0.6118,    9.1565,  ...,   58.4234, -104.5002,\n",
      "            145.5395]]],\n",
      "\n",
      "\n",
      "        [[[ -58.1958,   -4.1099,   10.9108,  ...,   57.6591, -126.8299,\n",
      "            151.0160]]],\n",
      "\n",
      "\n",
      "        [[[ -55.6331,   -0.6117,    9.1564,  ...,   58.4235, -104.5000,\n",
      "            145.5395]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -55.6331,   -0.6118,    9.1564,  ...,   58.4234, -104.5001,\n",
      "            145.5395]]],\n",
      "\n",
      "\n",
      "        [[[ -76.9086,   -4.3329,    4.4804,  ...,   62.7308, -134.1043,\n",
      "            159.6974]]],\n",
      "\n",
      "\n",
      "        [[[ -55.6319,   -0.6021,    9.1565,  ...,   58.4250, -104.4925,\n",
      "            145.5355]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -74.0728,   25.4222,    0.3589,  ...,   91.9005,  -86.7039,\n",
      "            113.3316]]],\n",
      "\n",
      "\n",
      "        [[[ -94.4884,   19.3552,   -4.0370,  ...,   92.1322, -108.9073,\n",
      "            135.3738]]],\n",
      "\n",
      "\n",
      "        [[[ -74.0725,   25.4217,    0.3588,  ...,   91.9004,  -86.7034,\n",
      "            113.3315]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -74.0725,   25.4217,    0.3588,  ...,   91.9004,  -86.7034,\n",
      "            113.3315]]],\n",
      "\n",
      "\n",
      "        [[[ -94.4868,   19.3518,   -4.0378,  ...,   92.1317, -108.9042,\n",
      "            135.3730]]],\n",
      "\n",
      "\n",
      "        [[[ -74.0726,   25.4218,    0.3588,  ...,   91.9004,  -86.7035,\n",
      "            113.3315]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -74.0773,   25.4223,    0.3579,  ...,   91.9020,  -86.7095,\n",
      "            113.3338]]],\n",
      "\n",
      "\n",
      "        [[[ -74.0728,   25.4223,    0.3589,  ...,   91.9005,  -86.7039,\n",
      "            113.3316]]],\n",
      "\n",
      "\n",
      "        [[[ -77.6745,   23.2684,    1.4022,  ...,   93.0961, -108.1298,\n",
      "            117.1643]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -77.6747,   23.2687,    1.4023,  ...,   93.0961, -108.1299,\n",
      "            117.1644]]],\n",
      "\n",
      "\n",
      "        [[[ -74.0728,   25.4223,    0.3589,  ...,   91.9005,  -86.7039,\n",
      "            113.3316]]],\n",
      "\n",
      "\n",
      "        [[[ -77.6755,   23.2703,    1.4026,  ...,   93.0963, -108.1311,\n",
      "            117.1648]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 3, Loss: 14025107.75\n",
      "Epoch 3, Loss: 12972684.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.51it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-65.5063,  45.0595,  13.7877,  ..., 116.5560, -63.1877,  66.0002]]],\n",
      "\n",
      "\n",
      "        [[[-84.8758,  41.5626,  11.1984,  ..., 120.1213, -82.3555,  81.8675]]],\n",
      "\n",
      "\n",
      "        [[[-65.5087,  45.0602,  13.7874,  ..., 116.5571, -63.1907,  66.0010]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-84.8774,  41.5656,  11.1991,  ..., 120.1219, -82.3583,  81.8682]]],\n",
      "\n",
      "\n",
      "        [[[-84.8758,  41.5626,  11.1984,  ..., 120.1212, -82.3555,  81.8674]]],\n",
      "\n",
      "\n",
      "        [[[-65.5068,  45.0604,  13.7879,  ..., 116.5562, -63.1885,  66.0004]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-65.5065,  45.0599,  13.7878,  ..., 116.5561, -63.1880,  66.0003]]],\n",
      "\n",
      "\n",
      "        [[[-65.5065,  45.0599,  13.7878,  ..., 116.5561, -63.1880,  66.0003]]],\n",
      "\n",
      "\n",
      "        [[[-68.8121,  43.9828,  15.3786,  ..., 119.3389, -83.3605,  67.3524]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-88.0925,  45.4290,   9.0795,  ..., 126.6793, -89.5413,  73.1294]]],\n",
      "\n",
      "\n",
      "        [[[-65.5065,  45.0599,  13.7878,  ..., 116.5561, -63.1880,  66.0003]]],\n",
      "\n",
      "\n",
      "        [[[-88.0915,  45.4268,   9.0790,  ..., 126.6788, -89.5394,  73.1288]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-40.8028,  54.2520,  43.2027,  ..., 133.7519, -67.5807,  12.2242]]],\n",
      "\n",
      "\n",
      "        [[[-38.7226,  54.7625,  40.2831,  ..., 129.9185, -48.1953,  13.6401]]],\n",
      "\n",
      "\n",
      "        [[[-40.8026,  54.2518,  43.2027,  ..., 133.7519, -67.5805,  12.2242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-38.7226,  54.7625,  40.2831,  ..., 129.9185, -48.1953,  13.6401]]],\n",
      "\n",
      "\n",
      "        [[[-38.7225,  54.7625,  40.2831,  ..., 129.9185, -48.1953,  13.6401]]],\n",
      "\n",
      "\n",
      "        [[[-40.8031,  54.2525,  43.2028,  ..., 133.7520, -67.5811,  12.2244]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-38.7223,  54.7620,  40.2830,  ..., 129.9184, -48.1949,  13.6400]]],\n",
      "\n",
      "\n",
      "        [[[-38.7222,  54.7619,  40.2829,  ..., 129.9184, -48.1948,  13.6400]]],\n",
      "\n",
      "\n",
      "        [[[-54.7119,  52.5806,  41.2622,  ..., 135.4200, -65.4821,  22.6514]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-38.7223,  54.7620,  40.2830,  ..., 129.9184, -48.1949,  13.6400]]],\n",
      "\n",
      "\n",
      "        [[[-59.2583,  56.0788,  37.8604,  ..., 141.7318, -73.3804,  16.1179]]],\n",
      "\n",
      "\n",
      "        [[[-54.6836,  52.5334,  41.2521,  ..., 135.4087, -65.4376,  22.6391]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 4, Loss: 10407250.25\n",
      "Epoch 4, Loss: 11035153.25\n",
      "tensor([[[[-13.3915,  52.2152,  76.0547,  ..., 133.7519, -66.8969, -40.3306]]],\n",
      "\n",
      "\n",
      "        [[[-12.5035,  52.7609,  71.5078,  ..., 129.5777, -47.5329, -36.2392]]],\n",
      "\n",
      "\n",
      "        [[[-31.0191,  54.0303,  71.8502,  ..., 141.8658, -72.7635, -38.2300]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-31.0193,  54.0307,  71.8503,  ..., 141.8660, -72.7638, -38.2299]]],\n",
      "\n",
      "\n",
      "        [[[-12.5039,  52.7610,  71.5078,  ..., 129.5780, -47.5336, -36.2393]]],\n",
      "\n",
      "\n",
      "        [[[-12.8370,  52.7842,  71.5140,  ..., 129.7990, -47.9875, -36.2750]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-12.5029,  52.7608,  71.5077,  ..., 129.5773, -47.5321, -36.2392]]],\n",
      "\n",
      "\n",
      "        [[[-12.5041,  52.7627,  71.5081,  ..., 129.5779, -47.5338, -36.2387]]],\n",
      "\n",
      "\n",
      "        [[[-25.1400,  50.3251,  76.6611,  ..., 135.2482, -64.7408, -33.7982]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-12.5029,  52.7609,  71.5077,  ..., 129.5773, -47.5322, -36.2392]]],\n",
      "\n",
      "\n",
      "        [[[-12.5033,  52.7613,  71.5078,  ..., 129.5775, -47.5326, -36.2390]]],\n",
      "\n",
      "\n",
      "        [[[-25.1393,  50.3240,  76.6609,  ..., 135.2479, -64.7397, -33.7985]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -3.6294,  41.3759,  99.3733,  ..., 117.3654, -57.8273, -76.8320]]],\n",
      "\n",
      "\n",
      "        [[[ -4.2090,  40.3021, 105.4034,  ..., 121.2244, -77.7310, -83.1754]]],\n",
      "\n",
      "\n",
      "        [[[ -4.2120,  40.3064, 105.4041,  ..., 121.2257, -77.7341, -83.1742]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -3.6297,  41.3759,  99.3733,  ..., 117.3655, -57.8276, -76.8321]]],\n",
      "\n",
      "\n",
      "        [[[ -3.6294,  41.3759,  99.3733,  ..., 117.3654, -57.8273, -76.8320]]],\n",
      "\n",
      "\n",
      "        [[[ -3.6294,  41.3759,  99.3733,  ..., 117.3654, -57.8273, -76.8320]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-21.5234,  41.7851, 102.2399,  ..., 128.9798, -83.9392, -82.5327]]],\n",
      "\n",
      "\n",
      "        [[[ -3.6288,  41.3754,  99.3731,  ..., 117.3651, -57.8264, -76.8320]]],\n",
      "\n",
      "\n",
      "        [[[ -3.6288,  41.3754,  99.3731,  ..., 117.3650, -57.8264, -76.8320]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-15.1499,  37.5459, 108.2692,  ..., 121.7192, -76.4107, -79.7697]]],\n",
      "\n",
      "\n",
      "        [[[-15.1476,  37.5426, 108.2686,  ..., 121.7181, -76.4076, -79.7706]]],\n",
      "\n",
      "\n",
      "        [[[-21.5235,  41.7853, 102.2399,  ..., 128.9799, -83.9394, -82.5326]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 5, Loss: 10997037.0\n",
      "Epoch 5, Loss: 12268839.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.61it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -13.7349,   22.4503,  121.3653,  ...,  103.3338,  -90.2289,\n",
      "           -110.1960]]],\n",
      "\n",
      "\n",
      "        [[[ -13.7326,   22.4470,  121.3648,  ...,  103.3326,  -90.2266,\n",
      "           -110.1969]]],\n",
      "\n",
      "\n",
      "        [[[ -13.7318,   22.4460,  121.3646,  ...,  103.3322,  -90.2259,\n",
      "           -110.1972]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -13.7329,   22.4475,  121.3648,  ...,  103.3328,  -90.2269,\n",
      "           -110.1968]]],\n",
      "\n",
      "\n",
      "        [[[ -12.6053,   24.3451,  114.4938,  ...,  100.1027,  -69.7222,\n",
      "           -102.3129]]],\n",
      "\n",
      "\n",
      "        [[[ -13.7324,   22.4467,  121.3647,  ...,  103.3326,  -90.2264,\n",
      "           -110.1970]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -25.2500,   18.4060,  125.5168,  ...,  102.5133,  -89.8785,\n",
      "           -108.7167]]],\n",
      "\n",
      "\n",
      "        [[[ -12.6050,   24.3446,  114.4937,  ...,  100.1025,  -69.7218,\n",
      "           -102.3130]]],\n",
      "\n",
      "\n",
      "        [[[ -12.6056,   24.3455,  114.4939,  ...,  100.1028,  -69.7226,\n",
      "           -102.3129]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -12.6051,   24.3447,  114.4937,  ...,  100.1025,  -69.7219,\n",
      "           -102.3130]]],\n",
      "\n",
      "\n",
      "        [[[ -12.6049,   24.3445,  114.4937,  ...,  100.1024,  -69.7218,\n",
      "           -102.3131]]],\n",
      "\n",
      "\n",
      "        [[[ -12.6050,   24.3446,  114.4937,  ...,  100.1025,  -69.7219,\n",
      "           -102.3131]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -31.8932,    4.7016,  119.2663,  ...,   85.0561,  -93.1538,\n",
      "           -119.1815]]],\n",
      "\n",
      "\n",
      "        [[[ -29.8517,    7.4275,  112.4299,  ...,   82.5168,  -72.5939,\n",
      "           -110.5928]]],\n",
      "\n",
      "\n",
      "        [[[ -29.8517,    7.4275,  112.4299,  ...,   82.5168,  -72.5939,\n",
      "           -110.5928]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -29.8517,    7.4275,  112.4299,  ...,   82.5168,  -72.5939,\n",
      "           -110.5928]]],\n",
      "\n",
      "\n",
      "        [[[ -30.0069,    7.4065,  112.6564,  ...,   82.5963,  -73.1087,\n",
      "           -110.7093]]],\n",
      "\n",
      "\n",
      "        [[[ -29.8698,    7.4240,  112.4461,  ...,   82.5255,  -72.6380,\n",
      "           -110.6045]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -29.8430,    7.4289,  112.4030,  ...,   82.5117,  -72.5427,\n",
      "           -110.5829]]],\n",
      "\n",
      "\n",
      "        [[[ -44.6202,   -0.6373,  123.3919,  ...,   82.9554,  -93.0985,\n",
      "           -118.2809]]],\n",
      "\n",
      "\n",
      "        [[[ -44.6223,   -0.6343,  123.3924,  ...,   82.9567,  -93.1012,\n",
      "           -118.2803]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -44.6222,   -0.6343,  123.3924,  ...,   82.9567,  -93.1011,\n",
      "           -118.2802]]],\n",
      "\n",
      "\n",
      "        [[[ -30.6618,    7.2988,  113.7151,  ...,   82.9350,  -75.4468,\n",
      "           -111.2333]]],\n",
      "\n",
      "\n",
      "        [[[ -44.6223,   -0.6342,  123.3925,  ...,   82.9567,  -93.1012,\n",
      "           -118.2802]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 6, Loss: 3025599.6875\n",
      "Epoch 6, Loss: 3631469.75\n",
      "tensor([[[[ -39.1457,   -2.6065,   98.4082,  ...,   65.5235,  -63.4084,\n",
      "           -104.8377]]],\n",
      "\n",
      "\n",
      "        [[[ -55.0914,  -11.9414,  107.8890,  ...,   64.0777,  -82.8721,\n",
      "           -112.0723]]],\n",
      "\n",
      "\n",
      "        [[[ -55.0931,  -11.9389,  107.8895,  ...,   64.0788,  -82.8743,\n",
      "           -112.0719]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -39.1454,   -2.6071,   98.4081,  ...,   65.5233,  -63.4080,\n",
      "           -104.8377]]],\n",
      "\n",
      "\n",
      "        [[[ -55.0929,  -11.9392,  107.8895,  ...,   64.0787,  -82.8741,\n",
      "           -112.0719]]],\n",
      "\n",
      "\n",
      "        [[[ -39.1461,   -2.6060,   98.4083,  ...,   65.5238,  -63.4089,\n",
      "           -104.8376]]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ -41.7267,   -5.8270,  104.7202,  ...,   67.3922,  -83.5538,\n",
      "           -113.4855]]],\n",
      "\n",
      "\n",
      "        [[[ -39.1477,   -2.6074,   98.4159,  ...,   65.5243,  -63.4226,\n",
      "           -104.8404]]],\n",
      "\n",
      "\n",
      "        [[[ -39.1458,   -2.6065,   98.4082,  ...,   65.5236,  -63.4085,\n",
      "           -104.8377]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -39.1458,   -2.6065,   98.4084,  ...,   65.5236,  -63.4088,\n",
      "           -104.8377]]],\n",
      "\n",
      "\n",
      "        [[[ -39.1458,   -2.6065,   98.4082,  ...,   65.5236,  -63.4085,\n",
      "           -104.8377]]],\n",
      "\n",
      "\n",
      "        [[[ -40.5783,   -3.5655,  104.7185,  ...,   66.0368,  -74.8486,\n",
      "           -107.0945]]]], device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-31.2309,  -2.2543,  81.1861,  ...,  48.6000, -43.5613, -90.7362]]],\n",
      "\n",
      "\n",
      "        [[[-31.2309,  -2.2543,  81.1862,  ...,  48.6000, -43.5615, -90.7362]]],\n",
      "\n",
      "\n",
      "        [[[-31.2309,  -2.2543,  81.1861,  ...,  48.6000, -43.5613, -90.7362]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-31.2308,  -2.2543,  81.1860,  ...,  48.6000, -43.5610, -90.7362]]],\n",
      "\n",
      "\n",
      "        [[[-33.5641,  -5.4600,  86.7905,  ...,  49.7851, -62.7519, -99.0238]]],\n",
      "\n",
      "\n",
      "        [[[-32.3043,  -3.0390,  87.2843,  ...,  48.7744, -54.4128, -92.6583]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-32.3043,  -3.0390,  87.2843,  ...,  48.7744, -54.4128, -92.6583]]],\n",
      "\n",
      "\n",
      "        [[[-31.2304,  -2.2548,  81.1858,  ...,  48.5997, -43.5606, -90.7362]]],\n",
      "\n",
      "\n",
      "        [[[-32.3043,  -3.0390,  87.2843,  ...,  48.7744, -54.4128, -92.6583]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-31.2309,  -2.2542,  81.1860,  ...,  48.6000, -43.5611, -90.7362]]],\n",
      "\n",
      "\n",
      "        [[[-32.3043,  -3.0390,  87.2843,  ...,  48.7744, -54.4128, -92.6583]]],\n",
      "\n",
      "\n",
      "        [[[-46.2823, -11.5507,  88.7500,  ...,  45.2662, -60.6237, -96.4538]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 7, Loss: 2946121.375\n",
      "Epoch 7, Loss: 3112212.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.99it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -9.0027,   6.2438,  68.3091,  ...,  32.6323, -19.4298, -74.5203]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0027,   6.2438,  68.3093,  ...,  32.6323, -19.4302, -74.5204]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0027,   6.2438,  68.3095,  ...,  32.6323, -19.4305, -74.5205]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -9.0027,   6.2438,  68.3093,  ...,  32.6323, -19.4302, -74.5204]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0027,   6.2438,  68.3093,  ...,  32.6323, -19.4302, -74.5204]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0076,   6.2409,  68.3466,  ...,  32.6321, -19.4959, -74.5308]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[-21.4416,  -2.0481,  74.4287,  ...,  27.4836, -33.5584, -78.4438]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0076,   6.2409,  68.3466,  ...,  32.6321, -19.4959, -74.5308]]],\n",
      "\n",
      "\n",
      "        [[[-21.4420,  -2.0477,  74.4288,  ...,  27.4839, -33.5588, -78.4438]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ -9.0026,   6.2437,  68.3090,  ...,  32.6322, -19.4296, -74.5203]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0031,   6.2438,  68.3117,  ...,  32.6324, -19.4345, -74.5211]]],\n",
      "\n",
      "\n",
      "        [[[ -9.0023,   6.2432,  68.3089,  ...,  32.6320, -19.4293, -74.5203]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 19.7708,  17.2316,  67.9550,  ...,  21.0013, -16.6877, -67.9344]]],\n",
      "\n",
      "\n",
      "        [[[ 20.1741,  19.4707,  63.1190,  ...,  20.9488,   0.4455, -60.5308]]],\n",
      "\n",
      "\n",
      "        [[[ 19.9016,  19.2719,  68.9538,  ...,  20.7237,  -9.5633, -61.9627]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 19.7686,  17.2342,  67.9557,  ...,  21.0026, -16.6897, -67.9344]]],\n",
      "\n",
      "\n",
      "        [[[ 19.7721,  17.2300,  67.9546,  ...,  21.0004, -16.6864, -67.9344]]],\n",
      "\n",
      "\n",
      "        [[[ 20.1733,  19.4705,  63.1318,  ...,  20.9485,   0.4234, -60.5339]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 11.1491,  12.7263,  68.6364,  ...,  14.4501, -11.2920, -62.9086]]],\n",
      "\n",
      "\n",
      "        [[[ 20.1727,  19.4724,  63.1195,  ...,  20.9498,   0.4439, -60.5309]]],\n",
      "\n",
      "\n",
      "        [[[ 20.1742,  19.4706,  63.1189,  ...,  20.9488,   0.4456, -60.5308]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 20.1732,  19.4717,  63.1193,  ...,  20.9494,   0.4445, -60.5309]]],\n",
      "\n",
      "\n",
      "        [[[ 11.1507,  12.7244,  68.6357,  ...,  14.4491, -11.2902, -62.9085]]],\n",
      "\n",
      "\n",
      "        [[[ 19.9016,  19.2719,  68.9538,  ...,  20.7237,  -9.5633, -61.9627]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 8, Loss: 4309941.125\n",
      "Epoch 8, Loss: 4449623.125\n",
      "tensor([[[[ 4.7894e+01,  3.5455e+01,  6.4672e+01,  ...,  1.7512e+01,\n",
      "            9.7223e+00, -5.1451e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8019e+01,  3.5640e+01,  7.0442e+01,  ...,  1.7300e+01,\n",
      "            3.3253e-02, -5.2743e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7892e+01,  3.5450e+01,  6.4558e+01,  ...,  1.7516e+01,\n",
      "            9.9129e+00, -5.1426e+01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.7892e+01,  3.5450e+01,  6.4556e+01,  ...,  1.7516e+01,\n",
      "            9.9176e+00, -5.1425e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7892e+01,  3.5450e+01,  6.4555e+01,  ...,  1.7516e+01,\n",
      "            9.9188e+00, -5.1425e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8497e+01,  3.3911e+01,  6.9420e+01,  ...,  1.7445e+01,\n",
      "           -6.7731e+00, -5.8627e+01]]]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 4.2073e+01,  3.0567e+01,  7.0204e+01,  ...,  1.0627e+01,\n",
      "           -7.5143e-01, -5.2833e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7892e+01,  3.5450e+01,  6.4555e+01,  ...,  1.7516e+01,\n",
      "            9.9193e+00, -5.1425e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7891e+01,  3.5451e+01,  6.4555e+01,  ...,  1.7517e+01,\n",
      "            9.9177e+00, -5.1425e+01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.8019e+01,  3.5640e+01,  7.0442e+01,  ...,  1.7300e+01,\n",
      "            3.3253e-02, -5.2743e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7891e+01,  3.5451e+01,  6.4555e+01,  ...,  1.7517e+01,\n",
      "            9.9183e+00, -5.1425e+01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2072e+01,  3.0568e+01,  7.0205e+01,  ...,  1.0627e+01,\n",
      "           -7.5204e-01, -5.2833e+01]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 66.8682,  51.8753,  69.0129,  ...,  23.6623,   7.4431, -47.7418]]],\n",
      "\n",
      "\n",
      "        [[[ 68.0944,  51.0284,  73.7793,  ...,  23.8798,  -8.9638, -54.9143]]],\n",
      "\n",
      "\n",
      "        [[[ 66.8560,  51.8523,  68.7878,  ...,  23.6618,   7.8205, -47.6919]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 66.8585,  51.8567,  68.8317,  ...,  23.6618,   7.7468, -47.7016]]],\n",
      "\n",
      "\n",
      "        [[[ 68.0953,  51.0274,  73.7791,  ...,  23.8791,  -8.9629, -54.9142]]],\n",
      "\n",
      "\n",
      "        [[[ 66.8560,  51.8521,  68.7864,  ...,  23.6617,   7.8229, -47.6916]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 67.2092,  52.4345,  74.7698,  ...,  23.6557,  -2.1950, -49.0156]]],\n",
      "\n",
      "\n",
      "        [[[ 66.8566,  51.8514,  68.7861,  ...,  23.6613,   7.8235, -47.6915]]],\n",
      "\n",
      "\n",
      "        [[[ 64.8166,  47.1072,  74.2914,  ...,  16.4786,  -1.5389, -48.5333]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 66.8544,  51.8538,  68.7869,  ...,  23.6628,   7.8213, -47.6919]]],\n",
      "\n",
      "\n",
      "        [[[ 67.2092,  52.4346,  74.7698,  ...,  23.6557,  -2.1950, -49.0156]]],\n",
      "\n",
      "\n",
      "        [[[ 66.8572,  51.8536,  68.8036,  ...,  23.6617,   7.7940, -47.6954]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 9, Loss: 551146.6875\n",
      "Epoch 9, Loss: 576309.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.24it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 75.3761,  66.0531,  71.7368,  ...,  34.4546,   1.1640, -48.8341]]],\n",
      "\n",
      "\n",
      "        [[[ 75.3762,  66.0535,  71.7390,  ...,  34.4548,   1.1602, -48.8347]]],\n",
      "\n",
      "\n",
      "        [[[ 76.8533,  65.8010,  76.7075,  ...,  35.1440, -15.7926, -56.2594]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 75.6887,  67.0305,  77.7299,  ...,  34.7892,  -9.0122, -50.2661]]],\n",
      "\n",
      "\n",
      "        [[[ 76.8555,  65.7988,  76.7068,  ...,  35.1427, -15.7908, -56.2591]]],\n",
      "\n",
      "\n",
      "        [[[ 76.8293,  65.8256,  76.7147,  ...,  35.1577, -15.8119, -56.2624]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 74.1750,  63.0570,  77.4235,  ...,  28.6669,  -9.2608, -49.8413]]],\n",
      "\n",
      "\n",
      "        [[[ 75.3760,  66.0525,  71.7338,  ...,  34.4544,   1.1690, -48.8334]]],\n",
      "\n",
      "\n",
      "        [[[ 74.1765,  63.0555,  77.4229,  ...,  28.6659,  -9.2593, -49.8411]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 75.6887,  67.0305,  77.7299,  ...,  34.7892,  -9.0122, -50.2661]]],\n",
      "\n",
      "\n",
      "        [[[ 74.1697,  63.0625,  77.4253,  ...,  28.6701,  -9.2661, -49.8421]]],\n",
      "\n",
      "\n",
      "        [[[ 74.1675,  63.0648,  77.4260,  ...,  28.6715,  -9.2683, -49.8425]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 77.6806,  76.4917,  71.8958,  ...,  41.2693,  -0.5637, -54.5712]]],\n",
      "\n",
      "\n",
      "        [[[ 77.6745,  76.4970,  71.8936,  ...,  41.2726,  -0.5619, -54.5713]]],\n",
      "\n",
      "\n",
      "        [[[ 79.2854,  76.5252,  76.7097,  ...,  42.1795, -17.4420, -62.3709]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 79.2615,  76.5494,  76.7171,  ...,  42.1930, -17.4608, -62.3742]]],\n",
      "\n",
      "\n",
      "        [[[ 77.8703,  77.7374,  77.8526,  ...,  41.8156, -10.7875, -56.2148]]],\n",
      "\n",
      "\n",
      "        [[[ 79.2784,  76.5322,  76.7118,  ...,  42.1834, -17.4475, -62.3719]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 77.6826,  76.4911,  71.9014,  ...,  41.2687,  -0.5728, -54.5725]]],\n",
      "\n",
      "\n",
      "        [[[ 76.6677,  74.7608,  77.4324,  ...,  36.3884, -11.5163, -56.2911]]],\n",
      "\n",
      "\n",
      "        [[[ 77.8703,  77.7374,  77.8526,  ...,  41.8156, -10.7875, -56.2148]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 77.6757,  76.4956,  71.8928,  ...,  41.2718,  -0.5600, -54.5709]]],\n",
      "\n",
      "\n",
      "        [[[ 76.6797,  74.7485,  77.4282,  ...,  36.3811, -11.5045, -56.2890]]],\n",
      "\n",
      "\n",
      "        [[[ 76.6763,  74.7520,  77.4294,  ...,  36.3831, -11.5078, -56.2896]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 10, Loss: 2285081.25\n",
      "Epoch 10, Loss: 2582389.375\n",
      "tensor([[[[ 79.8395,  82.8917,  70.6233,  ...,  37.0082,   9.6169, -63.9785]]],\n",
      "\n",
      "\n",
      "        [[[ 81.2550,  83.4192,  75.4804,  ...,  37.8499,  -7.1978, -72.3932]]],\n",
      "\n",
      "\n",
      "        [[[ 81.2559,  83.4183,  75.4801,  ...,  37.8494,  -7.1971, -72.3930]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 79.8634,  83.0866,  71.4843,  ...,  37.0718,   8.1626, -64.2596]]],\n",
      "\n",
      "\n",
      "        [[[ 79.8634,  83.0866,  71.4843,  ...,  37.0718,   8.1626, -64.2596]]],\n",
      "\n",
      "\n",
      "        [[[ 81.2536,  83.4207,  75.4809,  ...,  37.8507,  -7.1989, -72.3934]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 80.0281,  84.2324,  76.6349,  ...,  37.4396,  -0.5318, -65.9397]]],\n",
      "\n",
      "\n",
      "        [[[ 79.8395,  82.8911,  70.6212,  ...,  37.0080,   9.6205, -63.9778]]],\n",
      "\n",
      "\n",
      "        [[[ 79.8417,  82.8889,  70.6204,  ...,  37.0066,   9.6226, -63.9773]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 80.0281,  84.2324,  76.6349,  ...,  37.4396,  -0.5318, -65.9397]]],\n",
      "\n",
      "\n",
      "        [[[ 79.8418,  82.8888,  70.6203,  ...,  37.0066,   9.6227, -63.9773]]],\n",
      "\n",
      "\n",
      "        [[[ 79.2281,  81.8414,  75.9505,  ...,  31.6089,  -0.5600, -66.8774]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 85.7755,  86.6733,  75.4258,  ...,  22.5822,  14.8621, -83.2018]]],\n",
      "\n",
      "\n",
      "        [[[ 84.3767,  87.4431,  76.6572,  ...,  22.5069,  21.0829, -76.4834]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2355,  86.0183,  70.6384,  ...,  22.3991,  30.7875, -74.2015]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 84.3767,  87.4431,  76.6572,  ...,  22.5069,  21.0829, -76.4834]]],\n",
      "\n",
      "\n",
      "        [[[ 84.3767,  87.4431,  76.6572,  ...,  22.5069,  21.0829, -76.4833]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2354,  86.0189,  70.6405,  ...,  22.3992,  30.7841, -74.2023]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 84.3767,  87.4431,  76.6572,  ...,  22.5069,  21.0829, -76.4833]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2348,  86.0190,  70.6387,  ...,  22.3996,  30.7867, -74.2017]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2353,  86.0184,  70.6385,  ...,  22.3992,  30.7873, -74.2015]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 84.2349,  86.0189,  70.6386,  ...,  22.3995,  30.7868, -74.2016]]],\n",
      "\n",
      "\n",
      "        [[[ 84.2363,  86.0241,  70.6637,  ...,  22.3995,  30.7467, -74.2111]]],\n",
      "\n",
      "\n",
      "        [[[ 82.5785,  87.0405,  76.4259,  ...,  16.3208,  21.1910, -78.6827]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 11, Loss: 2719376.625\n",
      "Epoch 11, Loss: 2557409.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.84it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.12it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 86.2745,  88.7647,  79.5149,  ...,  -2.2589,  47.5262, -86.5948]]],\n",
      "\n",
      "\n",
      "        [[[ 87.4162,  87.5515,  73.5356,  ...,   5.7890,  54.5614, -81.2239]]],\n",
      "\n",
      "\n",
      "        [[[ 86.2720,  88.7673,  79.5159,  ...,  -2.2572,  47.5236, -86.5953]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 87.4878,  89.0218,  79.6256,  ...,   5.5264,  45.3857, -83.7418]]],\n",
      "\n",
      "\n",
      "        [[[ 87.4165,  87.5834,  73.6630,  ...,   5.7844,  54.3690, -81.2767]]],\n",
      "\n",
      "\n",
      "        [[[ 87.4878,  89.0218,  79.6256,  ...,   5.5264,  45.3857, -83.7418]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 89.0469,  88.2675,  78.3732,  ...,   5.2239,  39.6510, -90.6531]]],\n",
      "\n",
      "\n",
      "        [[[ 89.0475,  88.2669,  78.3730,  ...,   5.2235,  39.6515, -90.6531]]],\n",
      "\n",
      "\n",
      "        [[[ 87.4156,  87.5523,  73.5366,  ...,   5.7894,  54.5597, -81.2244]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 89.0436,  88.2709,  78.3743,  ...,   5.2260,  39.6481, -90.6536]]],\n",
      "\n",
      "\n",
      "        [[[ 87.4157,  87.5521,  73.5358,  ...,   5.7894,  54.5609, -81.2240]]],\n",
      "\n",
      "\n",
      "        [[[ 87.4156,  87.5523,  73.5366,  ...,   5.7894,  54.5597, -81.2244]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 82.7460,  91.2793,  85.5734,  ..., -10.9195,  64.6658, -86.9481]]],\n",
      "\n",
      "\n",
      "        [[[ 84.0322,  91.2880,  85.3088,  ...,  -2.3977,  61.3318, -84.0796]]],\n",
      "\n",
      "\n",
      "        [[[ 82.7212,  91.3046,  85.5822,  ..., -10.9023,  64.6397, -86.9533]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 82.7419,  91.2835,  85.5749,  ..., -10.9167,  64.6616, -86.9490]]],\n",
      "\n",
      "\n",
      "        [[[ 82.7455,  91.2798,  85.5735,  ..., -10.9191,  64.6653, -86.9481]]],\n",
      "\n",
      "\n",
      "        [[[ 82.7440,  91.2813,  85.5741,  ..., -10.9181,  64.6637, -86.9485]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 85.6113,  90.5783,  84.0893,  ...,  -2.8880,  55.8902, -91.0383]]],\n",
      "\n",
      "\n",
      "        [[[ 85.6129,  90.5767,  84.0888,  ...,  -2.8890,  55.8915, -91.0380]]],\n",
      "\n",
      "\n",
      "        [[[ 85.6135,  90.5761,  84.0886,  ...,  -2.8895,  55.8920, -91.0379]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 84.1628,  89.7622,  79.0870,  ...,  -1.9698,  70.1465, -81.4945]]],\n",
      "\n",
      "\n",
      "        [[[ 85.6145,  90.5750,  84.0883,  ...,  -2.8902,  55.8930, -91.0377]]],\n",
      "\n",
      "\n",
      "        [[[ 84.1627,  89.7623,  79.0873,  ...,  -1.9698,  70.1460, -81.4947]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 12, Loss: 834644.703125\n",
      "Epoch 12, Loss: 790943.921875\n",
      "tensor([[[[ 72.2370,  93.0022,  85.4323,  ...,   3.8184,  72.7033, -73.4711]]],\n",
      "\n",
      "\n",
      "        [[[ 72.2370,  93.0023,  85.4325,  ...,   3.8184,  72.7031, -73.4712]]],\n",
      "\n",
      "\n",
      "        [[[ 72.2370,  93.0023,  85.4326,  ...,   3.8184,  72.7029, -73.4713]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 72.2370,  93.0023,  85.4326,  ...,   3.8184,  72.7029, -73.4713]]],\n",
      "\n",
      "\n",
      "        [[[ 72.2370,  93.0023,  85.4324,  ...,   3.8184,  72.7031, -73.4712]]],\n",
      "\n",
      "\n",
      "        [[[ 73.1280,  93.9720,  90.6303,  ...,   3.1523,  58.5236, -82.7451]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 72.2373,  93.0019,  85.4322,  ...,   3.8182,  72.7036, -73.4711]]],\n",
      "\n",
      "\n",
      "        [[[ 72.2359,  93.0034,  85.4327,  ...,   3.8193,  72.7021, -73.4715]]],\n",
      "\n",
      "\n",
      "        [[[ 69.4533,  94.9893,  92.5249,  ...,  -4.3739,  67.1660, -78.0138]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 69.4533,  94.9893,  92.5249,  ...,  -4.3739,  67.1660, -78.0137]]],\n",
      "\n",
      "\n",
      "        [[[ 71.7245,  94.6014,  91.8027,  ...,   3.5370,  63.9680, -75.9198]]],\n",
      "\n",
      "\n",
      "        [[[ 72.2376,  93.0017,  85.4321,  ...,   3.8180,  72.7038, -73.4710]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 53.9406,  97.3389,  96.8360,  ...,  19.0219,  56.9503, -60.2088]]],\n",
      "\n",
      "\n",
      "        [[[ 50.1160,  98.0957,  97.8730,  ...,  12.6420,  59.0698, -60.8263]]],\n",
      "\n",
      "\n",
      "        [[[ 54.9377,  95.6848,  90.3500,  ...,  18.9460,  65.8094, -58.0748]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 50.1159,  98.0959,  97.8730,  ...,  12.6421,  59.0697, -60.8264]]],\n",
      "\n",
      "\n",
      "        [[[ 54.9386,  95.6838,  90.3497,  ...,  18.9453,  65.8104, -58.0745]]],\n",
      "\n",
      "\n",
      "        [[[ 54.9398,  95.6824,  90.3492,  ...,  18.9444,  65.8117, -58.0741]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 55.0462,  96.7859,  95.6750,  ...,  18.9379,  51.2963, -66.7452]]],\n",
      "\n",
      "\n",
      "        [[[ 55.0459,  96.7862,  95.6751,  ...,  18.9381,  51.2961, -66.7453]]],\n",
      "\n",
      "\n",
      "        [[[ 54.9392,  95.6832,  90.3496,  ...,  18.9449,  65.8109, -58.0744]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 55.0449,  96.7873,  95.6754,  ...,  18.9388,  51.2952, -66.7456]]],\n",
      "\n",
      "\n",
      "        [[[ 53.9406,  97.3389,  96.8360,  ...,  19.0219,  56.9503, -60.2088]]],\n",
      "\n",
      "\n",
      "        [[[ 55.0455,  96.7866,  95.6752,  ...,  18.9384,  51.2957, -66.7454]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 13, Loss: 624595.734375\n",
      "Epoch 13, Loss: 542060.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.41it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 32.7473,  98.1261,  99.7880,  ...,  29.0453,  51.2827, -38.6166]]],\n",
      "\n",
      "\n",
      "        [[[ 37.9736,  97.2611,  98.7433,  ...,  33.9633,  50.1916, -39.8932]]],\n",
      "\n",
      "\n",
      "        [[[ 39.4035,  95.6135,  92.2150,  ...,  33.5390,  59.1615, -38.1862]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 39.4041,  95.6128,  92.2086,  ...,  33.5391,  59.1699, -38.1847]]],\n",
      "\n",
      "\n",
      "        [[[ 39.4053,  95.6114,  92.2081,  ...,  33.5383,  59.1712, -38.1843]]],\n",
      "\n",
      "\n",
      "        [[[ 39.4054,  95.6113,  92.2081,  ...,  33.5382,  59.1712, -38.1842]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 37.9736,  97.2611,  98.7433,  ...,  33.9633,  50.1916, -39.8932]]],\n",
      "\n",
      "\n",
      "        [[[ 39.4052,  95.6116,  92.2082,  ...,  33.5384,  59.1710, -38.1843]]],\n",
      "\n",
      "\n",
      "        [[[ 38.8067,  96.7285,  97.5217,  ...,  34.1645,  44.3381, -46.0454]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 38.8094,  96.7255,  97.5208,  ...,  34.1627,  44.3405, -46.0446]]],\n",
      "\n",
      "\n",
      "        [[[ 39.4052,  95.6116,  92.2082,  ...,  33.5384,  59.1710, -38.1843]]],\n",
      "\n",
      "\n",
      "        [[[ 38.8094,  96.7255,  97.5208,  ...,  34.1627,  44.3405, -46.0446]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 24.2435,  93.9179,  99.2736,  ...,  35.2573,  52.5581, -16.9350]]],\n",
      "\n",
      "\n",
      "        [[[ 24.2456,  93.9154,  99.2727,  ...,  35.2558,  52.5605, -16.9342]]],\n",
      "\n",
      "\n",
      "        [[[ 24.2437,  93.9176,  99.2735,  ...,  35.2572,  52.5583, -16.9350]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 31.7711,  91.7683,  91.9064,  ...,  39.0659,  60.5633, -18.7527]]],\n",
      "\n",
      "\n",
      "        [[[ 24.2437,  93.9176,  99.2735,  ...,  35.2572,  52.5584, -16.9349]]],\n",
      "\n",
      "\n",
      "        [[[ 31.7711,  91.7683,  91.9064,  ...,  39.0659,  60.5633, -18.7527]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 31.7705,  91.7690,  91.9067,  ...,  39.0663,  60.5627, -18.7530]]],\n",
      "\n",
      "\n",
      "        [[[ 30.8188,  92.7270,  97.1064,  ...,  39.9149,  45.7814, -25.8226]]],\n",
      "\n",
      "\n",
      "        [[[ 30.8173,  92.7287,  97.1069,  ...,  39.9159,  45.7801, -25.8231]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 31.7705,  91.7690,  91.9067,  ...,  39.0663,  60.5627, -18.7530]]],\n",
      "\n",
      "\n",
      "        [[[ 31.7705,  91.7690,  91.9067,  ...,  39.0663,  60.5627, -18.7530]]],\n",
      "\n",
      "\n",
      "        [[[ 31.7705,  91.7690,  91.9067,  ...,  39.0663,  60.5627, -18.7530]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 14, Loss: 1431716.125\n",
      "Epoch 14, Loss: 1229040.25\n",
      "tensor([[[[33.2143, 85.6144, 92.2235,  ..., 33.9839, 70.5993, -4.6320]]],\n",
      "\n",
      "\n",
      "        [[[25.9414, 87.1001, 99.4520,  ..., 29.5417, 63.5953, -1.2221]]],\n",
      "\n",
      "\n",
      "        [[[33.2146, 85.6141, 92.2234,  ..., 33.9837, 70.5997, -4.6318]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[25.9397, 87.1021, 99.4527,  ..., 29.5429, 63.5934, -1.2228]]],\n",
      "\n",
      "\n",
      "        [[[33.2149, 85.6136, 92.2232,  ..., 33.9834, 70.6001, -4.6316]]],\n",
      "\n",
      "\n",
      "        [[[25.9396, 87.1022, 99.4528,  ..., 29.5430, 63.5933, -1.2229]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 32.3035,  86.3143,  97.3322,  ...,  34.5827,  56.2629, -11.1477]]],\n",
      "\n",
      "\n",
      "        [[[ 32.2995,  86.3189,  97.3336,  ...,  34.5853,  56.2592, -11.1493]]],\n",
      "\n",
      "\n",
      "        [[[ 32.3030,  86.3148,  97.3323,  ...,  34.5830,  56.2625, -11.1479]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 32.3015,  86.3165,  97.3329,  ...,  34.5840,  56.2611, -11.1485]]],\n",
      "\n",
      "\n",
      "        [[[ 33.2144,  85.6142,  92.2234,  ...,  33.9838,  70.5995,  -4.6319]]],\n",
      "\n",
      "\n",
      "        [[[ 32.3032,  86.3145,  97.3323,  ...,  34.5828,  56.2626, -11.1478]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 33.1989,  81.2297, 102.4598,  ...,  19.2096,  77.3401,   5.4631]]],\n",
      "\n",
      "\n",
      "        [[[ 33.1985,  81.2302, 102.4600,  ...,  19.2098,  77.3396,   5.4630]]],\n",
      "\n",
      "\n",
      "        [[[ 39.6159,  80.3065,  95.0573,  ...,  24.7926,  83.0133,   1.4532]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 33.1963,  81.2327, 102.4609,  ...,  19.2114,  77.3372,   5.4619]]],\n",
      "\n",
      "\n",
      "        [[[ 39.6174,  80.3048,  95.0567,  ...,  24.7915,  83.0150,   1.4540]]],\n",
      "\n",
      "\n",
      "        [[[ 38.0224,  81.6216, 101.6891,  ...,  25.0509,  74.6520,   0.5809]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 38.0224,  81.6216, 101.6891,  ...,  25.0509,  74.6520,   0.5808]]],\n",
      "\n",
      "\n",
      "        [[[ 38.9499,  80.7896, 100.1814,  ...,  24.9615,  69.2364,  -4.8655]]],\n",
      "\n",
      "\n",
      "        [[[ 38.9505,  80.7890, 100.1812,  ...,  24.9611,  69.2369,  -4.8652]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 38.9479,  80.7920, 100.1821,  ...,  24.9628,  69.2345,  -4.8663]]],\n",
      "\n",
      "\n",
      "        [[[ 38.9502,  80.7892, 100.1813,  ...,  24.9613,  69.2367,  -4.8653]]],\n",
      "\n",
      "\n",
      "        [[[ 38.9508,  80.7886, 100.1811,  ...,  24.9609,  69.2373,  -4.8651]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 15, Loss: 863913.09375\n",
      "Epoch 15, Loss: 734151.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.82it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 45.7341,  78.3393, 100.0264,  ...,  19.6038,  90.6449,   0.1487]]],\n",
      "\n",
      "\n",
      "        [[[ 44.2510,  79.6215, 106.7768,  ...,  19.7485,  82.4853,  -0.7712]]],\n",
      "\n",
      "\n",
      "        [[[ 44.2510,  79.6216, 106.7768,  ...,  19.7485,  82.4853,  -0.7712]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 40.1245,  79.0991, 107.8687,  ...,  13.3659,  85.7525,   3.8551]]],\n",
      "\n",
      "\n",
      "        [[[ 45.7353,  78.3380, 100.0259,  ...,  19.6029,  90.6461,   0.1493]]],\n",
      "\n",
      "\n",
      "        [[[ 45.7353,  78.3380, 100.0259,  ...,  19.6029,  90.6461,   0.1493]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 45.2888,  78.7578, 105.2606,  ...,  19.5209,  77.2267,  -6.3018]]],\n",
      "\n",
      "\n",
      "        [[[ 45.2893,  78.7572, 105.2604,  ...,  19.5206,  77.2271,  -6.3016]]],\n",
      "\n",
      "\n",
      "        [[[ 45.2898,  78.7567, 105.2602,  ...,  19.5203,  77.2275,  -6.3014]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 45.7347,  78.3387, 100.0262,  ...,  19.6034,  90.6454,   0.1490]]],\n",
      "\n",
      "\n",
      "        [[[ 45.2900,  78.7564, 105.2602,  ...,  19.5201,  77.2278,  -6.3013]]],\n",
      "\n",
      "\n",
      "        [[[ 44.2510,  79.6216, 106.7768,  ...,  19.7485,  82.4853,  -0.7712]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 48.4724,  79.9209, 105.6812,  ...,  22.2756,  90.1819,  -5.0699]]],\n",
      "\n",
      "\n",
      "        [[[ 48.0889,  80.4373, 111.0609,  ...,  22.2873,  76.7760, -11.8240]]],\n",
      "\n",
      "\n",
      "        [[[ 48.0923,  80.4333, 111.0598,  ...,  22.2849,  76.7792, -11.8226]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 48.4724,  79.9209, 105.6812,  ...,  22.2756,  90.1819,  -5.0699]]],\n",
      "\n",
      "\n",
      "        [[[ 47.0390,  81.2492, 112.5607,  ...,  22.4785,  82.0486,  -6.1215]]],\n",
      "\n",
      "\n",
      "        [[[ 48.0897,  80.4363, 111.0606,  ...,  22.2866,  76.7768, -11.8237]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 43.2021,  80.9902, 114.0766,  ...,  16.3663,  85.0668,  -2.1624]]],\n",
      "\n",
      "\n",
      "        [[[ 43.1997,  80.9929, 114.0775,  ...,  16.3679,  85.0643,  -2.1635]]],\n",
      "\n",
      "\n",
      "        [[[ 48.4719,  79.9215, 105.6814,  ...,  22.2760,  90.1813,  -5.0702]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 48.4722,  79.9211, 105.6813,  ...,  22.2757,  90.1817,  -5.0700]]],\n",
      "\n",
      "\n",
      "        [[[ 48.4726,  79.9207, 105.6812,  ...,  22.2755,  90.1821,  -5.0698]]],\n",
      "\n",
      "\n",
      "        [[[ 48.4718,  79.9216, 105.6815,  ...,  22.2761,  90.1813,  -5.0702]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 16, Loss: 209363.54296875\n",
      "Epoch 16, Loss: 213192.0703125\n",
      "tensor([[[[ 49.0537,  83.2109, 109.5198,  ...,  29.4110,  84.8798,  -9.7663]]],\n",
      "\n",
      "\n",
      "        [[[ 47.6348,  84.6245, 116.4826,  ...,  29.7731,  76.6634, -10.9342]]],\n",
      "\n",
      "\n",
      "        [[[ 49.0537,  83.2108, 109.5198,  ...,  29.4110,  84.8798,  -9.7663]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 48.6431,  83.8966, 114.9691,  ...,  29.7130,  71.2832, -16.7938]]],\n",
      "\n",
      "\n",
      "        [[[ 49.0537,  83.2109, 109.5198,  ...,  29.4110,  84.8798,  -9.7663]]],\n",
      "\n",
      "\n",
      "        [[[ 49.0537,  83.2108, 109.5198,  ...,  29.4110,  84.8798,  -9.7663]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 49.0543,  83.2102, 109.5195,  ...,  29.4106,  84.8804,  -9.7661]]],\n",
      "\n",
      "\n",
      "        [[[ 49.0522,  83.2125, 109.5203,  ...,  29.4121,  84.8782,  -9.7671]]],\n",
      "\n",
      "\n",
      "        [[[ 49.0542,  83.2102, 109.5196,  ...,  29.4106,  84.8804,  -9.7661]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 49.0542,  83.2103, 109.5196,  ...,  29.4107,  84.8803,  -9.7661]]],\n",
      "\n",
      "\n",
      "        [[[ 47.6348,  84.6245, 116.4826,  ...,  29.7731,  76.6634, -10.9342]]],\n",
      "\n",
      "\n",
      "        [[[ 47.6348,  84.6245, 116.4826,  ...,  29.7731,  76.6634, -10.9342]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 50.1803,  84.4381, 109.2869,  ...,  34.4122,  81.1462, -10.1730]]],\n",
      "\n",
      "\n",
      "        [[[ 48.7971,  85.8930, 116.2355,  ...,  34.8848,  72.8790, -11.3578]]],\n",
      "\n",
      "\n",
      "        [[[ 45.1202,  86.2765, 117.9258,  ...,  29.9804,  74.6682,  -8.1977]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 45.1200,  86.2767, 117.9259,  ...,  29.9805,  74.6679,  -8.1978]]],\n",
      "\n",
      "\n",
      "        [[[ 50.1797,  84.4387, 109.2871,  ...,  34.4126,  81.1456, -10.1733]]],\n",
      "\n",
      "\n",
      "        [[[ 50.1796,  84.4389, 109.2871,  ...,  34.4127,  81.1455, -10.1734]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 50.1798,  84.4386, 109.2871,  ...,  34.4126,  81.1457, -10.1733]]],\n",
      "\n",
      "\n",
      "        [[[ 49.7680,  85.2077, 114.6336,  ...,  34.9117,  67.4302, -17.2786]]],\n",
      "\n",
      "\n",
      "        [[[ 50.1798,  84.4386, 109.2871,  ...,  34.4126,  81.1457, -10.1733]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 49.7698,  85.2056, 114.6330,  ...,  34.9105,  67.4318, -17.2778]]],\n",
      "\n",
      "\n",
      "        [[[ 50.1798,  84.4386, 109.2871,  ...,  34.4126,  81.1457, -10.1733]]],\n",
      "\n",
      "\n",
      "        [[[ 49.7672,  85.2085, 114.6339,  ...,  34.9123,  67.4295, -17.2789]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 17, Loss: 350599.578125\n",
      "Epoch 17, Loss: 360667.359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.01it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 52.7205,  81.3268, 105.0313,  ...,  33.6987,  82.6191,  -5.3995]]],\n",
      "\n",
      "\n",
      "        [[[ 52.3793,  81.9834, 110.1018,  ...,  34.1372,  69.0239, -12.3443]]],\n",
      "\n",
      "\n",
      "        [[[ 52.7207,  81.3265, 105.0311,  ...,  33.6986,  82.6195,  -5.3994]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 52.7207,  81.3265, 105.0311,  ...,  33.6986,  82.6195,  -5.3994]]],\n",
      "\n",
      "\n",
      "        [[[ 52.3820,  81.9802, 110.1009,  ...,  34.1354,  69.0264, -12.3431]]],\n",
      "\n",
      "\n",
      "        [[[ 52.3821,  81.9802, 110.1008,  ...,  34.1354,  69.0264, -12.3431]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 47.9817,  82.8815, 113.0941,  ...,  29.1633,  76.2396,  -2.9811]]],\n",
      "\n",
      "\n",
      "        [[[ 47.9605,  82.9059, 113.1026,  ...,  29.1783,  76.2166,  -2.9911]]],\n",
      "\n",
      "\n",
      "        [[[ 52.7212,  81.3259, 105.0309,  ...,  33.6982,  82.6201,  -5.3991]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 47.9812,  82.8821, 113.0943,  ...,  29.1637,  76.2390,  -2.9813]]],\n",
      "\n",
      "\n",
      "        [[[ 47.9815,  82.8818, 113.0942,  ...,  29.1636,  76.2392,  -2.9812]]],\n",
      "\n",
      "\n",
      "        [[[ 47.9839,  82.8789, 113.0931,  ...,  29.1618,  76.2419,  -2.9800]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 54.7886,  74.6627,  98.9099,  ...,  29.2895,  87.9223,   2.5410]]],\n",
      "\n",
      "\n",
      "        [[[ 54.7898,  74.6613,  98.9094,  ...,  29.2886,  87.9236,   2.5416]]],\n",
      "\n",
      "\n",
      "        [[[ 53.5488,  75.9265, 105.5843,  ...,  29.6320,  79.8683,   1.6373]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 53.5488,  75.9265, 105.5843,  ...,  29.6320,  79.8683,   1.6373]]],\n",
      "\n",
      "\n",
      "        [[[ 50.3186,  75.4916, 106.1895,  ...,  24.1916,  82.1354,   5.8012]]],\n",
      "\n",
      "\n",
      "        [[[ 53.5488,  75.9265, 105.5843,  ...,  29.6320,  79.8683,   1.6373]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 53.5488,  75.9265, 105.5843,  ...,  29.6320,  79.8683,   1.6373]]],\n",
      "\n",
      "\n",
      "        [[[ 54.7895,  74.6616,  98.9095,  ...,  29.2889,  87.9232,   2.5415]]],\n",
      "\n",
      "\n",
      "        [[[ 54.7892,  74.6620,  98.9104,  ...,  29.2891,  87.9221,   2.5412]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 54.7895,  74.6616,  98.9096,  ...,  29.2889,  87.9232,   2.5415]]],\n",
      "\n",
      "\n",
      "        [[[ 54.5091,  75.0402, 103.6240,  ...,  29.4957,  74.6297,  -4.0917]]],\n",
      "\n",
      "\n",
      "        [[[ 54.5092,  75.0401, 103.6239,  ...,  29.4956,  74.6299,  -4.0916]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 18, Loss: 589129.921875\n",
      "Epoch 18, Loss: 570622.625\n",
      "tensor([[[[ 52.9210,  67.0015,  93.9526,  ...,  26.8871,  91.4930,  10.8184]]],\n",
      "\n",
      "\n",
      "        [[[ 48.2369,  66.9846, 100.6086,  ...,  21.4767,  86.1003,  14.9647]]],\n",
      "\n",
      "\n",
      "        [[[ 48.2366,  66.9850, 100.6086,  ...,  21.4770,  86.0999,  14.9645]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 52.9215,  67.0010,  93.9524,  ...,  26.8867,  91.4934,  10.8187]]],\n",
      "\n",
      "\n",
      "        [[[ 52.9199,  67.0029,  93.9531,  ...,  26.8879,  91.4918,  10.8178]]],\n",
      "\n",
      "\n",
      "        [[[ 52.9215,  67.0010,  93.9524,  ...,  26.8867,  91.4934,  10.8187]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[52.5336, 67.0614, 98.3696,  ..., 26.9527, 78.4330,  4.5141]]],\n",
      "\n",
      "\n",
      "        [[[52.9209, 67.0016, 93.9526,  ..., 26.8871, 91.4928, 10.8184]]],\n",
      "\n",
      "\n",
      "        [[[52.5318, 67.0635, 98.3702,  ..., 26.9539, 78.4315,  4.5132]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[52.9210, 67.0016, 93.9526,  ..., 26.8871, 91.4928, 10.8184]]],\n",
      "\n",
      "\n",
      "        [[[52.5333, 67.0617, 98.3697,  ..., 26.9529, 78.4328,  4.5140]]],\n",
      "\n",
      "\n",
      "        [[[52.9209, 67.0016, 93.9526,  ..., 26.8871, 91.4928, 10.8184]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[47.1616, 61.4615, 91.2582,  ..., 29.3803, 89.9991, 17.4125]]],\n",
      "\n",
      "\n",
      "        [[[46.4964, 61.3055, 95.4838,  ..., 29.5273, 76.9533, 11.3626]]],\n",
      "\n",
      "\n",
      "        [[[46.4976, 61.3039, 95.4833,  ..., 29.5265, 76.9543, 11.3632]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[46.4995, 61.3016, 95.4826,  ..., 29.5251, 76.9562, 11.3642]]],\n",
      "\n",
      "\n",
      "        [[[47.1618, 61.4612, 91.2579,  ..., 29.3801, 89.9995, 17.4126]]],\n",
      "\n",
      "\n",
      "        [[[46.4985, 61.3028, 95.4830,  ..., 29.5258, 76.9552, 11.3637]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.8001, 60.8405, 97.5706,  ..., 24.2525, 84.4042, 22.2504]]],\n",
      "\n",
      "\n",
      "        [[[41.8009, 60.8395, 97.5703,  ..., 24.2519, 84.4051, 22.2509]]],\n",
      "\n",
      "\n",
      "        [[[41.7988, 60.8422, 97.5712,  ..., 24.2535, 84.4027, 22.2497]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[47.1623, 61.4605, 91.2577,  ..., 29.3797, 90.0001, 17.4129]]],\n",
      "\n",
      "\n",
      "        [[[41.7995, 60.8413, 97.5709,  ..., 24.2530, 84.4035, 22.2501]]],\n",
      "\n",
      "\n",
      "        [[[41.8009, 60.8396, 97.5703,  ..., 24.2520, 84.4051, 22.2509]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 19, Loss: 323589.015625\n",
      "Epoch 19, Loss: 316727.328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.29it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.63it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[41.0111, 60.1627, 90.1954,  ..., 34.8502, 83.4816, 22.4091]]],\n",
      "\n",
      "\n",
      "        [[[41.0111, 60.1626, 90.1953,  ..., 34.8502, 83.4817, 22.4091]]],\n",
      "\n",
      "\n",
      "        [[[40.0648, 59.9779, 94.3023,  ..., 35.2086, 70.2337, 16.5483]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0111, 60.1626, 90.1953,  ..., 34.8502, 83.4816, 22.4091]]],\n",
      "\n",
      "\n",
      "        [[[40.0647, 59.9780, 94.3023,  ..., 35.2087, 70.2336, 16.5483]]],\n",
      "\n",
      "\n",
      "        [[[39.5706, 61.1742, 96.5757,  ..., 35.2951, 75.3928, 21.9591]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.0116, 60.1620, 90.1951,  ..., 34.8498, 83.4822, 22.4094]]],\n",
      "\n",
      "\n",
      "        [[[41.0109, 60.1628, 90.1952,  ..., 34.8503, 83.4817, 22.4090]]],\n",
      "\n",
      "\n",
      "        [[[41.0110, 60.1628, 90.1954,  ..., 34.8503, 83.4815, 22.4090]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0102, 60.1637, 90.1957,  ..., 34.8509, 83.4807, 22.4086]]],\n",
      "\n",
      "\n",
      "        [[[41.0106, 60.1632, 90.1955,  ..., 34.8506, 83.4811, 22.4088]]],\n",
      "\n",
      "\n",
      "        [[[41.0114, 60.1623, 90.1952,  ..., 34.8500, 83.4820, 22.4093]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[37.5694, 62.0888, 92.5794,  ..., 39.0290, 61.0113, 21.7564]]],\n",
      "\n",
      "\n",
      "        [[[37.5719, 62.0856, 92.5784,  ..., 39.0273, 61.0136, 21.7577]]],\n",
      "\n",
      "\n",
      "        [[[37.5725, 62.0849, 92.5781,  ..., 39.0269, 61.0142, 21.7580]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.6301, 62.1453, 88.6098,  ..., 38.5336, 74.5714, 27.4285]]],\n",
      "\n",
      "\n",
      "        [[[37.5702, 62.0877, 92.5791,  ..., 39.0284, 61.0121, 21.7568]]],\n",
      "\n",
      "\n",
      "        [[[37.2036, 63.2480, 94.9290,  ..., 39.0485, 66.2932, 27.0910]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[32.2668, 61.7988, 94.6022,  ..., 34.5233, 67.0165, 33.2587]]],\n",
      "\n",
      "\n",
      "        [[[38.6306, 62.1448, 88.6096,  ..., 38.5333, 74.5719, 27.4287]]],\n",
      "\n",
      "\n",
      "        [[[38.6303, 62.1452, 88.6097,  ..., 38.5335, 74.5716, 27.4285]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.6297, 62.1459, 88.6100,  ..., 38.5339, 74.5710, 27.4282]]],\n",
      "\n",
      "\n",
      "        [[[32.2651, 61.8009, 94.6030,  ..., 34.5246, 67.0146, 33.2577]]],\n",
      "\n",
      "\n",
      "        [[[32.2670, 61.7986, 94.6021,  ..., 34.5232, 67.0167, 33.2588]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 20, Loss: 137225.32421875\n",
      "Epoch 20, Loss: 153255.734375\n",
      "tensor([[[[41.3628, 66.7941, 89.1796,  ..., 37.5545, 52.7869, 27.3789]]],\n",
      "\n",
      "\n",
      "        [[[42.2459, 66.6150, 85.4237,  ..., 37.1631, 66.6099, 32.8369]]],\n",
      "\n",
      "\n",
      "        [[[40.9899, 67.8726, 91.6479,  ..., 37.6171, 58.1648, 32.6180]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.3612, 66.7960, 89.1802,  ..., 37.5555, 52.7856, 27.3781]]],\n",
      "\n",
      "\n",
      "        [[[41.3600, 66.7974, 89.1807,  ..., 37.5563, 52.7845, 27.3775]]],\n",
      "\n",
      "\n",
      "        [[[42.2459, 66.6150, 85.4237,  ..., 37.1632, 66.6099, 32.8369]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.2464, 66.6144, 85.4235,  ..., 37.1628, 66.6104, 32.8372]]],\n",
      "\n",
      "\n",
      "        [[[42.2465, 66.6143, 85.4235,  ..., 37.1628, 66.6105, 32.8372]]],\n",
      "\n",
      "\n",
      "        [[[42.2464, 66.6144, 85.4235,  ..., 37.1628, 66.6104, 32.8372]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.2449, 66.6162, 85.4242,  ..., 37.1639, 66.6089, 32.8363]]],\n",
      "\n",
      "\n",
      "        [[[36.3377, 66.9451, 91.0408,  ..., 32.9992, 58.0238, 39.2066]]],\n",
      "\n",
      "\n",
      "        [[[42.2458, 66.6151, 85.4238,  ..., 37.1632, 66.6098, 32.8368]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[47.9662, 72.1397, 85.3454,  ..., 33.2032, 45.9756, 32.2475]]],\n",
      "\n",
      "\n",
      "        [[[47.9653, 72.1408, 85.3458,  ..., 33.2039, 45.9748, 32.2470]]],\n",
      "\n",
      "\n",
      "        [[[48.5474, 71.6909, 81.8204,  ..., 33.0395, 60.0021, 37.5233]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[48.5474, 71.6909, 81.8204,  ..., 33.0395, 60.0021, 37.5233]]],\n",
      "\n",
      "\n",
      "        [[[48.5474, 71.6909, 81.8204,  ..., 33.0395, 60.0021, 37.5233]]],\n",
      "\n",
      "\n",
      "        [[[48.5474, 71.6909, 81.8204,  ..., 33.0395, 60.0022, 37.5233]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.4868, 72.7008, 86.9806,  ..., 28.3428, 50.6153, 44.3838]]],\n",
      "\n",
      "\n",
      "        [[[48.5460, 71.6926, 81.8210,  ..., 33.0406, 60.0008, 37.5224]]],\n",
      "\n",
      "\n",
      "        [[[43.4841, 72.7039, 86.9818,  ..., 28.3447, 50.6126, 44.3823]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[47.5272, 73.1209, 87.9510,  ..., 33.3631, 51.4279, 37.4025]]],\n",
      "\n",
      "\n",
      "        [[[48.5478, 71.6904, 81.8202,  ..., 33.0392, 60.0026, 37.5235]]],\n",
      "\n",
      "\n",
      "        [[[43.4845, 72.7034, 86.9816,  ..., 28.3444, 50.6130, 44.3825]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 21, Loss: 74113.787109375\n",
      "Epoch 21, Loss: 131506.45703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.23it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.96it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.95it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[52.6434, 78.1445, 86.0858,  ..., 30.2075, 46.5331, 39.0542]]],\n",
      "\n",
      "\n",
      "        [[[53.4649, 76.5492, 79.9949,  ..., 29.9896, 55.1810, 39.1513]]],\n",
      "\n",
      "\n",
      "        [[[53.4648, 76.5493, 79.9950,  ..., 29.9897, 55.1808, 39.1512]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[53.4651, 76.5490, 79.9948,  ..., 29.9894, 55.1812, 39.1515]]],\n",
      "\n",
      "\n",
      "        [[[49.0593, 78.2427, 84.9183,  ..., 24.9109, 45.1833, 46.1195]]],\n",
      "\n",
      "\n",
      "        [[[49.0608, 78.2411, 84.9175,  ..., 24.9100, 45.1846, 46.1203]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[53.1266, 77.2597, 83.3692,  ..., 29.9753, 41.0306, 33.9212]]],\n",
      "\n",
      "\n",
      "        [[[53.1278, 77.2583, 83.3687,  ..., 29.9745, 41.0316, 33.9218]]],\n",
      "\n",
      "\n",
      "        [[[52.6434, 78.1445, 86.0858,  ..., 30.2075, 46.5331, 39.0542]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[53.1276, 77.2585, 83.3688,  ..., 29.9746, 41.0315, 33.9218]]],\n",
      "\n",
      "\n",
      "        [[[52.6434, 78.1446, 86.0858,  ..., 30.2075, 46.5331, 39.0542]]],\n",
      "\n",
      "\n",
      "        [[[53.1280, 77.2581, 83.3686,  ..., 29.9743, 41.0318, 33.9219]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[55.0502, 80.0296, 81.3636,  ..., 29.4379, 53.2973, 36.8853]]],\n",
      "\n",
      "\n",
      "        [[[55.0502, 80.0296, 81.3636,  ..., 29.4379, 53.2972, 36.8853]]],\n",
      "\n",
      "\n",
      "        [[[55.0502, 80.0296, 81.3636,  ..., 29.4379, 53.2973, 36.8853]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[54.8137, 80.9377, 84.7252,  ..., 29.3557, 39.1572, 31.5305]]],\n",
      "\n",
      "\n",
      "        [[[54.8132, 80.9383, 84.7254,  ..., 29.3561, 39.1568, 31.5302]]],\n",
      "\n",
      "\n",
      "        [[[54.3411, 81.7549, 87.4991,  ..., 29.6123, 44.6573, 36.7216]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[50.8950, 82.2437, 86.4254,  ..., 24.2803, 43.0375, 43.4912]]],\n",
      "\n",
      "\n",
      "        [[[55.0502, 80.0296, 81.3636,  ..., 29.4379, 53.2973, 36.8853]]],\n",
      "\n",
      "\n",
      "        [[[50.8763, 82.2652, 86.4339,  ..., 24.2938, 43.0192, 43.4805]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[55.0507, 80.0290, 81.3634,  ..., 29.4376, 53.2978, 36.8856]]],\n",
      "\n",
      "\n",
      "        [[[55.0493, 80.0306, 81.3640,  ..., 29.4386, 53.2965, 36.8848]]],\n",
      "\n",
      "\n",
      "        [[[55.0507, 80.0290, 81.3633,  ..., 29.4375, 53.2978, 36.8856]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 22, Loss: 176065.921875\n",
      "Epoch 22, Loss: 217908.328125\n",
      "tensor([[[[54.8487, 83.7899, 89.6776,  ..., 29.2205, 41.8939, 26.0514]]],\n",
      "\n",
      "\n",
      "        [[[55.0490, 82.7195, 86.1783,  ..., 29.3493, 55.8247, 31.6630]]],\n",
      "\n",
      "\n",
      "        [[[55.0491, 82.7195, 86.1783,  ..., 29.3493, 55.8248, 31.6630]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[54.8476, 83.7912, 89.6780,  ..., 29.2213, 41.8930, 26.0508]]],\n",
      "\n",
      "\n",
      "        [[[55.0491, 82.7195, 86.1782,  ..., 29.3493, 55.8248, 31.6630]]],\n",
      "\n",
      "\n",
      "        [[[54.4051, 84.5529, 92.4491,  ..., 29.4976, 47.3070, 31.3637]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[55.0488, 82.7199, 86.1788,  ..., 29.3495, 55.8239, 31.6629]]],\n",
      "\n",
      "\n",
      "        [[[50.9526, 85.3600, 91.8028,  ..., 24.1665, 45.8516, 37.5429]]],\n",
      "\n",
      "\n",
      "        [[[55.0495, 82.7190, 86.1780,  ..., 29.3490, 55.8252, 31.6633]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[50.9555, 85.3568, 91.8016,  ..., 24.1645, 45.8544, 37.5445]]],\n",
      "\n",
      "\n",
      "        [[[50.9525, 85.3601, 91.8028,  ..., 24.1665, 45.8515, 37.5428]]],\n",
      "\n",
      "\n",
      "        [[[54.4051, 84.5529, 92.4491,  ..., 29.4976, 47.3070, 31.3637]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[56.1026, 84.5272, 91.6693,  ..., 27.1683, 62.6767, 27.1017]]],\n",
      "\n",
      "\n",
      "        [[[55.9936, 85.7187, 95.3309,  ..., 26.8968, 49.1491, 21.2654]]],\n",
      "\n",
      "\n",
      "        [[[56.1026, 84.5272, 91.6693,  ..., 27.1683, 62.6766, 27.1016]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[55.5383, 86.4459, 98.0972,  ..., 27.2485, 54.3873, 26.6816]]],\n",
      "\n",
      "\n",
      "        [[[56.1026, 84.5272, 91.6693,  ..., 27.1683, 62.6766, 27.1016]]],\n",
      "\n",
      "\n",
      "        [[[55.9932, 85.7192, 95.3310,  ..., 26.8971, 49.1487, 21.2652]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[55.5383, 86.4459, 98.0972,  ..., 27.2485, 54.3873, 26.6816]]],\n",
      "\n",
      "\n",
      "        [[[56.1022, 84.5277, 91.6694,  ..., 27.1686, 62.6763, 27.1014]]],\n",
      "\n",
      "\n",
      "        [[[52.2028, 87.4846, 97.9393,  ..., 21.6950, 53.5326, 32.3422]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[56.1030, 84.5268, 91.6691,  ..., 27.1680, 62.6770, 27.1019]]],\n",
      "\n",
      "\n",
      "        [[[56.1031, 84.5266, 91.6691,  ..., 27.1679, 62.6772, 27.1020]]],\n",
      "\n",
      "\n",
      "        [[[56.1026, 84.5272, 91.6693,  ..., 27.1683, 62.6767, 27.1016]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 23, Loss: 185345.984375\n",
      "Epoch 23, Loss: 196088.9296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.42it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.39it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.10it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.90it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 57.7371,  87.1836, 101.3154,  ...,  23.8355,  62.3267,  25.8342]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2060,  85.2079,  94.7850,  ...,  23.8429,  70.3676,  26.2899]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2445,  86.4712,  98.4996,  ...,  23.3766,  57.2778,  20.3972]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 58.2435,  86.4723,  98.4999,  ...,  23.3773,  57.2770,  20.3967]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2061,  85.2079,  94.7849,  ...,  23.8429,  70.3677,  26.2899]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2060,  85.2079,  94.7849,  ...,  23.8428,  70.3676,  26.2899]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 58.2064,  85.2075,  94.7848,  ...,  23.8426,  70.3680,  26.2901]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2057,  85.2086,  94.7864,  ...,  23.8430,  70.3658,  26.2897]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2065,  85.2074,  94.7847,  ...,  23.8425,  70.3681,  26.2902]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 54.6390,  88.3440, 101.4062,  ...,  17.9329,  62.1610,  31.3563]]],\n",
      "\n",
      "\n",
      "        [[[ 57.7371,  87.1836, 101.3154,  ...,  23.8355,  62.3267,  25.8342]]],\n",
      "\n",
      "\n",
      "        [[[ 58.2046,  85.2096,  94.7855,  ...,  23.8439,  70.3663,  26.2890]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[ 56.0393,  89.1417, 101.7280,  ...,  17.5263,  66.4420,  34.4386]]],\n",
      "\n",
      "\n",
      "        [[[ 59.3853,  85.8304,  95.0970,  ...,  23.5072,  74.1839,  29.0975]]],\n",
      "\n",
      "\n",
      "        [[[ 56.0373,  89.1440, 101.7288,  ...,  17.5278,  66.4399,  34.4375]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 56.0391,  89.1420, 101.7280,  ...,  17.5265,  66.4417,  34.4385]]],\n",
      "\n",
      "\n",
      "        [[[ 58.9840,  87.8575, 101.6634,  ...,  23.4872,  66.3016,  28.6901]]],\n",
      "\n",
      "\n",
      "        [[[ 56.0388,  89.1423, 101.7281,  ...,  17.5267,  66.4414,  34.4383]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[59.3853, 85.8304, 95.0970,  ..., 23.5072, 74.1839, 29.0975]]],\n",
      "\n",
      "\n",
      "        [[[59.3853, 85.8304, 95.0970,  ..., 23.5072, 74.1839, 29.0975]]],\n",
      "\n",
      "\n",
      "        [[[59.5392, 87.1651, 98.7397,  ..., 22.9764, 61.3570, 23.3100]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[59.3853, 85.8304, 95.0970,  ..., 23.5072, 74.1839, 29.0975]]],\n",
      "\n",
      "\n",
      "        [[[59.5391, 87.1652, 98.7398,  ..., 22.9765, 61.3569, 23.3100]]],\n",
      "\n",
      "\n",
      "        [[[59.5396, 87.1647, 98.7396,  ..., 22.9762, 61.3573, 23.3102]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 24, Loss: 94691.58984375\n",
      "Epoch 24, Loss: 85986.25\n",
      "tensor([[[[ 54.3130,  89.4190, 100.3634,  ...,  22.9307,  64.2852,  38.6499]]],\n",
      "\n",
      "\n",
      "        [[[ 57.7838,  85.9789,  93.9003,  ...,  28.3405,  72.2690,  32.9112]]],\n",
      "\n",
      "\n",
      "        [[[ 57.3805,  88.0422, 100.4636,  ...,  28.4331,  64.4126,  32.5768]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 57.7837,  85.9789,  93.9003,  ...,  28.3405,  72.2690,  32.9112]]],\n",
      "\n",
      "\n",
      "        [[[ 57.7828,  85.9799,  93.9007,  ...,  28.3411,  72.2681,  32.9107]]],\n",
      "\n",
      "\n",
      "        [[[ 54.3103,  89.4221, 100.3645,  ...,  22.9326,  64.2825,  38.6483]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[57.7833, 85.9794, 93.9006,  ..., 28.3409, 72.2685, 32.9109]]],\n",
      "\n",
      "\n",
      "        [[[57.9410, 87.3654, 97.4049,  ..., 27.9755, 59.4498, 27.2754]]],\n",
      "\n",
      "\n",
      "        [[[57.9412, 87.3652, 97.4049,  ..., 27.9754, 59.4499, 27.2755]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[57.9417, 87.3646, 97.4047,  ..., 27.9750, 59.4504, 27.2757]]],\n",
      "\n",
      "\n",
      "        [[[57.7833, 85.9794, 93.9006,  ..., 28.3409, 72.2684, 32.9109]]],\n",
      "\n",
      "\n",
      "        [[[57.9390, 87.3677, 97.4057,  ..., 27.9769, 59.4480, 27.2743]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[53.6052, 86.3307, 99.5300,  ..., 36.8676, 59.6274, 34.4321]]],\n",
      "\n",
      "\n",
      "        [[[54.0618, 84.2792, 92.9691,  ..., 36.5826, 67.5220, 34.7359]]],\n",
      "\n",
      "\n",
      "        [[[54.1367, 85.6376, 96.3498,  ..., 36.5357, 54.5860, 29.1630]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[54.1385, 85.6356, 96.3492,  ..., 36.5345, 54.5875, 29.1639]]],\n",
      "\n",
      "\n",
      "        [[[54.0618, 84.2792, 92.9690,  ..., 36.5826, 67.5220, 34.7359]]],\n",
      "\n",
      "\n",
      "        [[[54.1380, 85.6361, 96.3493,  ..., 36.5348, 54.5871, 29.1637]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[54.0616, 84.2794, 92.9690,  ..., 36.5828, 67.5219, 34.7358]]],\n",
      "\n",
      "\n",
      "        [[[50.2261, 87.6081, 99.2957,  ..., 32.1502, 58.9738, 40.6380]]],\n",
      "\n",
      "\n",
      "        [[[50.2270, 87.6070, 99.2953,  ..., 32.1496, 58.9747, 40.6386]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[54.0619, 84.2791, 92.9690,  ..., 36.5826, 67.5221, 34.7359]]],\n",
      "\n",
      "\n",
      "        [[[53.6052, 86.3307, 99.5300,  ..., 36.8676, 59.6274, 34.4321]]],\n",
      "\n",
      "\n",
      "        [[[54.0622, 84.2788, 92.9689,  ..., 36.5824, 67.5224, 34.7361]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 25, Loss: 34959.9833984375\n",
      "Epoch 25, Loss: 63463.474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.59it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[47.0259, 82.9579, 99.2126,  ..., 39.6836, 55.8979, 39.0829]]],\n",
      "\n",
      "\n",
      "        [[[47.0261, 82.9577, 99.2125,  ..., 39.6834, 55.8981, 39.0830]]],\n",
      "\n",
      "\n",
      "        [[[47.0284, 82.9550, 99.2115,  ..., 39.6818, 55.9004, 39.0843]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[51.1289, 80.0459, 92.9065,  ..., 43.3217, 64.7606, 33.4085]]],\n",
      "\n",
      "\n",
      "        [[[47.0261, 82.9576, 99.2125,  ..., 39.6834, 55.8981, 39.0830]]],\n",
      "\n",
      "\n",
      "        [[[51.1284, 80.0464, 92.9066,  ..., 43.3220, 64.7602, 33.4082]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[51.1632, 81.2608, 96.2052,  ..., 43.5253, 51.7985, 27.7576]]],\n",
      "\n",
      "\n",
      "        [[[51.1284, 80.0464, 92.9067,  ..., 43.3220, 64.7601, 33.4082]]],\n",
      "\n",
      "\n",
      "        [[[51.1284, 80.0464, 92.9067,  ..., 43.3220, 64.7602, 33.4082]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[51.1611, 81.2635, 96.2060,  ..., 43.5268, 51.7967, 27.7564]]],\n",
      "\n",
      "\n",
      "        [[[51.1284, 80.0464, 92.9068,  ..., 43.3220, 64.7600, 33.4082]]],\n",
      "\n",
      "\n",
      "        [[[51.1279, 80.0477, 92.9103,  ..., 43.3224, 64.7557, 33.4079]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[50.2053, 73.4076, 92.4299,  ..., 45.5659, 65.3787, 30.9572]]],\n",
      "\n",
      "\n",
      "        [[[50.2919, 74.3679, 95.6293,  ..., 45.8170, 52.5448, 25.1785]]],\n",
      "\n",
      "\n",
      "        [[[50.2055, 73.4072, 92.4293,  ..., 45.5657, 65.3795, 30.9574]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[49.7108, 75.2531, 99.0029,  ..., 46.0496, 57.5710, 30.5434]]],\n",
      "\n",
      "\n",
      "        [[[50.2055, 73.4072, 92.4293,  ..., 45.5657, 65.3795, 30.9574]]],\n",
      "\n",
      "\n",
      "        [[[50.2055, 73.4072, 92.4293,  ..., 45.5657, 65.3796, 30.9574]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[49.7108, 75.2531, 99.0029,  ..., 46.0496, 57.5710, 30.5434]]],\n",
      "\n",
      "\n",
      "        [[[49.7108, 75.2531, 99.0029,  ..., 46.0496, 57.5710, 30.5434]]],\n",
      "\n",
      "\n",
      "        [[[50.2056, 73.4071, 92.4292,  ..., 45.5656, 65.3797, 30.9574]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[46.0607, 75.6367, 98.6819,  ..., 42.1848, 56.6012, 36.2500]]],\n",
      "\n",
      "\n",
      "        [[[46.0606, 75.6368, 98.6819,  ..., 42.1849, 56.6012, 36.2499]]],\n",
      "\n",
      "\n",
      "        [[[50.2050, 73.4085, 92.4331,  ..., 45.5661, 65.3749, 30.9570]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 26, Loss: 48394.845703125\n",
      "Epoch 26, Loss: 68531.001953125\n",
      "tensor([[[[49.7202, 66.2398, 89.9750,  ..., 44.8913, 66.2936, 30.1233]]],\n",
      "\n",
      "\n",
      "        [[[49.8754, 66.9250, 92.9926,  ..., 45.0636, 53.5962, 24.2878]]],\n",
      "\n",
      "\n",
      "        [[[49.7202, 66.2398, 89.9751,  ..., 44.8913, 66.2935, 30.1233]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[49.8767, 66.9235, 92.9921,  ..., 45.0628, 53.5973, 24.2885]]],\n",
      "\n",
      "\n",
      "        [[[49.8768, 66.9234, 92.9921,  ..., 45.0627, 53.5974, 24.2886]]],\n",
      "\n",
      "\n",
      "        [[[49.7202, 66.2397, 89.9749,  ..., 44.8913, 66.2936, 30.1233]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[49.7207, 66.2392, 89.9747,  ..., 44.8910, 66.2941, 30.1236]]],\n",
      "\n",
      "\n",
      "        [[[45.5953, 67.7125, 95.9323,  ..., 41.4093, 57.6472, 35.2390]]],\n",
      "\n",
      "\n",
      "        [[[45.5952, 67.7127, 95.9323,  ..., 41.4094, 57.6471, 35.2389]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[49.2367, 67.9425, 96.4977,  ..., 45.3376, 58.5718, 29.6784]]],\n",
      "\n",
      "\n",
      "        [[[45.5980, 67.7093, 95.9311,  ..., 41.4074, 57.6499, 35.2405]]],\n",
      "\n",
      "\n",
      "        [[[45.5982, 67.7092, 95.9311,  ..., 41.4074, 57.6501, 35.2406]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[47.2114, 60.9248, 85.7275,  ..., 44.5752, 63.8210, 32.1208]]],\n",
      "\n",
      "\n",
      "        [[[47.2114, 60.9249, 85.7275,  ..., 44.5753, 63.8210, 32.1209]]],\n",
      "\n",
      "\n",
      "        [[[46.6945, 62.5301, 92.1551,  ..., 44.9780, 56.1095, 31.7111]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[46.6945, 62.5301, 92.1551,  ..., 44.9780, 56.1095, 31.7111]]],\n",
      "\n",
      "\n",
      "        [[[47.2113, 60.9248, 85.7274,  ..., 44.5752, 63.8211, 32.1208]]],\n",
      "\n",
      "\n",
      "        [[[47.3469, 61.4136, 88.4837,  ..., 44.6842, 51.1117, 26.3577]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[47.2118, 60.9243, 85.7273,  ..., 44.5749, 63.8215, 32.1211]]],\n",
      "\n",
      "\n",
      "        [[[42.8607, 61.8591, 91.1702,  ..., 41.0387, 54.8972, 37.4121]]],\n",
      "\n",
      "\n",
      "        [[[47.2105, 60.9260, 85.7278,  ..., 44.5759, 63.8202, 32.1203]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.8421, 61.8817, 91.1785,  ..., 41.0520, 54.8784, 37.4014]]],\n",
      "\n",
      "\n",
      "        [[[46.6945, 62.5301, 92.1551,  ..., 44.9780, 56.1095, 31.7111]]],\n",
      "\n",
      "\n",
      "        [[[46.6945, 62.5301, 92.1551,  ..., 44.9780, 56.1095, 31.7111]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 27, Loss: 90279.05078125\n",
      "Epoch 27, Loss: 79014.8984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.53it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[42.6852, 59.1527, 81.7182,  ..., 45.1970, 58.1626, 35.6726]]],\n",
      "\n",
      "\n",
      "        [[[42.6847, 59.1537, 81.7212,  ..., 45.1973, 58.1589, 35.6723]]],\n",
      "\n",
      "\n",
      "        [[[42.7046, 59.6082, 84.2249,  ..., 45.2871, 45.2935, 30.0529]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.7053, 59.6072, 84.2246,  ..., 45.2866, 45.2941, 30.0533]]],\n",
      "\n",
      "\n",
      "        [[[42.6847, 59.1537, 81.7212,  ..., 45.1973, 58.1589, 35.6723]]],\n",
      "\n",
      "\n",
      "        [[[42.6852, 59.1527, 81.7183,  ..., 45.1970, 58.1625, 35.6726]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.6843, 59.1539, 81.7186,  ..., 45.1977, 58.1617, 35.6720]]],\n",
      "\n",
      "\n",
      "        [[[42.6856, 59.1521, 81.7180,  ..., 45.1967, 58.1631, 35.6729]]],\n",
      "\n",
      "\n",
      "        [[[42.6856, 59.1522, 81.7180,  ..., 45.1967, 58.1630, 35.6728]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.8442, 60.0012, 86.6869,  ..., 41.7339, 48.5529, 41.3259]]],\n",
      "\n",
      "\n",
      "        [[[42.6852, 59.1526, 81.7182,  ..., 45.1969, 58.1627, 35.6726]]],\n",
      "\n",
      "\n",
      "        [[[37.8637, 59.9772, 86.6780,  ..., 41.7199, 48.5725, 41.3375]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.1486, 60.3020, 82.4106,  ..., 44.3511, 40.1465, 33.2468]]],\n",
      "\n",
      "\n",
      "        [[[38.1496, 60.3008, 82.4102,  ..., 44.3504, 40.1473, 33.2474]]],\n",
      "\n",
      "\n",
      "        [[[38.1487, 60.3019, 82.4107,  ..., 44.3510, 40.1465, 33.2468]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.2459, 59.7730, 80.0493,  ..., 44.3431, 53.1506, 38.7452]]],\n",
      "\n",
      "\n",
      "        [[[38.2459, 59.7729, 80.0493,  ..., 44.3431, 53.1505, 38.7451]]],\n",
      "\n",
      "\n",
      "        [[[38.1503, 60.2999, 82.4099,  ..., 44.3499, 40.1479, 33.2478]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[32.9573, 60.7864, 84.8191,  ..., 40.7515, 42.9687, 44.7213]]],\n",
      "\n",
      "\n",
      "        [[[38.2446, 59.7746, 80.0498,  ..., 44.3441, 53.1493, 38.7443]]],\n",
      "\n",
      "\n",
      "        [[[32.9552, 60.7890, 84.8201,  ..., 40.7530, 42.9666, 44.7200]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[37.5924, 61.4205, 86.3566,  ..., 44.6281, 45.3412, 38.4703]]],\n",
      "\n",
      "\n",
      "        [[[38.2460, 59.7729, 80.0492,  ..., 44.3430, 53.1507, 38.7452]]],\n",
      "\n",
      "\n",
      "        [[[37.5924, 61.4205, 86.3566,  ..., 44.6281, 45.3412, 38.4703]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 28, Loss: 78382.21875\n",
      "Epoch 28, Loss: 64375.142578125\n",
      "tensor([[[[36.7569, 62.3515, 83.7846,  ..., 39.2824, 39.8078, 34.5332]]],\n",
      "\n",
      "\n",
      "        [[[36.7596, 62.3480, 83.7835,  ..., 39.2805, 39.8101, 34.5347]]],\n",
      "\n",
      "\n",
      "        [[[36.7592, 62.3484, 83.7836,  ..., 39.2807, 39.8098, 34.5345]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[36.7597, 62.3478, 83.7834,  ..., 39.2804, 39.8102, 34.5348]]],\n",
      "\n",
      "\n",
      "        [[[36.2053, 63.4246, 87.7819,  ..., 39.6377, 45.0088, 39.7346]]],\n",
      "\n",
      "\n",
      "        [[[36.2053, 63.4246, 87.7819,  ..., 39.6377, 45.0088, 39.7346]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[31.4416, 63.0698, 86.3995,  ..., 35.3551, 42.5211, 46.0448]]],\n",
      "\n",
      "\n",
      "        [[[36.8363, 61.6898, 81.4308,  ..., 39.5379, 52.7379, 39.9920]]],\n",
      "\n",
      "\n",
      "        [[[36.2053, 63.4246, 87.7819,  ..., 39.6377, 45.0088, 39.7346]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[36.8363, 61.6899, 81.4308,  ..., 39.5379, 52.7379, 39.9920]]],\n",
      "\n",
      "\n",
      "        [[[36.8360, 61.6902, 81.4310,  ..., 39.5381, 52.7376, 39.9918]]],\n",
      "\n",
      "\n",
      "        [[[31.4416, 63.0698, 86.3995,  ..., 35.3550, 42.5211, 46.0448]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.1402, 64.0360, 84.9992,  ..., 33.1090, 56.0860, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[37.6088, 65.8695, 91.4470,  ..., 32.9849, 48.5338, 39.4970]]],\n",
      "\n",
      "\n",
      "        [[[37.6088, 65.8695, 91.4470,  ..., 32.9849, 48.5338, 39.4970]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.1406, 64.0355, 84.9991,  ..., 33.1087, 56.0864, 39.7731]]],\n",
      "\n",
      "\n",
      "        [[[33.0052, 65.8034, 90.4312,  ..., 28.1146, 46.3296, 45.7364]]],\n",
      "\n",
      "\n",
      "        [[[38.1398, 64.0365, 84.9994,  ..., 33.1094, 56.0857, 39.7725]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[38.1402, 64.0361, 84.9993,  ..., 33.1091, 56.0860, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[38.1402, 64.0361, 84.9994,  ..., 33.1091, 56.0859, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[38.2001, 64.8449, 87.4425,  ..., 32.5156, 43.3996, 34.2894]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.2021, 64.8423, 87.4417,  ..., 32.5141, 43.4014, 34.2906]]],\n",
      "\n",
      "\n",
      "        [[[38.2022, 64.8422, 87.4416,  ..., 32.5141, 43.4015, 34.2906]]],\n",
      "\n",
      "\n",
      "        [[[38.1397, 64.0373, 85.0026,  ..., 33.1092, 56.0820, 39.7725]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 29, Loss: 42291.3984375\n",
      "Epoch 29, Loss: 36806.2744140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.62it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.53it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.30it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[39.7391, 65.5714, 87.7849,  ..., 28.9788, 59.3672, 39.7730]]],\n",
      "\n",
      "\n",
      "        [[[39.7388, 65.5718, 87.7850,  ..., 28.9790, 59.3669, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[34.8911, 67.6279, 93.5843,  ..., 23.4564, 50.0553, 45.6771]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.3201, 67.4860, 94.3083,  ..., 28.6944, 51.9932, 39.4812]]],\n",
      "\n",
      "\n",
      "        [[[34.8727, 67.6501, 93.5922,  ..., 23.4704, 50.0367, 45.6661]]],\n",
      "\n",
      "\n",
      "        [[[39.7383, 65.5724, 87.7852,  ..., 28.9794, 59.3664, 39.7725]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.3201, 67.4860, 94.3083,  ..., 28.6944, 51.9932, 39.4812]]],\n",
      "\n",
      "\n",
      "        [[[39.7387, 65.5719, 87.7851,  ..., 28.9791, 59.3668, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[39.9480, 66.4904, 90.2827,  ..., 28.1529, 46.9191, 34.2767]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.7387, 65.5719, 87.7851,  ..., 28.9791, 59.3668, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[39.7387, 65.5719, 87.7851,  ..., 28.9791, 59.3667, 39.7728]]],\n",
      "\n",
      "\n",
      "        [[[39.9483, 66.4901, 90.2826,  ..., 28.1527, 46.9193, 34.2768]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.3075, 67.0386, 89.0380,  ..., 29.4677, 60.1634, 40.5038]]],\n",
      "\n",
      "\n",
      "        [[[35.6173, 69.3734, 95.0185,  ..., 23.9809, 50.9887, 46.4414]]],\n",
      "\n",
      "\n",
      "        [[[35.6168, 69.3740, 95.0186,  ..., 23.9813, 50.9881, 46.4412]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.3089, 67.0370, 89.0375,  ..., 29.4666, 60.1648, 40.5047]]],\n",
      "\n",
      "\n",
      "        [[[39.9836, 69.0293, 95.5987,  ..., 29.1435, 52.9126, 40.2127]]],\n",
      "\n",
      "\n",
      "        [[[40.3089, 67.0369, 89.0375,  ..., 29.4666, 60.1647, 40.5047]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.6158, 68.0640, 91.5237,  ..., 28.6170, 47.8400, 35.0283]]],\n",
      "\n",
      "\n",
      "        [[[40.6142, 68.0660, 91.5243,  ..., 28.6182, 47.8386, 35.0274]]],\n",
      "\n",
      "\n",
      "        [[[40.3088, 67.0371, 89.0377,  ..., 29.4666, 60.1645, 40.5046]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.3088, 67.0370, 89.0376,  ..., 29.4666, 60.1647, 40.5047]]],\n",
      "\n",
      "\n",
      "        [[[39.9836, 69.0293, 95.5987,  ..., 29.1435, 52.9125, 40.2127]]],\n",
      "\n",
      "\n",
      "        [[[39.9836, 69.0293, 95.5987,  ..., 29.1435, 52.9125, 40.2127]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 30, Loss: 32583.3056640625\n",
      "Epoch 30, Loss: 20859.7958984375\n",
      "tensor([[[[41.4989, 69.3270, 90.9303,  ..., 30.7430, 47.8736, 35.7353]]],\n",
      "\n",
      "\n",
      "        [[[41.0877, 68.2042, 88.5337,  ..., 31.5397, 60.1154, 41.1925]]],\n",
      "\n",
      "\n",
      "        [[[41.0877, 68.2042, 88.5337,  ..., 31.5397, 60.1153, 41.1925]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0878, 68.2042, 88.5336,  ..., 31.5397, 60.1154, 41.1925]]],\n",
      "\n",
      "\n",
      "        [[[40.8621, 70.2670, 95.0872,  ..., 31.2329, 52.9653, 40.8976]]],\n",
      "\n",
      "\n",
      "        [[[41.0878, 68.2042, 88.5336,  ..., 31.5396, 60.1154, 41.1925]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[36.5740, 70.7862, 94.4829,  ..., 26.2863, 50.9668, 47.1594]]],\n",
      "\n",
      "\n",
      "        [[[41.0882, 68.2036, 88.5334,  ..., 31.5393, 60.1159, 41.1928]]],\n",
      "\n",
      "\n",
      "        [[[36.5718, 70.7889, 94.4838,  ..., 26.2880, 50.9646, 47.1580]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0878, 68.2041, 88.5336,  ..., 31.5396, 60.1155, 41.1925]]],\n",
      "\n",
      "\n",
      "        [[[36.5745, 70.7856, 94.4827,  ..., 26.2858, 50.9675, 47.1597]]],\n",
      "\n",
      "\n",
      "        [[[41.0881, 68.2037, 88.5334,  ..., 31.5394, 60.1159, 41.1927]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.5040, 69.2394, 88.1155,  ..., 33.0673, 60.9739, 40.6767]]],\n",
      "\n",
      "\n",
      "        [[[43.0486, 70.4482, 90.4260,  ..., 32.2982, 48.8578, 35.1877]]],\n",
      "\n",
      "\n",
      "        [[[42.3917, 71.3685, 94.6604,  ..., 32.7770, 53.9446, 40.3512]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.0481, 70.4487, 90.4262,  ..., 32.2985, 48.8574, 35.1875]]],\n",
      "\n",
      "\n",
      "        [[[43.0476, 70.4493, 90.4264,  ..., 32.2988, 48.8570, 35.1872]]],\n",
      "\n",
      "\n",
      "        [[[42.5040, 69.2394, 88.1155,  ..., 33.0673, 60.9739, 40.6767]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.5036, 69.2398, 88.1156,  ..., 33.0676, 60.9736, 40.6764]]],\n",
      "\n",
      "\n",
      "        [[[42.5037, 69.2408, 88.1191,  ..., 33.0673, 60.9699, 40.6763]]],\n",
      "\n",
      "\n",
      "        [[[38.2439, 72.0518, 94.0450,  ..., 27.9760, 51.9653, 46.5274]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.3917, 71.3685, 94.6604,  ..., 32.7770, 53.9446, 40.3512]]],\n",
      "\n",
      "\n",
      "        [[[42.5037, 69.2397, 88.1155,  ..., 33.0675, 60.9737, 40.6765]]],\n",
      "\n",
      "\n",
      "        [[[38.2441, 72.0516, 94.0449,  ..., 27.9758, 51.9655, 46.5276]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 31, Loss: 26015.1796875\n",
      "Epoch 31, Loss: 30000.49609375\n",
      "tensor([[[[44.3152, 70.5541, 88.4976,  ..., 33.8938, 63.3348, 38.5647]]],\n",
      "\n",
      "\n",
      "        [[[44.3152, 70.5541, 88.4976,  ..., 33.8938, 63.3348, 38.5646]]],\n",
      "\n",
      "\n",
      "        [[[45.0066, 71.8627, 90.7584,  ..., 33.1232, 51.4111, 32.9717]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.3245, 72.7538, 95.0483,  ..., 33.6126, 56.4609, 38.1722]]],\n",
      "\n",
      "\n",
      "        [[[44.3245, 72.7538, 95.0483,  ..., 33.6126, 56.4609, 38.1722]]],\n",
      "\n",
      "\n",
      "        [[[45.0039, 71.8661, 90.7594,  ..., 33.1251, 51.4087, 32.9702]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.20it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[40.3607, 73.6236, 94.5045,  ..., 28.8705, 54.6584, 44.1074]]],\n",
      "\n",
      "\n",
      "        [[[40.3578, 73.6270, 94.5057,  ..., 28.8727, 54.6555, 44.1058]]],\n",
      "\n",
      "\n",
      "        [[[44.3153, 70.5540, 88.4975,  ..., 33.8938, 63.3349, 38.5647]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.3245, 72.7538, 95.0483,  ..., 33.6125, 56.4609, 38.1722]]],\n",
      "\n",
      "\n",
      "        [[[44.3153, 70.5540, 88.4974,  ..., 33.8937, 63.3350, 38.5647]]],\n",
      "\n",
      "\n",
      "        [[[40.3584, 73.6263, 94.5054,  ..., 28.8722, 54.6560, 44.1061]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[46.0312, 73.5836, 91.5758,  ..., 34.9902, 53.6805, 30.2442]]],\n",
      "\n",
      "\n",
      "        [[[45.2360, 72.1635, 89.3433,  ..., 35.7149, 65.4250, 35.9624]]],\n",
      "\n",
      "\n",
      "        [[[45.2360, 72.1635, 89.3431,  ..., 35.7149, 65.4252, 35.9624]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[46.0295, 73.5856, 91.5764,  ..., 34.9914, 53.6790, 30.2433]]],\n",
      "\n",
      "\n",
      "        [[[46.0316, 73.5832, 91.5756,  ..., 34.9899, 53.6809, 30.2444]]],\n",
      "\n",
      "\n",
      "        [[[45.2360, 72.1635, 89.3431,  ..., 35.7149, 65.4252, 35.9624]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[45.2364, 72.1630, 89.3429,  ..., 35.7146, 65.4256, 35.9626]]],\n",
      "\n",
      "\n",
      "        [[[41.4692, 75.5316, 95.4905,  ..., 30.8860, 57.0417, 41.1341]]],\n",
      "\n",
      "\n",
      "        [[[45.2363, 72.1632, 89.3430,  ..., 35.7147, 65.4255, 35.9626]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[45.2365, 72.1629, 89.3429,  ..., 35.7145, 65.4257, 35.9627]]],\n",
      "\n",
      "\n",
      "        [[[41.4670, 75.5342, 95.4914,  ..., 30.8876, 57.0394, 41.1328]]],\n",
      "\n",
      "\n",
      "        [[[41.4669, 75.5343, 95.4914,  ..., 30.8877, 57.0393, 41.1328]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 32, Loss: 30021.658203125\n",
      "Epoch 32, Loss: 38004.458984375\n",
      "tensor([[[[44.8027, 74.1334, 92.3782,  ..., 38.5808, 53.2678, 29.1318]]],\n",
      "\n",
      "\n",
      "        [[[44.8019, 74.1343, 92.3785,  ..., 38.5813, 53.2672, 29.1314]]],\n",
      "\n",
      "\n",
      "        [[[44.8024, 74.1337, 92.3783,  ..., 38.5810, 53.2675, 29.1317]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.8031, 74.1329, 92.3781,  ..., 38.5805, 53.2682, 29.1321]]],\n",
      "\n",
      "\n",
      "        [[[44.1493, 74.9721, 96.7473,  ..., 39.0232, 58.3034, 34.3985]]],\n",
      "\n",
      "\n",
      "        [[[43.9968, 72.6510, 90.1712,  ..., 39.1869, 64.9461, 34.9079]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.9965, 72.6514, 90.1712,  ..., 39.1872, 64.9459, 34.9077]]],\n",
      "\n",
      "\n",
      "        [[[44.1493, 74.9721, 96.7473,  ..., 39.0232, 58.3034, 34.3985]]],\n",
      "\n",
      "\n",
      "        [[[43.9970, 72.6507, 90.1711,  ..., 39.1868, 64.9464, 34.9081]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.9960, 72.6519, 90.1714,  ..., 39.1875, 64.9455, 34.9074]]],\n",
      "\n",
      "\n",
      "        [[[40.1529, 76.1872, 96.4600,  ..., 34.7552, 56.5437, 39.8941]]],\n",
      "\n",
      "\n",
      "        [[[40.1542, 76.1857, 96.4595,  ..., 34.7542, 56.5450, 39.8949]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.1722, 74.0237, 96.5006,  ..., 41.7673, 57.4470, 35.5312]]],\n",
      "\n",
      "\n",
      "        [[[39.0809, 75.2160, 96.2411,  ..., 37.7400, 55.5490, 41.0981]]],\n",
      "\n",
      "\n",
      "        [[[42.9729, 71.6944, 89.9441,  ..., 41.8719, 64.0167, 36.0279]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.9734, 71.6921, 89.9393,  ..., 41.8715, 64.0218, 36.0287]]],\n",
      "\n",
      "\n",
      "        [[[39.0786, 75.2188, 96.2421,  ..., 37.7417, 55.5466, 41.0967]]],\n",
      "\n",
      "\n",
      "        [[[42.9731, 71.6925, 89.9395,  ..., 41.8718, 64.0214, 36.0284]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.9730, 71.6928, 89.9398,  ..., 41.8719, 64.0210, 36.0283]]],\n",
      "\n",
      "\n",
      "        [[[43.7973, 73.1711, 92.0744,  ..., 41.3501, 52.3873, 30.2930]]],\n",
      "\n",
      "\n",
      "        [[[43.1722, 74.0237, 96.5006,  ..., 41.7673, 57.4470, 35.5312]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.9730, 71.6927, 89.9397,  ..., 41.8718, 64.0212, 36.0284]]],\n",
      "\n",
      "\n",
      "        [[[42.9730, 71.6926, 89.9395,  ..., 41.8718, 64.0213, 36.0284]]],\n",
      "\n",
      "\n",
      "        [[[43.1722, 74.0237, 96.5006,  ..., 41.7673, 57.4470, 35.5312]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 33, Loss: 27903.88671875\n",
      "Epoch 33, Loss: 42840.57421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.11it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[44.3635, 70.6005, 90.8480,  ..., 41.1537, 52.8255, 32.6066]]],\n",
      "\n",
      "\n",
      "        [[[44.3647, 70.5990, 90.8475,  ..., 41.1528, 52.8266, 32.6073]]],\n",
      "\n",
      "\n",
      "        [[[44.3647, 70.5990, 90.8475,  ..., 41.1528, 52.8267, 32.6073]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.3647, 70.5990, 90.8475,  ..., 41.1528, 52.8267, 32.6073]]],\n",
      "\n",
      "\n",
      "        [[[43.4543, 69.1937, 88.8216,  ..., 41.7148, 64.3597, 38.2530]]],\n",
      "\n",
      "\n",
      "        [[[43.4543, 69.1937, 88.8216,  ..., 41.7148, 64.3597, 38.2530]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.4544, 69.1936, 88.8215,  ..., 41.7147, 64.3598, 38.2530]]],\n",
      "\n",
      "\n",
      "        [[[39.6966, 72.5216, 95.0311,  ..., 37.5413, 55.9722, 43.5392]]],\n",
      "\n",
      "\n",
      "        [[[39.6966, 72.5216, 95.0311,  ..., 37.5413, 55.9722, 43.5392]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.6971, 72.5209, 95.0309,  ..., 37.5409, 55.9727, 43.5395]]],\n",
      "\n",
      "\n",
      "        [[[43.4543, 69.1936, 88.8215,  ..., 41.7148, 64.3598, 38.2530]]],\n",
      "\n",
      "\n",
      "        [[[43.4536, 69.1945, 88.8217,  ..., 41.7153, 64.3591, 38.2525]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.6072, 66.8888, 88.0572,  ..., 40.2426, 65.5101, 39.7686]]],\n",
      "\n",
      "\n",
      "        [[[44.9744, 69.1658, 94.5452,  ..., 40.0814, 59.1379, 39.3345]]],\n",
      "\n",
      "\n",
      "        [[[44.6079, 66.8880, 88.0570,  ..., 40.2421, 65.5107, 39.7691]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.0708, 70.0419, 94.2205,  ..., 35.8660, 57.3136, 45.1848]]],\n",
      "\n",
      "\n",
      "        [[[44.6077, 66.8905, 88.0625,  ..., 40.2423, 65.5050, 39.7684]]],\n",
      "\n",
      "\n",
      "        [[[44.6080, 66.8880, 88.0570,  ..., 40.2420, 65.5107, 39.7691]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[45.6336, 68.2295, 89.9907,  ..., 39.5820, 54.1153, 34.1808]]],\n",
      "\n",
      "\n",
      "        [[[45.6343, 68.2287, 89.9905,  ..., 39.5815, 54.1158, 34.1812]]],\n",
      "\n",
      "\n",
      "        [[[44.6076, 66.8884, 88.0572,  ..., 40.2423, 65.5103, 39.7689]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.9744, 69.1658, 94.5452,  ..., 40.0814, 59.1379, 39.3345]]],\n",
      "\n",
      "\n",
      "        [[[45.6314, 68.2322, 89.9915,  ..., 39.5835, 54.1134, 34.1796]]],\n",
      "\n",
      "\n",
      "        [[[44.6076, 66.8884, 88.0572,  ..., 40.2423, 65.5103, 39.7689]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 34, Loss: 27437.47265625\n",
      "Epoch 34, Loss: 17716.4375\n",
      "tensor([[[[41.3343, 68.5272, 93.5992,  ..., 35.0425, 58.0165, 45.1964]]],\n",
      "\n",
      "\n",
      "        [[[41.3369, 68.5241, 93.5981,  ..., 35.0406, 58.0191, 45.1979]]],\n",
      "\n",
      "\n",
      "        [[[44.7731, 65.4424, 87.4603,  ..., 39.5245, 66.0911, 39.8288]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.3343, 68.5272, 93.5992,  ..., 35.0425, 58.0165, 45.1964]]],\n",
      "\n",
      "\n",
      "        [[[44.7729, 65.4426, 87.4604,  ..., 39.5247, 66.0909, 39.8287]]],\n",
      "\n",
      "\n",
      "        [[[44.7731, 65.4424, 87.4603,  ..., 39.5245, 66.0911, 39.8288]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.7727, 65.4429, 87.4605,  ..., 39.5248, 66.0907, 39.8285]]],\n",
      "\n",
      "\n",
      "        [[[45.8725, 66.7572, 89.3120,  ..., 38.8008, 54.8094, 34.2339]]],\n",
      "\n",
      "\n",
      "        [[[45.8727, 66.7569, 89.3120,  ..., 38.8007, 54.8096, 34.2340]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.7729, 65.4454, 87.4668,  ..., 39.5248, 66.0844, 39.8279]]],\n",
      "\n",
      "\n",
      "        [[[44.7727, 65.4429, 87.4605,  ..., 39.5248, 66.0907, 39.8285]]],\n",
      "\n",
      "\n",
      "        [[[45.2035, 67.7136, 93.9147,  ..., 39.3309, 59.8105, 39.3859]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.7231, 65.3026, 87.1014,  ..., 39.6555, 65.2502, 38.9613]]],\n",
      "\n",
      "\n",
      "        [[[43.7220, 65.3039, 87.1017,  ..., 39.6563, 65.2492, 38.9605]]],\n",
      "\n",
      "\n",
      "        [[[44.1862, 67.5975, 93.5260,  ..., 39.4462, 59.0260, 38.4897]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.2332, 68.4793, 93.2470,  ..., 35.1756, 57.1253, 44.1668]]],\n",
      "\n",
      "\n",
      "        [[[43.7223, 65.3035, 87.1016,  ..., 39.6561, 65.2495, 38.9608]]],\n",
      "\n",
      "\n",
      "        [[[44.1862, 67.5975, 93.5260,  ..., 39.4462, 59.0260, 38.4897]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.7227, 65.3032, 87.1017,  ..., 39.6558, 65.2496, 38.9610]]],\n",
      "\n",
      "\n",
      "        [[[44.8430, 66.6501, 88.8839,  ..., 38.9088, 54.0193, 33.3180]]],\n",
      "\n",
      "\n",
      "        [[[43.7227, 65.3032, 87.1016,  ..., 39.6558, 65.2497, 38.9610]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.1862, 67.5975, 93.5260,  ..., 39.4462, 59.0260, 38.4897]]],\n",
      "\n",
      "\n",
      "        [[[44.8404, 66.6533, 88.8849,  ..., 38.9106, 54.0170, 33.3166]]],\n",
      "\n",
      "\n",
      "        [[[43.7227, 65.3032, 87.1016,  ..., 39.6558, 65.2497, 38.9610]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 35, Loss: 19119.53125\n",
      "Epoch 35, Loss: 16529.552734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.61it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[43.8675, 66.5623, 87.3699,  ..., 38.2840, 53.2318, 33.0903]]],\n",
      "\n",
      "\n",
      "        [[[42.7251, 65.1836, 85.7047,  ..., 39.0853, 64.4100, 38.7506]]],\n",
      "\n",
      "\n",
      "        [[[42.7250, 65.1839, 85.7052,  ..., 39.0853, 64.4095, 38.7505]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.8667, 66.5632, 87.3702,  ..., 38.2845, 53.2312, 33.0899]]],\n",
      "\n",
      "\n",
      "        [[[43.8643, 66.5661, 87.3711,  ..., 38.2861, 53.2291, 33.0886]]],\n",
      "\n",
      "\n",
      "        [[[43.8674, 66.5624, 87.3700,  ..., 38.2840, 53.2317, 33.0903]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.7244, 65.1844, 85.7049,  ..., 39.0858, 64.4095, 38.7502]]],\n",
      "\n",
      "\n",
      "        [[[39.1741, 68.4677, 91.7329,  ..., 34.5335, 56.2219, 43.8691]]],\n",
      "\n",
      "\n",
      "        [[[43.2203, 67.5030, 92.0729,  ..., 38.8430, 58.2402, 38.2662]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.1909, 68.4479, 91.7261,  ..., 34.5214, 56.2389, 43.8787]]],\n",
      "\n",
      "\n",
      "        [[[39.1916, 68.4472, 91.7258,  ..., 34.5209, 56.2396, 43.8791]]],\n",
      "\n",
      "\n",
      "        [[[43.2203, 67.5029, 92.0729,  ..., 38.8430, 58.2402, 38.2662]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.2843, 64.6252, 84.1474,  ..., 37.7608, 64.1978, 39.5390]]],\n",
      "\n",
      "\n",
      "        [[[42.2843, 64.6252, 84.1473,  ..., 37.7607, 64.1979, 39.5391]]],\n",
      "\n",
      "\n",
      "        [[[38.7710, 67.9236, 90.0240,  ..., 33.0235, 56.0578, 44.7126]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.2843, 64.6252, 84.1474,  ..., 37.7607, 64.1978, 39.5391]]],\n",
      "\n",
      "\n",
      "        [[[42.2827, 64.6271, 84.1478,  ..., 37.7620, 64.1963, 39.5380]]],\n",
      "\n",
      "\n",
      "        [[[38.7745, 67.9195, 90.0227,  ..., 33.0209, 56.0614, 44.7146]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.2840, 64.6256, 84.1476,  ..., 37.7610, 64.1974, 39.5388]]],\n",
      "\n",
      "\n",
      "        [[[43.4694, 66.0177, 85.6867,  ..., 36.8741, 53.0994, 33.9057]]],\n",
      "\n",
      "\n",
      "        [[[42.2840, 64.6257, 84.1478,  ..., 37.7610, 64.1972, 39.5388]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.8227, 66.9601, 90.4548,  ..., 37.4702, 58.0944, 39.0647]]],\n",
      "\n",
      "\n",
      "        [[[42.2841, 64.6257, 84.1479,  ..., 37.7609, 64.1972, 39.5388]]],\n",
      "\n",
      "\n",
      "        [[[42.8227, 66.9601, 90.4548,  ..., 37.4702, 58.0944, 39.0647]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 36, Loss: 16144.029296875\n",
      "Epoch 36, Loss: 13561.220703125\n",
      "tensor([[[[42.0611, 63.6606, 83.4273,  ..., 37.2082, 64.4962, 40.4827]]],\n",
      "\n",
      "\n",
      "        [[[42.6485, 66.0016, 89.6921,  ..., 36.8895, 58.4705, 40.0213]]],\n",
      "\n",
      "\n",
      "        [[[38.6002, 66.9343, 89.2576,  ..., 32.3895, 56.4581, 45.7258]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.5975, 66.9374, 89.2587,  ..., 32.3913, 56.4554, 45.7242]]],\n",
      "\n",
      "\n",
      "        [[[42.0609, 63.6609, 83.4274,  ..., 37.2084, 64.4959, 40.4826]]],\n",
      "\n",
      "\n",
      "        [[[42.0606, 63.6605, 83.4260,  ..., 37.2086, 64.4972, 40.4826]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.0615, 63.6654, 83.4381,  ..., 37.2082, 64.4854, 40.4815]]],\n",
      "\n",
      "\n",
      "        [[[42.0607, 63.6611, 83.4276,  ..., 37.2085, 64.4957, 40.4825]]],\n",
      "\n",
      "\n",
      "        [[[42.0607, 63.6612, 83.4276,  ..., 37.2085, 64.4956, 40.4825]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.2985, 65.0462, 84.8763,  ..., 36.2706, 53.5035, 34.8844]]],\n",
      "\n",
      "\n",
      "        [[[43.3008, 65.0433, 84.8755,  ..., 36.2690, 53.5055, 34.8857]]],\n",
      "\n",
      "\n",
      "        [[[43.3017, 65.0423, 84.8752,  ..., 36.2685, 53.5062, 34.8861]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.5111, 62.5722, 83.9950,  ..., 38.5202, 64.5526, 41.0659]]],\n",
      "\n",
      "\n",
      "        [[[42.7881, 63.9396, 85.4080,  ..., 37.6133, 53.6604, 35.4893]]],\n",
      "\n",
      "\n",
      "        [[[41.5111, 62.5723, 83.9952,  ..., 38.5202, 64.5525, 41.0659]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.7886, 63.9390, 85.4079,  ..., 37.6130, 53.6609, 35.4896]]],\n",
      "\n",
      "\n",
      "        [[[41.5111, 62.5722, 83.9950,  ..., 38.5202, 64.5526, 41.0659]]],\n",
      "\n",
      "\n",
      "        [[[41.5111, 62.5723, 83.9953,  ..., 38.5202, 64.5524, 41.0659]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.5102, 62.5732, 83.9952,  ..., 38.5209, 64.5520, 41.0654]]],\n",
      "\n",
      "\n",
      "        [[[41.5114, 62.5719, 83.9949,  ..., 38.5200, 64.5529, 41.0661]]],\n",
      "\n",
      "\n",
      "        [[[38.0579, 65.8057, 89.9351,  ..., 33.8510, 56.5870, 46.3346]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[38.0554, 65.8088, 89.9361,  ..., 33.8528, 56.5844, 46.3332]]],\n",
      "\n",
      "\n",
      "        [[[38.0554, 65.8087, 89.9361,  ..., 33.8528, 56.5844, 46.3332]]],\n",
      "\n",
      "\n",
      "        [[[41.5108, 62.5725, 83.9949,  ..., 38.5205, 64.5525, 41.0657]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 37, Loss: 18539.7099609375\n",
      "Epoch 37, Loss: 20437.1796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.60it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[42.6234, 63.6309, 87.3453,  ..., 39.7322, 53.3087, 35.7241]]],\n",
      "\n",
      "\n",
      "        [[[42.6252, 63.6289, 87.3446,  ..., 39.7310, 53.3102, 35.7250]]],\n",
      "\n",
      "\n",
      "        [[[42.6247, 63.6294, 87.3448,  ..., 39.7313, 53.3098, 35.7248]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.2971, 62.2456, 85.9089,  ..., 40.5723, 64.1223, 41.2965]]],\n",
      "\n",
      "\n",
      "        [[[41.2971, 62.2458, 85.9092,  ..., 40.5723, 64.1219, 41.2964]]],\n",
      "\n",
      "\n",
      "        [[[42.6243, 63.6299, 87.3449,  ..., 39.7315, 53.3095, 35.7246]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.2982, 62.2504, 85.9207,  ..., 40.5719, 64.1110, 41.2954]]],\n",
      "\n",
      "\n",
      "        [[[41.2974, 62.2453, 85.9087,  ..., 40.5721, 64.1227, 41.2966]]],\n",
      "\n",
      "\n",
      "        [[[41.9801, 64.6088, 92.1702,  ..., 40.3074, 58.2324, 40.8369]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.2974, 62.2452, 85.9086,  ..., 40.5720, 64.1227, 41.2967]]],\n",
      "\n",
      "\n",
      "        [[[37.8912, 65.5322, 92.1244,  ..., 36.1465, 56.1707, 46.5467]]],\n",
      "\n",
      "\n",
      "        [[[41.2968, 62.2459, 85.9087,  ..., 40.5725, 64.1223, 41.2963]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.1587, 65.6198, 93.7987,  ..., 36.7271, 55.7772, 47.0384]]],\n",
      "\n",
      "\n",
      "        [[[39.1602, 65.6180, 93.7981,  ..., 36.7260, 55.7788, 47.0393]]],\n",
      "\n",
      "\n",
      "        [[[42.3581, 62.2489, 87.3681,  ..., 41.0988, 63.7067, 41.7723]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.1623, 65.6156, 93.7973,  ..., 36.7245, 55.7810, 47.0405]]],\n",
      "\n",
      "\n",
      "        [[[39.1586, 65.6199, 93.7987,  ..., 36.7272, 55.7772, 47.0384]]],\n",
      "\n",
      "\n",
      "        [[[39.1614, 65.6167, 93.7976,  ..., 36.7252, 55.7799, 47.0400]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.3578, 62.2493, 87.3684,  ..., 41.0990, 63.7062, 41.7721]]],\n",
      "\n",
      "\n",
      "        [[[42.3578, 62.2493, 87.3684,  ..., 41.0990, 63.7062, 41.7721]]],\n",
      "\n",
      "\n",
      "        [[[43.7904, 63.6624, 88.8024,  ..., 40.2563, 52.9768, 36.2180]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.1242, 64.6391, 93.6236,  ..., 40.8341, 57.8829, 41.3151]]],\n",
      "\n",
      "\n",
      "        [[[42.3577, 62.2498, 87.3693,  ..., 41.0991, 63.7052, 41.7719]]],\n",
      "\n",
      "\n",
      "        [[[42.3579, 62.2494, 87.3688,  ..., 41.0989, 63.7059, 41.7721]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 38, Loss: 16732.99755859375\n",
      "Epoch 38, Loss: 19579.830078125\n",
      "tensor([[[[43.4722, 62.4579, 87.8792,  ..., 40.8509, 62.6556, 42.6487]]],\n",
      "\n",
      "\n",
      "        [[[44.3223, 64.8779, 94.0988,  ..., 40.5664, 56.8886, 42.2021]]],\n",
      "\n",
      "\n",
      "        [[[43.4717, 62.4566, 87.8756,  ..., 40.8510, 62.6589, 42.6490]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.3223, 64.8779, 94.0988,  ..., 40.5664, 56.8886, 42.2021]]],\n",
      "\n",
      "\n",
      "        [[[43.4722, 62.4579, 87.8792,  ..., 40.8508, 62.6556, 42.6488]]],\n",
      "\n",
      "\n",
      "        [[[40.4850, 65.9310, 94.4035,  ..., 36.4400, 54.6790, 47.9823]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.4719, 62.4583, 87.8794,  ..., 40.8511, 62.6553, 42.6485]]],\n",
      "\n",
      "\n",
      "        [[[43.4720, 62.4585, 87.8799,  ..., 40.8511, 62.6548, 42.6485]]],\n",
      "\n",
      "\n",
      "        [[[43.4735, 62.4640, 87.8932,  ..., 40.8507, 62.6422, 42.6473]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[45.0111, 63.9050, 89.2680,  ..., 39.9734, 51.9869, 37.1302]]],\n",
      "\n",
      "\n",
      "        [[[45.0108, 63.9053, 89.2681,  ..., 39.9736, 51.9867, 37.1301]]],\n",
      "\n",
      "\n",
      "        [[[45.0094, 63.9069, 89.2686,  ..., 39.9745, 51.9856, 37.1293]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.6970, 63.1613, 87.4667,  ..., 41.2168, 61.0874, 43.3122]]],\n",
      "\n",
      "\n",
      "        [[[40.8129, 66.7872, 93.9671,  ..., 36.8416, 53.0068, 48.6904]]],\n",
      "\n",
      "\n",
      "        [[[40.8126, 66.7876, 93.9672,  ..., 36.8418, 53.0065, 48.6903]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.6973, 63.1611, 87.4669,  ..., 41.2167, 61.0873, 43.3123]]],\n",
      "\n",
      "\n",
      "        [[[43.6965, 63.1619, 87.4668,  ..., 41.2173, 61.0869, 43.3119]]],\n",
      "\n",
      "\n",
      "        [[[43.6966, 63.1618, 87.4667,  ..., 41.2172, 61.0870, 43.3119]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.6973, 63.1615, 87.4676,  ..., 41.2167, 61.0865, 43.3122]]],\n",
      "\n",
      "\n",
      "        [[[44.6102, 65.6198, 93.6202,  ..., 40.9219, 55.3715, 42.8713]]],\n",
      "\n",
      "\n",
      "        [[[43.6973, 63.1613, 87.4673,  ..., 41.2167, 61.0868, 43.3123]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.6102, 65.6198, 93.6202,  ..., 40.9219, 55.3715, 42.8713]]],\n",
      "\n",
      "\n",
      "        [[[44.6102, 65.6198, 93.6202,  ..., 40.9219, 55.3715, 42.8713]]],\n",
      "\n",
      "\n",
      "        [[[45.3008, 64.6599, 88.7656,  ..., 40.3319, 50.4609, 37.8211]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 39, Loss: 14868.154296875\n",
      "Epoch 39, Loss: 15129.3759765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.08it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.17it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[40.2077, 67.5120, 92.9262,  ..., 37.1837, 51.9321, 48.4609]]],\n",
      "\n",
      "\n",
      "        [[[43.0957, 63.7484, 86.5205,  ..., 41.5227, 60.0466, 43.1399]]],\n",
      "\n",
      "\n",
      "        [[[43.0957, 63.7452, 86.5149,  ..., 41.5224, 60.0523, 43.1408]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.0957, 63.7483, 86.5205,  ..., 41.5227, 60.0467, 43.1399]]],\n",
      "\n",
      "\n",
      "        [[[40.2077, 67.5120, 92.9262,  ..., 37.1837, 51.9322, 48.4609]]],\n",
      "\n",
      "\n",
      "        [[[40.2098, 67.5095, 92.9254,  ..., 37.1822, 51.9343, 48.4621]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.0499, 66.2383, 92.5834,  ..., 41.2123, 54.4049, 42.6855]]],\n",
      "\n",
      "\n",
      "        [[[43.0959, 63.7496, 86.5230,  ..., 41.5227, 60.0442, 43.1396]]],\n",
      "\n",
      "\n",
      "        [[[44.7276, 65.2880, 87.6995,  ..., 40.6299, 49.4951, 37.6410]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.0961, 63.7483, 86.5210,  ..., 41.5225, 60.0463, 43.1400]]],\n",
      "\n",
      "\n",
      "        [[[44.7265, 65.2893, 87.6998,  ..., 40.6307, 49.4942, 37.6404]]],\n",
      "\n",
      "\n",
      "        [[[43.0960, 63.7482, 86.5209,  ..., 41.5225, 60.0465, 43.1400]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.4689, 63.9069, 85.4469,  ..., 40.8976, 60.5945, 42.3096]]],\n",
      "\n",
      "\n",
      "        [[[42.4715, 63.9154, 85.4659,  ..., 40.8969, 60.5766, 42.3078]]],\n",
      "\n",
      "\n",
      "        [[[39.5756, 67.7410, 91.7248,  ..., 36.4819, 52.6604, 47.4993]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5772, 67.7391, 91.7241,  ..., 36.4808, 52.6620, 47.5002]]],\n",
      "\n",
      "\n",
      "        [[[39.5765, 67.7399, 91.7244,  ..., 36.4813, 52.6613, 47.4998]]],\n",
      "\n",
      "\n",
      "        [[[39.5756, 67.7410, 91.7248,  ..., 36.4820, 52.6604, 47.4993]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.1273, 65.4586, 86.4859,  ..., 39.9573, 50.2039, 36.7771]]],\n",
      "\n",
      "\n",
      "        [[[44.1269, 65.4590, 86.4860,  ..., 39.9575, 50.2036, 36.7769]]],\n",
      "\n",
      "\n",
      "        [[[43.4598, 66.4089, 91.3982,  ..., 40.5473, 55.0796, 41.8286]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.4689, 63.9071, 85.4472,  ..., 40.8976, 60.5941, 42.3096]]],\n",
      "\n",
      "\n",
      "        [[[44.1278, 65.4580, 86.4857,  ..., 39.9569, 50.2044, 36.7774]]],\n",
      "\n",
      "\n",
      "        [[[42.4689, 63.9070, 85.4471,  ..., 40.8976, 60.5942, 42.3096]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 40, Loss: 14694.5673828125\n",
      "Epoch 40, Loss: 9801.44970703125\n",
      "tensor([[[[42.6271, 64.0698, 84.4345,  ..., 39.3383, 62.5014, 41.1946]]],\n",
      "\n",
      "\n",
      "        [[[42.6263, 64.0703, 84.4342,  ..., 39.3388, 62.5012, 41.1942]]],\n",
      "\n",
      "\n",
      "        [[[39.8228, 67.9589, 90.5690,  ..., 34.7321, 54.9366, 46.2244]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.6264, 64.0703, 84.4342,  ..., 39.3388, 62.5012, 41.1943]]],\n",
      "\n",
      "\n",
      "        [[[42.6267, 64.0704, 84.4348,  ..., 39.3386, 62.5007, 41.1943]]],\n",
      "\n",
      "\n",
      "        [[[42.6260, 64.0707, 84.4342,  ..., 39.3391, 62.5010, 41.1940]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.3413, 65.6229, 85.3154,  ..., 38.3114, 52.3505, 35.6194]]],\n",
      "\n",
      "\n",
      "        [[[42.6267, 64.0706, 84.4352,  ..., 39.3386, 62.5003, 41.1942]]],\n",
      "\n",
      "\n",
      "        [[[44.3440, 65.6196, 85.3145,  ..., 38.3096, 52.3528, 35.6208]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.3439, 65.6198, 85.3145,  ..., 38.3097, 52.3527, 35.6208]]],\n",
      "\n",
      "\n",
      "        [[[42.6268, 64.0708, 84.4356,  ..., 39.3386, 62.5000, 41.1942]]],\n",
      "\n",
      "\n",
      "        [[[44.3439, 65.6196, 85.3144,  ..., 38.3096, 52.3527, 35.6208]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.6511, 64.3230, 83.5414,  ..., 38.3048, 63.6912, 40.5917]]],\n",
      "\n",
      "\n",
      "        [[[42.6556, 64.3362, 83.5696,  ..., 38.3032, 63.6651, 40.5886]]],\n",
      "\n",
      "\n",
      "        [[[44.4184, 65.8544, 84.2361,  ..., 37.2113, 53.7837, 35.0058]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.4194, 65.8532, 84.2357,  ..., 37.2106, 53.7845, 35.0064]]],\n",
      "\n",
      "\n",
      "        [[[44.4174, 65.8555, 84.2363,  ..., 37.2119, 53.7828, 35.0053]]],\n",
      "\n",
      "\n",
      "        [[[43.7395, 66.8009, 89.2026,  ..., 37.8422, 58.5498, 40.0700]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.6556, 64.3362, 83.5696,  ..., 38.3032, 63.6651, 40.5886]]],\n",
      "\n",
      "\n",
      "        [[[42.6515, 64.3223, 83.5408,  ..., 38.3046, 63.6921, 40.5920]]],\n",
      "\n",
      "\n",
      "        [[[39.9173, 68.2555, 89.5121,  ..., 33.5717, 56.4421, 45.5317]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.9188, 68.2537, 89.5114,  ..., 33.5706, 56.4436, 45.5326]]],\n",
      "\n",
      "\n",
      "        [[[42.6511, 64.3229, 83.5414,  ..., 38.3048, 63.6913, 40.5917]]],\n",
      "\n",
      "\n",
      "        [[[42.6499, 64.3236, 83.5405,  ..., 38.3057, 63.6914, 40.5912]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 41, Loss: 8761.721923828125\n",
      "Epoch 41, Loss: 13002.30908203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.62it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[42.4771, 64.9893, 83.0040,  ..., 38.5126, 63.5430, 40.3725]]],\n",
      "\n",
      "\n",
      "        [[[42.4770, 64.9890, 83.0033,  ..., 38.5127, 63.5436, 40.3726]]],\n",
      "\n",
      "\n",
      "        [[[44.2819, 66.4901, 83.4816,  ..., 37.4090, 53.8635, 34.8011]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.2807, 66.4916, 83.4821,  ..., 37.4099, 53.8624, 34.8004]]],\n",
      "\n",
      "\n",
      "        [[[44.2823, 66.4896, 83.4815,  ..., 37.4087, 53.8638, 34.8013]]],\n",
      "\n",
      "\n",
      "        [[[43.6017, 67.4250, 88.4725,  ..., 38.0310, 58.6090, 39.8571]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.4765, 64.9883, 83.0017,  ..., 38.5129, 63.5450, 40.3727]]],\n",
      "\n",
      "\n",
      "        [[[39.7836, 68.9843, 88.8027,  ..., 33.8040, 56.4950, 45.2798]]],\n",
      "\n",
      "\n",
      "        [[[39.7851, 68.9826, 88.8021,  ..., 33.8029, 56.4965, 45.2807]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.4772, 64.9879, 83.0021,  ..., 38.5124, 63.5450, 40.3730]]],\n",
      "\n",
      "\n",
      "        [[[39.7856, 68.9820, 88.8019,  ..., 33.8026, 56.4969, 45.2809]]],\n",
      "\n",
      "\n",
      "        [[[42.4772, 64.9883, 83.0026,  ..., 38.5125, 63.5445, 40.3729]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.5898, 69.5109, 88.8217,  ..., 34.5913, 55.9119, 45.2948]]],\n",
      "\n",
      "\n",
      "        [[[42.2551, 65.5220, 83.1920,  ..., 39.2169, 62.7529, 40.3771]]],\n",
      "\n",
      "\n",
      "        [[[43.4073, 67.8638, 88.4119,  ..., 38.7285, 58.0851, 39.8881]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.5897, 69.5109, 88.8217,  ..., 34.5913, 55.9118, 45.2948]]],\n",
      "\n",
      "\n",
      "        [[[42.2550, 65.5225, 83.1927,  ..., 39.2170, 62.7521, 40.3769]]],\n",
      "\n",
      "\n",
      "        [[[39.5896, 69.5111, 88.8218,  ..., 34.5914, 55.9117, 45.2947]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.4073, 67.8638, 88.4119,  ..., 38.7285, 58.0851, 39.8881]]],\n",
      "\n",
      "\n",
      "        [[[44.0891, 66.9376, 83.4059,  ..., 38.1243, 53.3502, 34.8447]]],\n",
      "\n",
      "\n",
      "        [[[42.2641, 65.5483, 83.2451,  ..., 39.2138, 62.7039, 40.3707]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.2548, 65.5229, 83.1929,  ..., 39.2172, 62.7518, 40.3768]]],\n",
      "\n",
      "\n",
      "        [[[44.0893, 66.9373, 83.4058,  ..., 38.1242, 53.3504, 34.8448]]],\n",
      "\n",
      "\n",
      "        [[[42.2548, 65.5229, 83.1929,  ..., 39.2172, 62.7518, 40.3768]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 42, Loss: 13360.42626953125\n",
      "Epoch 42, Loss: 11112.7099609375\n",
      "tensor([[[[42.7744, 66.0508, 83.9874,  ..., 39.7692, 62.9745, 40.3157]]],\n",
      "\n",
      "\n",
      "        [[[42.7897, 66.0910, 84.0686,  ..., 39.7638, 62.9008, 40.3060]]],\n",
      "\n",
      "\n",
      "        [[[42.7743, 66.0512, 83.9880,  ..., 39.7693, 62.9738, 40.3155]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.9563, 68.1966, 88.8152,  ..., 39.2681, 58.7476, 39.8827]]],\n",
      "\n",
      "\n",
      "        [[[43.9563, 68.1966, 88.8152,  ..., 39.2681, 58.7476, 39.8827]]],\n",
      "\n",
      "\n",
      "        [[[40.2097, 69.9205, 89.3436,  ..., 35.2031, 56.6329, 45.2696]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.7742, 66.0521, 83.9892,  ..., 39.7694, 62.9725, 40.3152]]],\n",
      "\n",
      "\n",
      "        [[[42.7744, 66.0523, 83.9897,  ..., 39.7693, 62.9721, 40.3152]]],\n",
      "\n",
      "\n",
      "        [[[42.7746, 66.0529, 83.9909,  ..., 39.7692, 62.9711, 40.3150]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.6518, 67.2784, 83.7996,  ..., 38.6780, 54.0481, 34.8498]]],\n",
      "\n",
      "\n",
      "        [[[44.6511, 67.2793, 83.8000,  ..., 38.6786, 54.0474, 34.8493]]],\n",
      "\n",
      "\n",
      "        [[[42.7742, 66.0521, 83.9892,  ..., 39.7694, 62.9725, 40.3152]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.9556, 69.7364, 89.8516,  ..., 35.2439, 57.6585, 45.4114]]],\n",
      "\n",
      "\n",
      "        [[[44.6187, 67.9866, 89.2100,  ..., 39.2874, 59.6847, 40.0310]]],\n",
      "\n",
      "\n",
      "        [[[43.4295, 66.1766, 84.9733,  ..., 39.8118, 63.2794, 40.3650]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.9557, 69.7362, 89.8516,  ..., 35.2438, 57.6587, 45.4114]]],\n",
      "\n",
      "\n",
      "        [[[43.4295, 66.1770, 84.9738,  ..., 39.8118, 63.2789, 40.3649]]],\n",
      "\n",
      "\n",
      "        [[[40.9556, 69.7364, 89.8516,  ..., 35.2439, 57.6585, 45.4114]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[45.3334, 67.0643, 84.1806,  ..., 38.6994, 55.0300, 35.0114]]],\n",
      "\n",
      "\n",
      "        [[[43.4298, 66.1786, 84.9765,  ..., 39.8118, 63.2763, 40.3644]]],\n",
      "\n",
      "\n",
      "        [[[43.4309, 66.1799, 84.9799,  ..., 39.8113, 63.2736, 40.3642]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[45.3338, 67.0639, 84.1805,  ..., 38.6992, 55.0303, 35.0116]]],\n",
      "\n",
      "\n",
      "        [[[43.4297, 66.1787, 84.9766,  ..., 39.8119, 63.2762, 40.3644]]],\n",
      "\n",
      "\n",
      "        [[[43.4301, 66.1792, 84.9778,  ..., 39.8117, 63.2752, 40.3643]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 43, Loss: 13057.31201171875\n",
      "Epoch 43, Loss: 9916.67822265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.19it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[43.6975, 65.9996, 85.0611,  ..., 39.9163, 62.8384, 40.4833]]],\n",
      "\n",
      "\n",
      "        [[[43.6975, 66.0002, 85.0619,  ..., 39.9163, 62.8376, 40.4831]]],\n",
      "\n",
      "\n",
      "        [[[44.8684, 67.4297, 88.6421,  ..., 39.3747, 59.8975, 40.2619]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.8684, 67.4297, 88.6421,  ..., 39.3747, 59.8975, 40.2619]]],\n",
      "\n",
      "\n",
      "        [[[41.2515, 69.1743, 89.3018,  ..., 35.3589, 57.8913, 45.6434]]],\n",
      "\n",
      "\n",
      "        [[[41.2516, 69.1743, 89.3018,  ..., 35.3589, 57.8913, 45.6435]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[45.5968, 66.4975, 83.5781,  ..., 38.7901, 55.2734, 35.2568]]],\n",
      "\n",
      "\n",
      "        [[[45.5960, 66.4984, 83.5783,  ..., 38.7906, 55.2727, 35.2564]]],\n",
      "\n",
      "\n",
      "        [[[45.5965, 66.4979, 83.5782,  ..., 38.7903, 55.2731, 35.2566]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.7544, 66.0977, 85.2764,  ..., 39.8927, 62.6540, 40.4629]]],\n",
      "\n",
      "\n",
      "        [[[43.6973, 66.0006, 85.0623,  ..., 39.9165, 62.8371, 40.4829]]],\n",
      "\n",
      "\n",
      "        [[[45.5970, 66.4973, 83.5780,  ..., 38.7899, 55.2736, 35.2569]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[40.5479, 68.7914, 88.1826,  ..., 36.0529, 56.4046, 45.8382]]],\n",
      "\n",
      "\n",
      "        [[[43.0740, 65.8716, 84.4638,  ..., 40.5251, 61.0332, 40.6041]]],\n",
      "\n",
      "\n",
      "        [[[43.0745, 65.8735, 84.4671,  ..., 40.5250, 61.0301, 40.6035]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.2164, 67.0187, 87.5504,  ..., 39.9799, 58.5542, 40.4671]]],\n",
      "\n",
      "\n",
      "        [[[43.0746, 65.8723, 84.4654,  ..., 40.5249, 61.0319, 40.6040]]],\n",
      "\n",
      "\n",
      "        [[[40.5601, 68.7772, 88.1777,  ..., 36.0442, 56.4167, 45.8450]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.2164, 67.0187, 87.5504,  ..., 39.9799, 58.5542, 40.4671]]],\n",
      "\n",
      "\n",
      "        [[[43.0760, 65.8822, 84.4814,  ..., 40.5250, 61.0165, 40.6009]]],\n",
      "\n",
      "\n",
      "        [[[43.0836, 65.9040, 84.5219,  ..., 40.5227, 60.9797, 40.5952]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.1626, 65.9942, 84.7510,  ..., 40.4862, 60.7926, 40.5820]]],\n",
      "\n",
      "\n",
      "        [[[43.0747, 65.8735, 84.4673,  ..., 40.5249, 61.0300, 40.6036]]],\n",
      "\n",
      "\n",
      "        [[[43.0756, 65.8754, 84.4712,  ..., 40.5246, 61.0266, 40.6031]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 44, Loss: 13393.81982421875\n",
      "Epoch 44, Loss: 7731.12451171875\n",
      "tensor([[[[42.2088, 65.2692, 83.4601,  ..., 40.6199, 59.2049, 40.7347]]],\n",
      "\n",
      "\n",
      "        [[[43.3467, 66.3687, 86.4432,  ..., 40.0726, 56.8021, 40.6084]]],\n",
      "\n",
      "\n",
      "        [[[42.2091, 65.2694, 83.4608,  ..., 40.6198, 59.2043, 40.7346]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.6355, 68.1195, 87.0348,  ..., 36.1660, 54.4989, 45.9758]]],\n",
      "\n",
      "\n",
      "        [[[42.2088, 65.2692, 83.4602,  ..., 40.6199, 59.2048, 40.7346]]],\n",
      "\n",
      "\n",
      "        [[[42.2094, 65.2702, 83.4622,  ..., 40.6197, 59.2030, 40.7344]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.2105, 65.2732, 83.4678,  ..., 40.6194, 59.1979, 40.7336]]],\n",
      "\n",
      "\n",
      "        [[[44.0679, 65.4283, 81.2875,  ..., 39.5095, 52.1668, 35.6244]]],\n",
      "\n",
      "\n",
      "        [[[42.2065, 65.3005, 83.5012,  ..., 40.6243, 59.1616, 40.7235]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.0687, 65.4273, 81.2871,  ..., 39.5089, 52.1675, 35.6249]]],\n",
      "\n",
      "\n",
      "        [[[42.2093, 65.2712, 83.4634,  ..., 40.6198, 59.2017, 40.7341]]],\n",
      "\n",
      "\n",
      "        [[[44.0696, 65.4263, 81.2868,  ..., 39.5083, 52.1682, 35.6253]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.8382, 64.7027, 83.2733,  ..., 39.7847, 58.6796, 40.5514]]],\n",
      "\n",
      "\n",
      "        [[[41.8383, 64.7035, 83.2744,  ..., 39.7847, 58.6784, 40.5511]]],\n",
      "\n",
      "\n",
      "        [[[39.2677, 67.6047, 86.9561,  ..., 35.2621, 53.8729, 45.7248]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.2668, 67.6057, 86.9565,  ..., 35.2627, 53.8721, 45.7243]]],\n",
      "\n",
      "\n",
      "        [[[42.9887, 65.8437, 86.3078,  ..., 39.2254, 56.2308, 40.4040]]],\n",
      "\n",
      "\n",
      "        [[[39.2668, 67.6057, 86.9565,  ..., 35.2627, 53.8721, 45.7243]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.7182, 64.8999, 81.1240,  ..., 38.6491, 51.6136, 35.4221]]],\n",
      "\n",
      "\n",
      "        [[[43.7159, 64.9026, 81.1248,  ..., 38.6506, 51.6116, 35.4210]]],\n",
      "\n",
      "\n",
      "        [[[41.8400, 64.7108, 83.2866,  ..., 39.7845, 58.6670, 40.5490]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.8403, 64.7073, 83.2823,  ..., 39.7839, 58.6716, 40.5502]]],\n",
      "\n",
      "\n",
      "        [[[42.9887, 65.8437, 86.3078,  ..., 39.2254, 56.2308, 40.4040]]],\n",
      "\n",
      "\n",
      "        [[[41.9385, 64.8356, 83.5845,  ..., 39.7392, 58.4187, 40.5270]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "Epoch 45, Loss: 10610.75830078125\n",
      "Epoch 45, Loss: 7074.058837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.90it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 19.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.86it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[39.3700, 67.9104, 88.4233,  ..., 34.2668, 54.3662, 45.1807]]],\n",
      "\n",
      "\n",
      "        [[[41.8950, 64.9113, 84.5337,  ..., 38.8674, 59.1026, 40.0935]]],\n",
      "\n",
      "\n",
      "        [[[42.0051, 65.0537, 84.8681,  ..., 38.8164, 58.8270, 40.0668]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.8942, 64.9106, 84.5317,  ..., 38.8679, 59.1041, 40.0936]]],\n",
      "\n",
      "\n",
      "        [[[41.8953, 64.9121, 84.5351,  ..., 38.8673, 59.1013, 40.0933]]],\n",
      "\n",
      "\n",
      "        [[[41.8953, 64.9126, 84.5359,  ..., 38.8674, 59.1006, 40.0931]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.0590, 66.0652, 87.5816,  ..., 38.2939, 56.6799, 39.9333]]],\n",
      "\n",
      "\n",
      "        [[[41.8966, 64.9154, 84.5414,  ..., 38.8669, 59.0957, 40.0924]]],\n",
      "\n",
      "\n",
      "        [[[41.8975, 64.9165, 84.5440,  ..., 38.8665, 59.0936, 40.0922]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.7998, 65.1398, 82.3979,  ..., 37.7072, 52.0964, 34.9449]]],\n",
      "\n",
      "\n",
      "        [[[41.8965, 64.9153, 84.5410,  ..., 38.8669, 59.0960, 40.0924]]],\n",
      "\n",
      "\n",
      "        [[[41.8961, 64.9152, 84.5404,  ..., 38.8672, 59.0965, 40.0924]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.9568, 66.6547, 89.0901,  ..., 38.3195, 56.7688, 40.0653]]],\n",
      "\n",
      "\n",
      "        [[[41.7895, 65.5419, 86.1154,  ..., 38.8947, 59.0831, 40.2114]]],\n",
      "\n",
      "\n",
      "        [[[41.7950, 65.5470, 86.1293,  ..., 38.8919, 59.0723, 40.2108]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.7035, 65.7533, 83.9096,  ..., 37.7406, 52.2115, 35.0831]]],\n",
      "\n",
      "\n",
      "        [[[43.7022, 65.7549, 83.9100,  ..., 37.7415, 52.2104, 35.0824]]],\n",
      "\n",
      "\n",
      "        [[[41.7893, 65.5419, 86.1152,  ..., 38.8948, 59.0832, 40.2114]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[41.7889, 65.5399, 86.1119,  ..., 38.8948, 59.0863, 40.2120]]],\n",
      "\n",
      "\n",
      "        [[[42.9568, 66.6547, 89.0901,  ..., 38.3195, 56.7688, 40.0652]]],\n",
      "\n",
      "\n",
      "        [[[41.7894, 65.5406, 86.1135,  ..., 38.8946, 59.0850, 40.2119]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.9568, 66.6547, 89.0901,  ..., 38.3195, 56.7688, 40.0652]]],\n",
      "\n",
      "\n",
      "        [[[39.2797, 68.6207, 90.1479,  ..., 34.3209, 54.4622, 45.2965]]],\n",
      "\n",
      "\n",
      "        [[[42.9568, 66.6547, 89.0901,  ..., 38.3195, 56.7688, 40.0652]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 46, Loss: 8285.0478515625\n",
      "Epoch 46, Loss: 9024.18212890625\n",
      "tensor([[[[43.7678, 65.9792, 84.3590,  ..., 37.8145, 51.8475, 35.7377]]],\n",
      "\n",
      "\n",
      "        [[[41.8227, 65.6578, 86.4522,  ..., 38.9560, 58.8094, 40.8723]]],\n",
      "\n",
      "\n",
      "        [[[43.7681, 65.9788, 84.3589,  ..., 37.8142, 51.8478, 35.7379]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.0148, 66.8615, 89.5539,  ..., 38.3841, 56.3909, 40.7033]]],\n",
      "\n",
      "\n",
      "        [[[43.0148, 66.8615, 89.5538,  ..., 38.3841, 56.3909, 40.7033]]],\n",
      "\n",
      "\n",
      "        [[[43.7678, 65.9791, 84.3590,  ..., 37.8144, 51.8476, 35.7378]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.3637, 68.9120, 90.7289,  ..., 34.4170, 54.0450, 45.9691]]],\n",
      "\n",
      "\n",
      "        [[[43.0148, 66.8615, 89.5539,  ..., 38.3841, 56.3909, 40.7034]]],\n",
      "\n",
      "\n",
      "        [[[41.9546, 65.8209, 86.8362,  ..., 38.8957, 58.5011, 40.8432]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.8218, 65.6548, 86.4469,  ..., 38.9562, 58.8141, 40.8731]]],\n",
      "\n",
      "\n",
      "        [[[41.8212, 65.6531, 86.4439,  ..., 38.9564, 58.8168, 40.8736]]],\n",
      "\n",
      "\n",
      "        [[[41.8209, 65.6529, 86.4433,  ..., 38.9566, 58.8172, 40.8736]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.0322, 66.4986, 89.3918,  ..., 38.5630, 55.8170, 41.3054]]],\n",
      "\n",
      "\n",
      "        [[[41.7968, 65.1012, 85.9999,  ..., 39.1267, 58.5052, 41.5315]]],\n",
      "\n",
      "\n",
      "        [[[41.7955, 65.1265, 86.0325,  ..., 39.1302, 58.4718, 41.5224]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.7879, 65.6252, 84.1740,  ..., 38.0054, 51.2818, 36.3550]]],\n",
      "\n",
      "\n",
      "        [[[43.0322, 66.4986, 89.3918,  ..., 38.5630, 55.8170, 41.3054]]],\n",
      "\n",
      "\n",
      "        [[[41.7988, 65.1084, 86.0120,  ..., 39.1264, 58.4943, 41.5294]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.4008, 68.5788, 90.6275,  ..., 34.6375, 53.4108, 46.6023]]],\n",
      "\n",
      "\n",
      "        [[[41.7962, 65.0992, 85.9964,  ..., 39.1269, 58.5083, 41.5321]]],\n",
      "\n",
      "\n",
      "        [[[43.0322, 66.4986, 89.3918,  ..., 38.5630, 55.8170, 41.3054]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.7967, 65.0999, 85.9980,  ..., 39.1267, 58.5071, 41.5320]]],\n",
      "\n",
      "\n",
      "        [[[39.4028, 68.5764, 90.6267,  ..., 34.6361, 53.4128, 46.6034]]],\n",
      "\n",
      "\n",
      "        [[[39.4007, 68.5789, 90.6276,  ..., 34.6376, 53.4107, 46.6022]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 47, Loss: 8211.4521484375\n",
      "Epoch 47, Loss: 9111.2666015625\n",
      "tensor([[[[43.9317, 65.2212, 84.2336,  ..., 38.4492, 51.4411, 36.3490]]],\n",
      "\n",
      "\n",
      "        [[[41.9001, 64.5691, 85.8919,  ..., 39.5435, 58.7946, 41.5667]]],\n",
      "\n",
      "\n",
      "        [[[43.9313, 65.2216, 84.2337,  ..., 38.4494, 51.4408, 36.3488]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.9318, 65.2211, 84.2336,  ..., 38.4491, 51.4412, 36.3490]]],\n",
      "\n",
      "\n",
      "        [[[42.0275, 64.7468, 86.2843,  ..., 39.4910, 58.4760, 41.5306]]],\n",
      "\n",
      "\n",
      "        [[[41.8994, 64.5683, 85.8900,  ..., 39.5438, 58.7962, 41.5668]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.5687, 68.1962, 90.7897,  ..., 35.1317, 53.5545, 46.5640]]],\n",
      "\n",
      "\n",
      "        [[[39.5683, 68.1967, 90.7898,  ..., 35.1320, 53.5541, 46.5637]]],\n",
      "\n",
      "\n",
      "        [[[39.5706, 68.1940, 90.7889,  ..., 35.1303, 53.5564, 46.5650]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[41.8971, 64.5633, 85.8804,  ..., 39.5446, 58.8044, 41.5681]]],\n",
      "\n",
      "\n",
      "        [[[41.8985, 64.5651, 85.8846,  ..., 39.5440, 58.8011, 41.5678]]],\n",
      "\n",
      "\n",
      "        [[[42.0275, 64.7468, 86.2843,  ..., 39.4910, 58.4760, 41.5306]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.1796, 64.4347, 86.3775,  ..., 40.0329, 59.5908, 41.1547]]],\n",
      "\n",
      "\n",
      "        [[[42.1803, 64.4124, 86.3486,  ..., 40.0300, 59.6199, 41.1628]]],\n",
      "\n",
      "\n",
      "        [[[42.2037, 64.4539, 86.4316,  ..., 40.0217, 59.5509, 41.1527]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.1801, 64.4125, 86.3484,  ..., 40.0300, 59.6200, 41.1627]]],\n",
      "\n",
      "\n",
      "        [[[42.1817, 64.4157, 86.3547,  ..., 40.0296, 59.6147, 41.1619]]],\n",
      "\n",
      "\n",
      "        [[[42.1823, 64.4217, 86.3634,  ..., 40.0299, 59.6065, 41.1599]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.1802, 64.4111, 86.3467,  ..., 40.0298, 59.6217, 41.1632]]],\n",
      "\n",
      "\n",
      "        [[[39.9236, 68.1705, 91.4918,  ..., 35.7024, 54.3836, 46.0547]]],\n",
      "\n",
      "\n",
      "        [[[43.4837, 66.0055, 90.0267,  ..., 39.4899, 56.7206, 40.8558]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[43.4837, 66.0056, 90.0267,  ..., 39.4899, 56.7206, 40.8559]]],\n",
      "\n",
      "\n",
      "        [[[39.9254, 68.1684, 91.4911,  ..., 35.7011, 54.3853, 46.0558]]],\n",
      "\n",
      "\n",
      "        [[[42.1803, 64.4120, 86.3481,  ..., 40.0299, 59.6204, 41.1629]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 48, Loss: 7711.782470703125\n",
      "Epoch 48, Loss: 9159.30908203125\n",
      "tensor([[[[44.6109, 64.5194, 84.5698,  ..., 38.5921, 52.6665, 36.0891]]],\n",
      "\n",
      "\n",
      "        [[[42.4889, 63.6125, 85.8908,  ..., 39.6523, 60.2310, 41.3992]]],\n",
      "\n",
      "\n",
      "        [[[42.6321, 63.8208, 86.3354,  ..., 39.5982, 59.8731, 41.3526]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.4885, 63.6119, 85.8894,  ..., 39.6525, 60.2321, 41.3994]]],\n",
      "\n",
      "\n",
      "        [[[42.4926, 63.6156, 85.8992,  ..., 39.6506, 60.2248, 41.3988]]],\n",
      "\n",
      "\n",
      "        [[[42.4870, 63.6083, 85.8830,  ..., 39.6529, 60.2377, 41.4004]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.96it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[40.3318, 67.5361, 91.3445,  ..., 35.3144, 54.8196, 46.2252]]],\n",
      "\n",
      "\n",
      "        [[[42.4870, 63.6081, 85.8827,  ..., 39.6528, 60.2380, 41.4005]]],\n",
      "\n",
      "\n",
      "        [[[40.3314, 67.5366, 91.3447,  ..., 35.3147, 54.8192, 46.2249]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[40.3296, 67.5388, 91.3454,  ..., 35.3160, 54.8174, 46.2239]]],\n",
      "\n",
      "\n",
      "        [[[42.4870, 63.6073, 85.8817,  ..., 39.6527, 60.2389, 41.4007]]],\n",
      "\n",
      "\n",
      "        [[[43.8400, 65.3707, 89.8110,  ..., 39.1165, 57.1245, 41.0329]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.5397, 64.2161, 84.0991,  ..., 38.5888, 52.0682, 36.5485]]],\n",
      "\n",
      "\n",
      "        [[[44.5402, 64.2155, 84.0990,  ..., 38.5885, 52.0685, 36.5487]]],\n",
      "\n",
      "\n",
      "        [[[44.5419, 64.2136, 84.0984,  ..., 38.5874, 52.0699, 36.5496]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.3787, 63.1369, 85.2073,  ..., 39.6303, 59.8421, 41.8980]]],\n",
      "\n",
      "\n",
      "        [[[42.3778, 63.1332, 85.2015,  ..., 39.6304, 59.8474, 41.8991]]],\n",
      "\n",
      "\n",
      "        [[[42.3767, 63.1298, 85.1958,  ..., 39.6306, 59.8523, 41.9001]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[42.3756, 63.1279, 85.1920,  ..., 39.6309, 59.8555, 41.9006]]],\n",
      "\n",
      "\n",
      "        [[[40.2698, 67.2583, 90.9351,  ..., 35.3270, 54.1491, 46.6889]]],\n",
      "\n",
      "\n",
      "        [[[40.2725, 67.2552, 90.9341,  ..., 35.3251, 54.1517, 46.6904]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.3767, 63.1289, 85.1946,  ..., 39.6304, 59.8536, 41.9005]]],\n",
      "\n",
      "\n",
      "        [[[42.3767, 63.1292, 85.1950,  ..., 39.6305, 59.8531, 41.9003]]],\n",
      "\n",
      "\n",
      "        [[[43.7718, 65.0580, 89.3518,  ..., 39.1044, 56.5257, 41.4789]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 49, Loss: 7509.611328125\n",
      "Epoch 49, Loss: 8303.634521484375\n",
      "tensor([[[[44.1676, 64.1406, 83.6536,  ..., 38.8670, 51.0775, 36.6814]]],\n",
      "\n",
      "\n",
      "        [[[44.1709, 64.1366, 83.6525,  ..., 38.8648, 51.0802, 36.6831]]],\n",
      "\n",
      "\n",
      "        [[[44.1700, 64.1376, 83.6528,  ..., 38.8654, 51.0795, 36.6826]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[44.1701, 64.1375, 83.6527,  ..., 38.8653, 51.0796, 36.6827]]],\n",
      "\n",
      "\n",
      "        [[[44.1707, 64.1369, 83.6525,  ..., 38.8650, 51.0801, 36.6830]]],\n",
      "\n",
      "\n",
      "        [[[41.9854, 62.9285, 84.6211,  ..., 39.8801, 59.0052, 42.0670]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[39.8861, 67.2201, 90.5498,  ..., 35.6398, 53.0587, 46.8039]]],\n",
      "\n",
      "\n",
      "        [[[41.9855, 62.9273, 84.6196,  ..., 39.8799, 59.0068, 42.0674]]],\n",
      "\n",
      "\n",
      "        [[[39.8860, 67.2202, 90.5498,  ..., 35.6399, 53.0586, 46.8039]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[39.8860, 67.2202, 90.5499,  ..., 35.6399, 53.0586, 46.8039]]],\n",
      "\n",
      "\n",
      "        [[[43.4085, 64.9700, 88.9146,  ..., 39.3664, 55.5434, 41.6039]]],\n",
      "\n",
      "\n",
      "        [[[41.9845, 62.9267, 84.6178,  ..., 39.8804, 59.0081, 42.0674]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[[44.2354, 64.4530, 83.7320,  ..., 38.8586, 51.1415, 36.0717]]],\n",
      "\n",
      "\n",
      "        [[[42.0261, 63.1641, 84.6440,  ..., 39.8571, 59.0838, 41.4994]]],\n",
      "\n",
      "\n",
      "        [[[44.2365, 64.4518, 83.7317,  ..., 38.8578, 51.1424, 36.0722]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.0281, 63.1690, 84.6525,  ..., 39.8567, 59.0765, 41.4980]]],\n",
      "\n",
      "\n",
      "        [[[43.4753, 65.2663, 88.9907,  ..., 39.3477, 55.5945, 41.0004]]],\n",
      "\n",
      "\n",
      "        [[[44.2350, 64.4535, 83.7322,  ..., 38.8588, 51.1412, 36.0715]]]],\n",
      "       device='cuda:1', grad_fn=<AddBackward0>)\n",
      "tensor([[[[43.4753, 65.2663, 88.9907,  ..., 39.3477, 55.5945, 41.0004]]],\n",
      "\n",
      "\n",
      "        [[[39.9728, 67.5998, 90.7285,  ..., 35.6460, 53.0986, 46.1201]]],\n",
      "\n",
      "\n",
      "        [[[42.0255, 63.1624, 84.6412,  ..., 39.8572, 59.0863, 41.5000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[42.0316, 63.1558, 84.6396,  ..., 39.8530, 59.0912, 41.5035]]],\n",
      "\n",
      "\n",
      "        [[[42.0262, 63.1639, 84.6438,  ..., 39.8570, 59.0841, 41.4995]]],\n",
      "\n",
      "\n",
      "        [[[42.0259, 63.1625, 84.6417,  ..., 39.8570, 59.0860, 41.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch 50, Loss: 6310.44140625\n",
      "Training time: 6.488998889923096 seconds\n",
      "Epoch 50, Loss: 7826.74462890625\n",
      "Training time: 6.489726305007935 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from st_DynGNN import run\n",
    "devices = torch.device(\"cuda\") if torch.cuda.is_available() \\\n",
    "else torch.device(\"cpu\")\n",
    "\n",
    "dgl_graph.create_formats_()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_gpus = 2\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.spawn(run, args=(list(range(num_gpus)),dgl_graph), nprocs=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "Invalid key \"0\". Must be one of the edge types.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 6\u001b[0m h_a, h_f \u001b[39m=\u001b[39m model(dgl_graph, x)\n\u001b[1;32m      8\u001b[0m h_a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(h_a)\n\u001b[1;32m      9\u001b[0m h_a\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, blocks, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, blocks, x):\n\u001b[0;32m---> 17\u001b[0m     h_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(blocks[\u001b[39m0\u001b[39;49m], x)\n\u001b[1;32m     18\u001b[0m     h_a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(blocks[\u001b[39m1\u001b[39m], h_1)\n\u001b[1;32m     19\u001b[0m     h_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(blocks[\u001b[39m0\u001b[39m], x)\n",
      "File \u001b[0;32m/home/rxf131/ondemand/ubuntu2204/python310/dgl/heterograph.py:2409\u001b[0m, in \u001b[0;36mDGLGraph.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2406\u001b[0m etypes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_etypes(key)\n\u001b[1;32m   2408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(etypes) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2409\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\n\u001b[1;32m   2410\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mInvalid key \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Must be one of the edge types.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2411\u001b[0m             orig_key\n\u001b[1;32m   2412\u001b[0m         )\n\u001b[1;32m   2413\u001b[0m     )\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(etypes) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2416\u001b[0m     \u001b[39m# no ambiguity: return the unitgraph itself\u001b[39;00m\n\u001b[1;32m   2417\u001b[0m     srctype, etype, dsttype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_canonical_etypes[etypes[\u001b[39m0\u001b[39m]]\n",
      "\u001b[0;31mDGLError\u001b[0m: Invalid key \"0\". Must be one of the edge types."
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "model_save_path = os.path.join('st_DynGNN_best.pt')\n",
    "state_dict = {k.replace('module.', ''): v for k, v in torch.load(model_save_path).items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "h_a, h_f = model(dgl_graph, x)\n",
    "\n",
    "h_a = torch.squeeze(h_a)\n",
    "h_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000, 46.5279,  0.0000,  ..., 32.2680,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, 46.5266,  0.0000,  ..., 32.2655,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, 46.5265,  0.0000,  ..., 32.2652,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.0000, 43.4879,  0.0000,  ..., 31.6806,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, 43.4868,  0.0000,  ..., 31.6780,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, 43.4882,  0.0000,  ..., 31.6811,  0.0000,  0.0000]]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1478.1057, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "    for j in range(120):\n",
    "        if int(j/K) == i:\n",
    "            d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(torch.squeeze(h_a)+torch.squeeze(h_f), x)  + \\\n",
    "                10*torch.std(torch.diff(h_a)) +\\\n",
    "                0 * torch.var(h_a) +\\\n",
    "                100 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(torch.squeeze(h_f), 0, 1))[1])) +\\\n",
    "                5 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(torch.squeeze(h_f), E.T).float(), L.float()), torch.matmul(E, torch.squeeze(h_f).T).float()), 0))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available physical CPUs: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     num_gpus = torch.cuda.device_count()\n",
    "#     print(\"Number of available GPUs:\", num_gpus)\n",
    "# else:\n",
    "num_cpus = psutil.cpu_count(logical=False)  # Count physical CPUs (sockets)\n",
    "print(\"Number of available physical CPUs:\", num_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(120, 60)\n",
      "  (conv2): GCNConv(60, 120)\n",
      "  (conv3): GCNConv(120, 60)\n",
      "  (conv4): GCNConv(60, 120)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, GCNConv, TransformerConv, AGNNConv, FusedGATConv, ChebConv, SAGEConv, GraphConv, TAGConv, GMMConv \n",
    "from scipy.stats import linregress\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "        self.conv1 = GCNConv(120, 60)\n",
    "        self.conv2 = GCNConv(60, 120)\n",
    "        self.conv3 = GCNConv(120, 60)\n",
    "        self.conv4 = GCNConv(60, 120)\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "  \n",
    "        h_1 = self.conv1(x, edge_index)\n",
    "        h_a = self.conv2(h_1, edge_index)\n",
    "\n",
    "\n",
    "        h_2 = self.conv3(x, edge_index)\n",
    "        h_f = self.conv4(h_2, edge_index)\n",
    "\n",
    "        return h_a, h_f\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 4043.5540\n",
      "Epoch: 2\tLoss: 3077.6011\n",
      "Epoch: 3\tLoss: 1203.8195\n",
      "Epoch: 4\tLoss: 1130.1881\n",
      "Epoch: 5\tLoss: 788.4680\n",
      "Epoch: 6\tLoss: 574.6251\n",
      "Epoch: 7\tLoss: 534.2384\n",
      "Epoch: 8\tLoss: 519.5971\n",
      "Epoch: 9\tLoss: 398.7972\n",
      "Epoch: 10\tLoss: 292.3871\n",
      "Epoch: 11\tLoss: 272.8162\n",
      "Epoch: 12\tLoss: 253.1906\n",
      "Epoch: 13\tLoss: 203.6208\n",
      "Epoch: 14\tLoss: 169.9689\n",
      "Epoch: 15\tLoss: 150.9902\n",
      "Epoch: 16\tLoss: 138.5900\n",
      "Epoch: 17\tLoss: 126.7226\n",
      "Epoch: 18\tLoss: 110.5001\n",
      "Epoch: 19\tLoss: 104.1171\n",
      "Epoch: 20\tLoss: 88.5925\n",
      "Epoch: 21\tLoss: 69.0048\n",
      "Epoch: 22\tLoss: 61.3695\n",
      "Epoch: 23\tLoss: 55.2481\n",
      "Epoch: 24\tLoss: 49.1813\n",
      "Epoch: 25\tLoss: 43.2701\n",
      "Epoch: 26\tLoss: 38.8014\n",
      "Epoch: 27\tLoss: 36.3555\n",
      "Epoch: 28\tLoss: 34.8477\n",
      "Epoch: 29\tLoss: 32.6112\n",
      "Epoch: 30\tLoss: 31.5829\n",
      "Epoch: 31\tLoss: 29.7149\n",
      "Epoch: 32\tLoss: 28.3097\n",
      "Epoch: 33\tLoss: 26.9053\n",
      "Epoch: 34\tLoss: 23.6492\n",
      "Epoch: 35\tLoss: 21.8967\n",
      "Epoch: 36\tLoss: 21.4657\n",
      "Epoch: 37\tLoss: 21.1018\n",
      "Epoch: 38\tLoss: 20.0730\n",
      "Epoch: 39\tLoss: 19.3213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40\tLoss: 19.2897\n",
      "Epoch: 41\tLoss: 18.5695\n",
      "Epoch: 42\tLoss: 17.1709\n",
      "Epoch: 43\tLoss: 16.2519\n",
      "Epoch: 44\tLoss: 15.7972\n",
      "Epoch: 45\tLoss: 15.4778\n",
      "Epoch: 46\tLoss: 15.6919\n",
      "Epoch: 47\tLoss: 16.8263\n",
      "Epoch: 48\tLoss: 20.7117\n",
      "Epoch: 49\tLoss: 32.2773\n",
      "Epoch: 50\tLoss: 64.5979\n",
      "Epoch: 51\tLoss: 132.1909\n",
      "Epoch: 52\tLoss: 203.4060\n",
      "Epoch: 53\tLoss: 146.8800\n",
      "Epoch: 54\tLoss: 21.9300\n",
      "Epoch: 55\tLoss: 51.9439\n",
      "Epoch: 56\tLoss: 116.7872\n",
      "Epoch: 57\tLoss: 40.7097\n",
      "Epoch: 58\tLoss: 23.9973\n",
      "Epoch: 59\tLoss: 80.8747\n",
      "Epoch: 60\tLoss: 29.1475\n",
      "Epoch: 61\tLoss: 23.9380\n",
      "Epoch: 62\tLoss: 58.4518\n",
      "Epoch: 63\tLoss: 14.3489\n",
      "Epoch: 64\tLoss: 30.3226\n",
      "Epoch: 65\tLoss: 36.9141\n",
      "Epoch: 66\tLoss: 9.1399\n",
      "Epoch: 67\tLoss: 33.3893\n",
      "Epoch: 68\tLoss: 17.2631\n",
      "Epoch: 69\tLoss: 15.0563\n",
      "Epoch: 70\tLoss: 26.4170\n",
      "Epoch: 71\tLoss: 8.4907\n",
      "Epoch: 72\tLoss: 21.2588\n",
      "Epoch: 73\tLoss: 14.3692\n",
      "Epoch: 74\tLoss: 11.1695\n",
      "Epoch: 75\tLoss: 18.6672\n",
      "Epoch: 76\tLoss: 8.0061\n",
      "Epoch: 77\tLoss: 15.1045\n",
      "Epoch: 78\tLoss: 11.3733\n",
      "Epoch: 79\tLoss: 9.2950\n",
      "Epoch: 80\tLoss: 13.6646\n",
      "Epoch: 81\tLoss: 7.5555\n",
      "Epoch: 82\tLoss: 11.5349\n",
      "Epoch: 83\tLoss: 9.5687\n",
      "Epoch: 84\tLoss: 8.0695\n",
      "Epoch: 85\tLoss: 10.7618\n",
      "Epoch: 86\tLoss: 7.2484\n",
      "Epoch: 87\tLoss: 9.1357\n",
      "Epoch: 88\tLoss: 8.5329\n",
      "Epoch: 89\tLoss: 7.1260\n",
      "Epoch: 90\tLoss: 8.8370\n",
      "Epoch: 91\tLoss: 7.0211\n",
      "Epoch: 92\tLoss: 7.4949\n",
      "Epoch: 93\tLoss: 7.7846\n",
      "Epoch: 94\tLoss: 6.5387\n",
      "Epoch: 95\tLoss: 7.4497\n",
      "Epoch: 96\tLoss: 6.8722\n",
      "Epoch: 97\tLoss: 6.4659\n",
      "Epoch: 98\tLoss: 7.0554\n",
      "Epoch: 99\tLoss: 6.3217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Lambda1 = 0.1\n",
    "# Lambda2 = 250\n",
    "# Lambda3 = 1\n",
    "\n",
    "Lambda1 = 0\n",
    "Lambda2 = 100\n",
    "Lambda3 = 5\n",
    "\n",
    "m1 = [[1] for i in range(1,121)]\n",
    "m2 = [[i] for i in range(1,121)]\n",
    "m = torch.Tensor([m1, m2])\n",
    "m = torch.squeeze(m).T\n",
    "\n",
    "K = 12\n",
    "d = [ [ 0 for y in range(120) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(120):\n",
    "    if int(j/K) == i:\n",
    "      d[i][j] = 1/K\n",
    "E = torch.as_tensor(d)\n",
    "\n",
    "W = [ [ 1 for y in range(10) ] for x in range(10) ]\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    W[i][j] = abs(j-i)\n",
    "W = torch.as_tensor(W)\n",
    "W = W.fill_diagonal_(1)\n",
    "\n",
    "\n",
    "I = [ 1 for x in range(10) ]\n",
    "I = torch.as_tensor(I)\n",
    "L = torch.diag(torch.matmul(W, I), 0) - W\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay= 0.02)\n",
    "\n",
    "\n",
    "def train(x, edge_index, Lambda1, Lambda2):\n",
    "  optimizer.zero_grad()\n",
    "  h_a, h_f = model(x, edge_index)\n",
    "  loss = criterion(h_a+h_f, x)\n",
    "  # loss = criterion(h_a+h_f, x)  + \\\n",
    "  # 10*torch.std(torch.diff(h_a)) +\\\n",
    "  # Lambda1 * torch.var(h_a) +\\\n",
    "  # Lambda2 * torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])) +\\\n",
    "  # Lambda3 * torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0))\n",
    "\n",
    "  # print(criterion(h_a + h_f, x))\n",
    "  # print(torch.sum(torch.abs(torch.matmul(torch.matmul(torch.linalg.inv(torch.matmul(torch.transpose(m, 0, 1), m)), torch.transpose(m, 0, 1)), torch.transpose(h_f, 0, 1))[1])))\n",
    "  # print(torch.sum(torch.diagonal(torch.matmul(torch.matmul(torch.matmul(h_f, E.T).long(), L.long()), torch.matmul(E, h_f.T).long()), 0)))\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss, criterion(h_a+h_f, x), h_a, h_f\n",
    "\n",
    "epochs = range(1, 100)\n",
    "losses = []\n",
    "embeddings = []\n",
    "min_loss = 1000\n",
    "for epoch in epochs:\n",
    "  loss, loss_2, h_a, h_f= train(x, edge_index, Lambda1, Lambda2)\n",
    "  losses.append(loss)\n",
    "  print(f\"Epoch: {epoch}\\tLoss: {loss:.4f}\")\n",
    "  # if loss_2 < 100:\n",
    "  #   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.3434, 26.4559, 25.8054,  ..., 20.5591, 42.3927, 17.1584],\n",
       "        [22.3434, 26.4559, 25.8054,  ..., 20.5591, 42.3927, 17.1584],\n",
       "        [22.3434, 26.4559, 25.8054,  ..., 20.5591, 42.3927, 17.1584],\n",
       "        ...,\n",
       "        [22.9697, 25.4002, 23.6304,  ..., 17.8749, 37.6075, 12.5646],\n",
       "        [22.9697, 25.4002, 23.6304,  ..., 17.8749, 37.6075, 12.5646],\n",
       "        [22.9697, 25.4002, 23.6304,  ..., 17.8749, 37.6075, 12.5646]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_f"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
